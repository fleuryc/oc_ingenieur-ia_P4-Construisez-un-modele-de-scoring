{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prêt à dépenser : Construire un modèle de scoring\n",
    "\n",
    "\n",
    "## Contexte\n",
    "\n",
    "\"Prêt à dépenser\" (Home Credit) est une société financière qui propose des crédits à la consommation pour des personnes ayant peu ou pas d'historique de prêt.\n",
    "Pour accorder un crédit à la consommation, l'entreprise calcule la probabilité qu'un client le rembourse, ou non. Elle souhaite donc développer un algorithme de scoring pour aider à décider si un prêt peut être accordé à un client.\n",
    "\n",
    "Les chargés de relation client seront les utilisateurs du modèle de scoring. Puisqu'ils s'adressent aux clients, ils ont besoin que votre modèle soit facilement interprétable. Les chargés de relation souhaitent, en plus, disposer d'une mesure de l'importance des variables qui ont poussé le modèle à donner cette probabilité à un client.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chargement des modules du projet\n",
    "\n",
    "Afin de simplifier le Notebook, le code métier du projet est placé dans le répertoire [src/](../src/).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import project modules from source directory\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "# System modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Append source directory to system path\n",
    "src_path = os.path.abspath(os.path.join(\"../src\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Helper functions\n",
    "import data.helpers as data_helpers\n",
    "import features.helpers as feat_helpers\n",
    "import visualization.helpers as vis_helpers\n",
    "import models.helpers as models_helpers\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons utiliser le langage [Python](https://www.python.org/about/gettingstarted/), et présenter ici le code, les résultats et l'analyse sous forme de [Notebook JupyterLab](https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html).\n",
    "\n",
    "Nous allons aussi utiliser les bibliothèques usuelles d'exploration et analyse de données, afin d'améliorer la simplicité et la performance de notre code :\n",
    "  * [NumPy](https://numpy.org/doc/stable/user/quickstart.html) et [Pandas](https://pandas.pydata.org/docs/user_guide/index.html) : effectuer des calculs scientifiques (statistiques, algèbre, ...) et manipuler des séries et tableaux de données volumineuses et complexes\n",
    "  * [scikit-learn](https://scikit-learn.org/stable/getting_started.html) et [XGBoost](https://xgboost.readthedocs.io/en/latest/get_started.html) : pour effectuer des analyses prédictives \n",
    "  * [Matplotlib](https://matplotlib.org/stable/tutorials/introductory/usage.html), [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html), [Seaborn](https://seaborn.pydata.org/tutorial/function_overview.html) et [Plotly](https://plotly.com/python/getting-started/) : générer des graphiques lisibles, intéractifs et pertinents\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Accelerate the development cycle\n",
    "SAMPLE_FRAC: float = 1\n",
    "\n",
    "# Prevent excessive memory usage used by plotly\n",
    "DRAW_PLOTS: bool = False\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chargement des données\n",
    "\n",
    "Les données mises à disposition sont issues de [Home Credit](https://www.homecredit.net/) et plus précisément de la compétition hébergée sur Kaggle [Home Credit Default Risk - Can you predict how capable each applicant is of repaying a loan?](https://www.kaggle.com/c/home-credit-default-risk)\n",
    "\n",
    "Les données sont fournies sous la forme de plusieurs fichiers CSV pouvant être liés entre eux de la manière suivante :\n",
    "\n",
    "![Home Credit data relations](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)\n",
    "\n",
    "_source : [Introduction: Home Credit Default Risk Competition](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction) by [Will Koehrsen](https://www.kaggle.com/willkoehrsen)_\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Download and extract the raw data\n",
    "data_helpers.download_extract_zip(\n",
    "    zip_file_url=\"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/Projet+-+Impl%C3%A9menter+un+mod%C3%A8le+de+scoring/Projet+Mise+en+prod+-+home-credit-default-risk.zip\",\n",
    "    files_names=(\n",
    "        \"application_test.csv\",\n",
    "        \"application_train.csv\",\n",
    "        \"bureau_balance.csv\",\n",
    "        \"bureau.csv\",\n",
    "        \"credit_card_balance.csv\",\n",
    "        \"installments_payments.csv\",\n",
    "        \"POS_CASH_balance.csv\",\n",
    "        \"previous_application.csv\",\n",
    "    ),\n",
    "    target_path=\"../data/raw/\",\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons charger toutes les données du fichier `application_train.csv` afin de travailler préparer les données du jeu d'entraînement et de test (variable `TARGET` vaut `O` : le client n'a pas fait défaut ou `1` : le client a fait défaut). Nous les séparerons les jeux de données au moment de l'entraînement et évaluation de nos modèles.\n",
    "\n",
    "Les fichiers contiennent un grand nombre de variables booléennes et catégorielles que nous pouvons déjà typer comme telles.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Read column names\n",
    "application_train_column_names = pd.read_csv(\n",
    "    \"../data/raw/application_train.csv\", nrows=0\n",
    ").columns.values\n",
    "\n",
    "# TARGET variable must be present in the Train datase\n",
    "if \"TARGET\" not in application_train_column_names:\n",
    "    raise ValueError(\n",
    "        \"TARGET column not found in application_train.csv. Please check that the file is not corrupted.\"\n",
    "    )\n",
    "\n",
    "# SK_ID_CURR variable must be present in the Train datase\n",
    "if \"SK_ID_CURR\" not in application_train_column_names:\n",
    "    raise ValueError(\n",
    "        \"SK_ID_CURR column not found in application_train_column_names.csv. Please check that the file is not corrupted.\"\n",
    "    )\n",
    "\n",
    "# Set column types according to fields description (../data/raw/HomeCredit_columns_description.csv)\n",
    "# Categorical variables\n",
    "column_types = {\n",
    "    col: \"category\"\n",
    "    for col in application_train_column_names\n",
    "    if col.startswith((\"NAME_\",))\n",
    "    or col.endswith((\"_TYPE\"))\n",
    "    or col\n",
    "    in [\n",
    "        \"CODE_GENDER\",\n",
    "        \"WEEKDAY_APPR_PROCESS_START\",\n",
    "        \"FONDKAPREMONT_MODE\",\n",
    "        \"HOUSETYPE_MODE\",\n",
    "        \"WALLSMATERIAL_MODE\",\n",
    "        \"EMERGENCYSTATE_MODE\",\n",
    "    ]\n",
    "}\n",
    "# Boolean variables\n",
    "column_types |= {\n",
    "    col: bool\n",
    "    for col in application_train_column_names\n",
    "    if col.startswith((\"FLAG_\", \"REG_\", \"LIVE_\"))\n",
    "}\n",
    "\n",
    "# Load application data\n",
    "app_df = pd.read_csv(\n",
    "    \"../data/raw/application_train.csv\",\n",
    "    dtype=column_types,\n",
    "    true_values=[\"Y\", \"Yes\", \"1\"],\n",
    "    false_values=[\"N\", \"No\", \"0\"],\n",
    "    na_values=[\"XNA\"], # bad values\n",
    ")\n",
    "\n",
    "# Sample to speed up development\n",
    "if  float == type(SAMPLE_FRAC) and 0 < SAMPLE_FRAC < 1:\n",
    "    app_df = app_df.sample(frac=SAMPLE_FRAC)\n",
    "\n",
    "# Let's display basic statistical info about the data\n",
    "app_df.describe(include=\"all\")\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>...</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>307511.000000</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>307511</td>\n",
       "      <td>307507</td>\n",
       "      <td>307511</td>\n",
       "      <td>307511</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>3.075110e+05</td>\n",
       "      <td>3.075110e+05</td>\n",
       "      <td>307499.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>307511</td>\n",
       "      <td>307511</td>\n",
       "      <td>307511</td>\n",
       "      <td>307511</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>278232</td>\n",
       "      <td>202448</td>\n",
       "      <td>202924</td>\n",
       "      <td>213312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>305011</td>\n",
       "      <td>307328</td>\n",
       "      <td>307355</td>\n",
       "      <td>307408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>278180.518577</td>\n",
       "      <td>0.080729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.417052</td>\n",
       "      <td>1.687979e+05</td>\n",
       "      <td>5.990260e+05</td>\n",
       "      <td>27108.573909</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.034362</td>\n",
       "      <td>0.267395</td>\n",
       "      <td>0.265474</td>\n",
       "      <td>1.899974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>102790.175348</td>\n",
       "      <td>0.272419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.722121</td>\n",
       "      <td>2.371231e+05</td>\n",
       "      <td>4.024908e+05</td>\n",
       "      <td>14493.737315</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083849</td>\n",
       "      <td>0.110757</td>\n",
       "      <td>0.204685</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.794056</td>\n",
       "      <td>1.869295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>100002.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.565000e+04</td>\n",
       "      <td>4.500000e+04</td>\n",
       "      <td>1615.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>189145.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.125000e+05</td>\n",
       "      <td>2.700000e+05</td>\n",
       "      <td>16524.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>278202.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.471500e+05</td>\n",
       "      <td>5.135310e+05</td>\n",
       "      <td>24903.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>367142.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.025000e+05</td>\n",
       "      <td>8.086500e+05</td>\n",
       "      <td>34596.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>456255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.170000e+08</td>\n",
       "      <td>4.050000e+06</td>\n",
       "      <td>258025.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           SK_ID_CURR         TARGET NAME_CONTRACT_TYPE CODE_GENDER  \\\n",
       "count   307511.000000  307511.000000             307511      307507   \n",
       "unique            NaN            NaN                  2           2   \n",
       "top               NaN            NaN         Cash loans           F   \n",
       "freq              NaN            NaN             278232      202448   \n",
       "mean    278180.518577       0.080729                NaN         NaN   \n",
       "std     102790.175348       0.272419                NaN         NaN   \n",
       "min     100002.000000       0.000000                NaN         NaN   \n",
       "25%     189145.500000       0.000000                NaN         NaN   \n",
       "50%     278202.000000       0.000000                NaN         NaN   \n",
       "75%     367142.500000       0.000000                NaN         NaN   \n",
       "max     456255.000000       1.000000                NaN         NaN   \n",
       "\n",
       "       FLAG_OWN_CAR FLAG_OWN_REALTY   CNT_CHILDREN  AMT_INCOME_TOTAL  \\\n",
       "count        307511          307511  307511.000000      3.075110e+05   \n",
       "unique            2               2            NaN               NaN   \n",
       "top           False            True            NaN               NaN   \n",
       "freq         202924          213312            NaN               NaN   \n",
       "mean            NaN             NaN       0.417052      1.687979e+05   \n",
       "std             NaN             NaN       0.722121      2.371231e+05   \n",
       "min             NaN             NaN       0.000000      2.565000e+04   \n",
       "25%             NaN             NaN       0.000000      1.125000e+05   \n",
       "50%             NaN             NaN       0.000000      1.471500e+05   \n",
       "75%             NaN             NaN       1.000000      2.025000e+05   \n",
       "max             NaN             NaN      19.000000      1.170000e+08   \n",
       "\n",
       "          AMT_CREDIT    AMT_ANNUITY  ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19  \\\n",
       "count   3.075110e+05  307499.000000  ...            307511           307511   \n",
       "unique           NaN            NaN  ...                 2                2   \n",
       "top              NaN            NaN  ...             False            False   \n",
       "freq             NaN            NaN  ...            305011           307328   \n",
       "mean    5.990260e+05   27108.573909  ...               NaN              NaN   \n",
       "std     4.024908e+05   14493.737315  ...               NaN              NaN   \n",
       "min     4.500000e+04    1615.500000  ...               NaN              NaN   \n",
       "25%     2.700000e+05   16524.000000  ...               NaN              NaN   \n",
       "50%     5.135310e+05   24903.000000  ...               NaN              NaN   \n",
       "75%     8.086500e+05   34596.000000  ...               NaN              NaN   \n",
       "max     4.050000e+06  258025.500000  ...               NaN              NaN   \n",
       "\n",
       "       FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
       "count            307511           307511              265992.000000   \n",
       "unique                2                2                        NaN   \n",
       "top               False            False                        NaN   \n",
       "freq             307355           307408                        NaN   \n",
       "mean                NaN              NaN                   0.006402   \n",
       "std                 NaN              NaN                   0.083849   \n",
       "min                 NaN              NaN                   0.000000   \n",
       "25%                 NaN              NaN                   0.000000   \n",
       "50%                 NaN              NaN                   0.000000   \n",
       "75%                 NaN              NaN                   0.000000   \n",
       "max                 NaN              NaN                   4.000000   \n",
       "\n",
       "       AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
       "count              265992.000000               265992.000000   \n",
       "unique                       NaN                         NaN   \n",
       "top                          NaN                         NaN   \n",
       "freq                         NaN                         NaN   \n",
       "mean                    0.007000                    0.034362   \n",
       "std                     0.110757                    0.204685   \n",
       "min                     0.000000                    0.000000   \n",
       "25%                     0.000000                    0.000000   \n",
       "50%                     0.000000                    0.000000   \n",
       "75%                     0.000000                    0.000000   \n",
       "max                     9.000000                    8.000000   \n",
       "\n",
       "        AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
       "count               265992.000000              265992.000000   \n",
       "unique                        NaN                        NaN   \n",
       "top                           NaN                        NaN   \n",
       "freq                          NaN                        NaN   \n",
       "mean                     0.267395                   0.265474   \n",
       "std                      0.916002                   0.794056   \n",
       "min                      0.000000                   0.000000   \n",
       "25%                      0.000000                   0.000000   \n",
       "50%                      0.000000                   0.000000   \n",
       "75%                      0.000000                   0.000000   \n",
       "max                     27.000000                 261.000000   \n",
       "\n",
       "        AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "count                265992.000000  \n",
       "unique                         NaN  \n",
       "top                            NaN  \n",
       "freq                           NaN  \n",
       "mean                      1.899974  \n",
       "std                       1.869295  \n",
       "min                       0.000000  \n",
       "25%                       0.000000  \n",
       "50%                       1.000000  \n",
       "75%                       3.000000  \n",
       "max                      25.000000  \n",
       "\n",
       "[11 rows x 122 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "app_df.info()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 307511 entries, 0 to 307510\n",
      "Columns: 122 entries, SK_ID_CURR to AMT_REQ_CREDIT_BUREAU_YEAR\n",
      "dtypes: bool(34), category(14), float64(65), int64(9)\n",
      "memory usage: 187.7 MB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le jeu de données contient 122 variables, dont la variable cible que nous devons estimer : `TARGET`. Parmis ces variables, nous avons :\n",
    "- 34 variables booléennes\n",
    "- 14 variables catégorielles\n",
    "- 74 variables numériques"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse exploratoire\n",
    "\n",
    "Nous allons analyser la distribution de quelques variables.\n",
    "\n",
    "### Variable cible\n",
    "\n",
    "Voyons spécifiquement la distribution de la variable `TARGET` qui est celle que nous devrons estimer par la suite.\n",
    "Les valeurs nulles représentent notre jeu d'entrainement.\n",
    "Nous pouvons oberver que nous avons à faire à un problème de __classification binaire déséquilibré__ (il y a deux valeurs possibles, mais les deux valeurs ne sont pas également représentées).\n",
    "Ceci va influencer la manière dont nous allons construire et entraîner notre modèle.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Let's plot the distribution of the TARGET variable\n",
    "if DRAW_PLOTS:\n",
    "    fig = px.bar(\n",
    "        app_df[\"TARGET\"].replace({\n",
    "            \"0\": \"TARGET=0 : payments OK\", \n",
    "            \"1\": \"TARGET=1 : payment difficulties\", \n",
    "        }).value_counts(),\n",
    "        title=\"Distribution of TARGET variable\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "    ).update_xaxes(\n",
    "        title=\"TARGET\",\n",
    "    ).update_yaxes(title=\"Count\")\n",
    "    fig.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Valeurs vides\n",
    "\n",
    "Nous voyons que toutes les variables ont moins de 30% de valeurs vides, et près de la moitié a moins de 1% de valeurs vides. Le jeu de données est donc relativement bien rempli, ce qui ne devrait pas poser de problème pour la suite.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Let's display variables with missing values ratio\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_empty_values(app_df)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Valeurs impossibles\n",
    "\n",
    "Quelques valeurs présentes dans les données semblent impossibles. Nous allons supprimer ces \"outliers\".\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Define data constraints\n",
    "data_constraints = {\n",
    "    \"DAYS_EMPLOYED\": {\"min\": -35000, \"max\": 0,}, # max 100 years, only negative values\n",
    "}\n",
    "\n",
    "if DRAW_PLOTS:\n",
    "    # Let's display box plots for variables with outliers\n",
    "    vis_helpers.plot_boxes(app_df, plot_columns=data_constraints.keys(), categorical_column=\"TARGET\")\n",
    "\n",
    "# Remove values that are outside possible range\n",
    "app_df = feat_helpers.drop_impossible_values(\n",
    "    app_df, constraints=data_constraints,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables quantitatives\n",
    "\n",
    "Nous allons simplement afficher la distribution de quelques variables numériques. Nous voyons déjà que selon la valeur de TARGET, la distribution (moyenne) des variables peut être sensiblement différente.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Draw the BoxPlots of some numeric columns, split per Target\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_boxes(app_df,\n",
    "        plot_columns=[\n",
    "            \"AMT_INCOME_TOTAL\",\n",
    "            \"AMT_CREDIT\",\n",
    "            \"AMT_ANNUITY\",\n",
    "            \"AMT_GOODS_PRICE\",\n",
    "            \"DAYS_BIRTH\",\n",
    "            \"DAYS_EMPLOYED\",\n",
    "            \"OWN_CAR_AGE\",\n",
    "            \"REGION_RATING_CLIENT\",\n",
    "            \"REGION_RATING_CLIENT_W_CITY\",\n",
    "            \"EXT_SOURCE_1\",\n",
    "            \"EXT_SOURCE_2\",\n",
    "            \"EXT_SOURCE_3\",\n",
    "            \"DAYS_LAST_PHONE_CHANGE\",\n",
    "            \"AMT_REQ_CREDIT_BUREAU_YEAR\",\n",
    "        ],\n",
    "        categorical_column=\"TARGET\",\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables qualitatives\n",
    "\n",
    "De la même manière, nous allons simplement afficher la distribution de quelques variables catégorielles. Nous voyons déjà que selon la valeur de TARGET, la distribution (répartition entre classes) des variables peut être sensiblement différente (`TARGET=0` pour 77,6% des `NAME_CONTRACT_TYPE=\"Cash loans\"`, tandis que `TARGET=0` pour 93,1% des `NAME_CONTRACT_TYPE=\"Revolving loans\"`).\n",
    "Certaines variables ont une répartition très inégale entre classes (`FLAG_MOBIL` vaut systématiquement `True` et jamais `False`).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Draw the Bar charts of some categorical columns, split per Target\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_categories_bars(app_df,\n",
    "        plot_columns=[\n",
    "            \"NAME_CONTRACT_TYPE\",\n",
    "            \"CODE_GENDER\",\n",
    "            \"FLAG_OWN_CAR\",\n",
    "            \"FLAG_OWN_REALTY\",\n",
    "            \"NAME_INCOME_TYPE\",\n",
    "            \"NAME_EDUCATION_TYPE\",\n",
    "            \"NAME_FAMILY_STATUS\",\n",
    "            \"NAME_HOUSING_TYPE\",\n",
    "            \"OCCUPATION_TYPE\",\n",
    "            \"FLAG_MOBIL\",\n",
    "        ],\n",
    "        categorical_column=\"TARGET\",\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encodage des catégories\n",
    "\n",
    "Lorsque les données qualitatives ne sont pas ordinales (on ne peu pas les classer selon un certain ordre), l'encodage \"One Hot Encoding\" sera plus performant que le \"Label Encoding\".\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "## Pandas get_dummies() categorical data encoder\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html\n",
    "# Convert categorical variable into dummy/indicator variables.\n",
    "# a.k.a \"One Hot Encoding\"\n",
    "\n",
    "app_df = pd.get_dummies(app_df, dtype=bool)\n",
    "\n",
    "app_df.describe(include=\"bool\")\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>FLAG_MOBIL</th>\n",
       "      <th>FLAG_EMP_PHONE</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_CONT_MOBILE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>FLAG_EMAIL</th>\n",
       "      <th>REG_REGION_NOT_LIVE_REGION</th>\n",
       "      <th>REG_REGION_NOT_WORK_REGION</th>\n",
       "      <th>...</th>\n",
       "      <th>HOUSETYPE_MODE_terraced house</th>\n",
       "      <th>WALLSMATERIAL_MODE_Block</th>\n",
       "      <th>WALLSMATERIAL_MODE_Mixed</th>\n",
       "      <th>WALLSMATERIAL_MODE_Monolithic</th>\n",
       "      <th>WALLSMATERIAL_MODE_Others</th>\n",
       "      <th>WALLSMATERIAL_MODE_Panel</th>\n",
       "      <th>WALLSMATERIAL_MODE_Stone, brick</th>\n",
       "      <th>WALLSMATERIAL_MODE_Wooden</th>\n",
       "      <th>EMERGENCYSTATE_MODE_No</th>\n",
       "      <th>EMERGENCYSTATE_MODE_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>...</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>157719</td>\n",
       "      <td>171082</td>\n",
       "      <td>252136</td>\n",
       "      <td>252125</td>\n",
       "      <td>190830</td>\n",
       "      <td>251601</td>\n",
       "      <td>182123</td>\n",
       "      <td>236126</td>\n",
       "      <td>247790</td>\n",
       "      <td>236525</td>\n",
       "      <td>...</td>\n",
       "      <td>251130</td>\n",
       "      <td>244626</td>\n",
       "      <td>250217</td>\n",
       "      <td>250616</td>\n",
       "      <td>250778</td>\n",
       "      <td>197798</td>\n",
       "      <td>198699</td>\n",
       "      <td>247695</td>\n",
       "      <td>131341</td>\n",
       "      <td>250191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FLAG_OWN_CAR FLAG_OWN_REALTY FLAG_MOBIL FLAG_EMP_PHONE FLAG_WORK_PHONE  \\\n",
       "count        252137          252137     252137         252137          252137   \n",
       "unique            2               2          2              2               2   \n",
       "top           False            True       True           True           False   \n",
       "freq         157719          171082     252136         252125          190830   \n",
       "\n",
       "       FLAG_CONT_MOBILE FLAG_PHONE FLAG_EMAIL REG_REGION_NOT_LIVE_REGION  \\\n",
       "count            252137     252137     252137                     252137   \n",
       "unique                2          2          2                          2   \n",
       "top                True      False      False                      False   \n",
       "freq             251601     182123     236126                     247790   \n",
       "\n",
       "       REG_REGION_NOT_WORK_REGION  ... HOUSETYPE_MODE_terraced house  \\\n",
       "count                      252137  ...                        252137   \n",
       "unique                          2  ...                             2   \n",
       "top                         False  ...                         False   \n",
       "freq                       236525  ...                        251130   \n",
       "\n",
       "       WALLSMATERIAL_MODE_Block WALLSMATERIAL_MODE_Mixed  \\\n",
       "count                    252137                   252137   \n",
       "unique                        2                        2   \n",
       "top                       False                    False   \n",
       "freq                     244626                   250217   \n",
       "\n",
       "       WALLSMATERIAL_MODE_Monolithic WALLSMATERIAL_MODE_Others  \\\n",
       "count                         252137                    252137   \n",
       "unique                             2                         2   \n",
       "top                            False                     False   \n",
       "freq                          250616                    250778   \n",
       "\n",
       "       WALLSMATERIAL_MODE_Panel WALLSMATERIAL_MODE_Stone, brick  \\\n",
       "count                    252137                          252137   \n",
       "unique                        2                               2   \n",
       "top                       False                           False   \n",
       "freq                     197798                          198699   \n",
       "\n",
       "       WALLSMATERIAL_MODE_Wooden EMERGENCYSTATE_MODE_No  \\\n",
       "count                     252137                 252137   \n",
       "unique                         2                      2   \n",
       "top                        False                   True   \n",
       "freq                      247695                 131341   \n",
       "\n",
       "       EMERGENCYSTATE_MODE_Yes  \n",
       "count                   252137  \n",
       "unique                       2  \n",
       "top                      False  \n",
       "freq                    250191  \n",
       "\n",
       "[4 rows x 168 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous avons ici créé 134 nouvelles variables booléennes qui correspondent aux différentes classes de chacune des 14 anciennes variables catégorielles qui ont été encodées et supprimées.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Nous allons tenter d'enrichir nos données en intégrant des variables qui sont des compositions non linéaires des variables existantes.\n",
    "\n",
    "\n",
    "### Données métier\n",
    "\n",
    "Afin d'apporter plus de sens aux données que nous allons fournir à nos modèles, nous pouvons faire appel aux experts métier qui peuvent nous indiquer des informations qui sont réputées importantes afin de prédire si un client risque d'avoir des problèmes de remboursement ou non.\n",
    "\n",
    "Les informations métier pertinentes sont :\n",
    "- Montant emprunté / Prix du bien acheté : `AMT_CREDIT / AMT_GOODS_PRICE`\n",
    "- Montant des annuités / Montant emprunté : `AMT_ANNUITY / AMT_CREDIT`\n",
    "- Montant des annuités / Revenu annuel : `AMT_ANNUITY / AMT_INCOME_TOTAL`\n",
    "- Ancienneté au travail / Age : `DAYS_EMPLOYED / DAYS_BIRTH`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Create the new features\n",
    "app_df[\"CREDIT_PRICE_RATIO\"] = app_df[\"AMT_CREDIT\"] / app_df[\"AMT_GOODS_PRICE\"]\n",
    "app_df[\"ANNUITY_CREDIT_RATIO\"] = app_df[\"AMT_ANNUITY\"] / app_df[\"AMT_CREDIT\"]\n",
    "app_df[\"ANNUITY_INCOME_RATIO\"] = app_df[\"AMT_ANNUITY\"] / app_df[\"AMT_INCOME_TOTAL\"]\n",
    "app_df[\"EMPLOYED_BIRTH_RATIO\"] = app_df[\"DAYS_EMPLOYED\"] / app_df[\"DAYS_BIRTH\"]\n",
    "\n",
    "# Draw the BoxPlots for these features\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_boxes(app_df,\n",
    "        plot_columns=[\n",
    "            \"CREDIT_PRICE_RATIO\",\n",
    "            \"ANNUITY_CREDIT_RATIO\",\n",
    "            \"ANNUITY_INCOME_RATIO\",\n",
    "            \"EMPLOYED_BIRTH_RATIO\",\n",
    "        ],\n",
    "        categorical_column=\"TARGET\",\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composition polynomiales de variables existantes\n",
    "\n",
    "Les variables `EXT_SOURCE_{1-3}` n'ont a priori pas de sens concret. On peut imaginer que `TARGET` ne soit pas forcément linéairement dépendant de ces variables. Nous allons donc générer des combinaisons polynomiales de ces variables.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "# Let's keep only non null data\n",
    "ext_source = app_df[[\"SK_ID_CURR\", \"TARGET\", \"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]].dropna()\n",
    "\n",
    "# Let's create the new features\n",
    "poly = PolynomialFeatures()\n",
    "poly_feat = pd.DataFrame(poly.fit_transform(\n",
    "        X=ext_source[[\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]],\n",
    "        y=ext_source[\"TARGET\"],\n",
    "    )\n",
    ")\n",
    "poly_feat.columns=poly.get_feature_names()\n",
    "\n",
    "poly_feat.insert(0, \"SK_ID_CURR\", ext_source[\"SK_ID_CURR\"].values)\n",
    "\n",
    "# Merge the new features with the original dataset\n",
    "app_df = app_df.merge(\n",
    "    poly_feat,\n",
    "    on=\"SK_ID_CURR\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Draw the BoxPlots for these features\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_boxes(app_df,\n",
    "        plot_columns=poly_feat.columns[5:],\n",
    "        categorical_column=\"TARGET\",\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Features selection\n",
    "\n",
    "Le but ici est d'éliminer un certain nombre de variables afin d'accélérer l'entrainement et la prédiction de nos modèles. Nous souhaitons éliminer les variables qui pénaliseront le moins possible les performances de nos modèles.\n",
    "\n",
    "Nous savons déjà que les colonnes `SK_ID_CURR` (simple identifiant sans sens métier), `1` et `x{0-2}` (variables polynomiales d'ordre 0 et 1), et `FLAG_MOBIL` (vaut toujours 1) n'apportent pas d'information. Nous allons donc les éliminer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Let's drop the features that are not useful for the prediction\n",
    "app_df = app_df.drop(\n",
    "    columns=[\"SK_ID_CURR\", \"1\", \"x0\", \"x1\", \"x2\", \"FLAG_MOBIL\"]\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons ici observer la corrélation :\n",
    "- de chaque variable avec la variables cible `TARGET` : les variables les moins corrélées à `TARGET` seront a priori les moins utiles pour prédire sa valeur.\n",
    "- entre les différentes variables deux à deux : si deux variables sont très corrélées, elles apportent une information redondante et nous pouvons donc en éliminer une des deux.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Let's compute the correlation matrix\n",
    "app_correlations = app_df.corr().abs().sort_values(\n",
    "    \"TARGET\", ascending=False, axis=0\n",
    ").sort_values(\n",
    "    \"TARGET\", ascending=False, axis=1\n",
    ")\n",
    "\n",
    "if DRAW_PLOTS:\n",
    "    fig = px.imshow(app_correlations,\n",
    "        title=\"Correlations between features\",\n",
    "        width=1200,\n",
    "        height=1200,\n",
    "    )\n",
    "    fig.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons qu'il y a des variables très peu corrélées à `TARGET` (ex. : `abs(corr(\"TARGET\", \"FLAG_EMP_PHONE\")) < 0.0001`), et d'autres très corrélées entre elles (ex. : `abs(corr(\"LIVINGAPARTMENTS_AVG\", \"\"LIVINGAPARTMENTS_MEDI\")) > 0.99`).\n",
    "Nous allons simplifier notre jeu de données en supprimant ces variables.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Let's find variables that are highly de-correlated from TARGET\n",
    "corr_target_min_threshold = 0.01\n",
    "highly_decorrelated_from_target = pd.DataFrame(columns=[\"correlation with TARGET\"])\n",
    "for col in app_correlations.columns:\n",
    "    if col != \"TARGET\" and (\n",
    "        pd.isnull(app_correlations[col][\"TARGET\"])\n",
    "        or abs(app_correlations[col][\"TARGET\"]) < corr_target_min_threshold\n",
    "    ):\n",
    "        highly_decorrelated_from_target.loc[col] = {\n",
    "            \"correlation with TARGET\": app_correlations[col][\"TARGET\"],\n",
    "        }\n",
    "\n",
    "highly_decorrelated_from_target.sort_values(by=[\"correlation with TARGET\"])\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correlation with TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION_TYPE_Industry: type 11</th>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLAG_EMP_PHONE</th>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME_EDUCATION_TYPE_Incomplete higher</th>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION_TYPE_Trade: type 1</th>\n",
       "      <td>0.000328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION_TYPE_Kindergarten</th>\n",
       "      <td>0.009678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEARS_BEGINEXPLUATATION_MODE</th>\n",
       "      <td>0.009697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION_TYPE_University</th>\n",
       "      <td>0.009729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OCCUPATION_TYPE_Cooking staff</th>\n",
       "      <td>0.009858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME_INCOME_TYPE_Unemployed</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       correlation with TARGET\n",
       "ORGANIZATION_TYPE_Industry: type 11                   0.000023\n",
       "FLAG_EMP_PHONE                                        0.000080\n",
       "NAME_EDUCATION_TYPE_Incomplete higher                 0.000223\n",
       "FLAG_DOCUMENT_20                                      0.000241\n",
       "ORGANIZATION_TYPE_Trade: type 1                       0.000328\n",
       "...                                                        ...\n",
       "ORGANIZATION_TYPE_Kindergarten                        0.009678\n",
       "YEARS_BEGINEXPLUATATION_MODE                          0.009697\n",
       "ORGANIZATION_TYPE_University                          0.009729\n",
       "OCCUPATION_TYPE_Cooking staff                         0.009858\n",
       "NAME_INCOME_TYPE_Unemployed                                NaN\n",
       "\n",
       "[119 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Let's find variables that have a highly correlated pair\n",
    "corr_pair_max_threshold = 0.9\n",
    "highly_correlated = pd.DataFrame(columns=[\"pair\", \"correlation\"])\n",
    "for i in range(len(app_correlations.columns)):\n",
    "    for j in range(i + 1, len(app_correlations.columns)):\n",
    "        if app_correlations.iloc[i, j] > corr_pair_max_threshold:\n",
    "            # variables are highly correlated\n",
    "            if app_correlations.iloc[0, i] > app_correlations.iloc[0, j]:\n",
    "                # first variable is more correlated with target => we want to keep it\n",
    "                keep_index = i\n",
    "                drop_index = j\n",
    "            else:\n",
    "                keep_index = j\n",
    "                drop_index = i\n",
    "\n",
    "            highly_correlated.loc[app_correlations.columns[drop_index]] = {\n",
    "                \"pair\": app_correlations.columns[keep_index],\n",
    "                \"correlation\": app_correlations.iloc[i, j],\n",
    "            }\n",
    "\n",
    "highly_correlated.sort_values(by=\"correlation\", ascending=False)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NAME_CONTRACT_TYPE_Revolving loans</th>\n",
       "      <td>NAME_CONTRACT_TYPE_Cash loans</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CODE_GENDER_F</th>\n",
       "      <td>CODE_GENDER_M</td>\n",
       "      <td>0.999966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEARS_BUILD_AVG</th>\n",
       "      <td>YEARS_BUILD_MEDI</td>\n",
       "      <td>0.998625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OBS_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <td>OBS_30_CNT_SOCIAL_CIRCLE</td>\n",
       "      <td>0.998473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOORSMIN_MEDI</th>\n",
       "      <td>FLOORSMIN_AVG</td>\n",
       "      <td>0.997280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOORSMAX_MEDI</th>\n",
       "      <td>FLOORSMAX_AVG</td>\n",
       "      <td>0.997094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTRANCES_MEDI</th>\n",
       "      <td>ENTRANCES_AVG</td>\n",
       "      <td>0.996868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_AVG</th>\n",
       "      <td>COMMONAREA_MEDI</td>\n",
       "      <td>0.996439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEVATORS_MEDI</th>\n",
       "      <td>ELEVATORS_AVG</td>\n",
       "      <td>0.996105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIVINGAREA_MEDI</th>\n",
       "      <td>LIVINGAREA_AVG</td>\n",
       "      <td>0.995623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BASEMENTAREA_MEDI</th>\n",
       "      <td>BASEMENTAREA_AVG</td>\n",
       "      <td>0.995085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APARTMENTS_MEDI</th>\n",
       "      <td>APARTMENTS_AVG</td>\n",
       "      <td>0.994987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIVINGAPARTMENTS_MEDI</th>\n",
       "      <td>LIVINGAPARTMENTS_AVG</td>\n",
       "      <td>0.993817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEARS_BEGINEXPLUATATION_AVG</th>\n",
       "      <td>YEARS_BEGINEXPLUATATION_MEDI</td>\n",
       "      <td>0.993356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDAREA_AVG</th>\n",
       "      <td>LANDAREA_MEDI</td>\n",
       "      <td>0.992257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAPARTMENTS_MEDI</th>\n",
       "      <td>NONLIVINGAPARTMENTS_AVG</td>\n",
       "      <td>0.991873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAREA_MEDI</th>\n",
       "      <td>NONLIVINGAREA_AVG</td>\n",
       "      <td>0.991335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEARS_BUILD_MODE</th>\n",
       "      <td>YEARS_BUILD_AVG</td>\n",
       "      <td>0.989513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOORSMIN_MODE</th>\n",
       "      <td>FLOORSMIN_MEDI</td>\n",
       "      <td>0.988223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOORSMAX_MODE</th>\n",
       "      <td>FLOORSMAX_MEDI</td>\n",
       "      <td>0.988045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <td>AMT_GOODS_PRICE</td>\n",
       "      <td>0.986451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEVATORS_MODE</th>\n",
       "      <td>ELEVATORS_MEDI</td>\n",
       "      <td>0.982439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAPARTMENTS_MODE</th>\n",
       "      <td>NONLIVINGAPARTMENTS_MEDI</td>\n",
       "      <td>0.981891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTRANCES_MODE</th>\n",
       "      <td>ENTRANCES_MEDI</td>\n",
       "      <td>0.980663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_MODE</th>\n",
       "      <td>COMMONAREA_AVG</td>\n",
       "      <td>0.977694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BASEMENTAREA_MODE</th>\n",
       "      <td>BASEMENTAREA_MEDI</td>\n",
       "      <td>0.977032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APARTMENTS_MODE</th>\n",
       "      <td>APARTMENTS_MEDI</td>\n",
       "      <td>0.976759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x0^2</th>\n",
       "      <td>EXT_SOURCE_1</td>\n",
       "      <td>0.976631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIVINGAPARTMENTS_MODE</th>\n",
       "      <td>LIVINGAPARTMENTS_MEDI</td>\n",
       "      <td>0.976294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2^2</th>\n",
       "      <td>EXT_SOURCE_3</td>\n",
       "      <td>0.975462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAREA_MODE</th>\n",
       "      <td>NONLIVINGAREA_MEDI</td>\n",
       "      <td>0.975432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x1^2</th>\n",
       "      <td>EXT_SOURCE_2</td>\n",
       "      <td>0.974734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIVINGAREA_MODE</th>\n",
       "      <td>LIVINGAREA_MEDI</td>\n",
       "      <td>0.974149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDAREA_MODE</th>\n",
       "      <td>LANDAREA_AVG</td>\n",
       "      <td>0.974084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEARS_BEGINEXPLUATATION_MODE</th>\n",
       "      <td>YEARS_BEGINEXPLUATATION_AVG</td>\n",
       "      <td>0.971959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EMPLOYED_BIRTH_RATIO</th>\n",
       "      <td>DAYS_EMPLOYED</td>\n",
       "      <td>0.954679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <td>REGION_RATING_CLIENT_W_CITY</td>\n",
       "      <td>0.950318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTALAREA_MODE</th>\n",
       "      <td>LIVINGAREA_MEDI</td>\n",
       "      <td>0.920174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSETYPE_MODE_block of flats</th>\n",
       "      <td>EMERGENCYSTATE_MODE_No</td>\n",
       "      <td>0.913905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APARTMENTS_AVG</th>\n",
       "      <td>LIVINGAREA_MEDI</td>\n",
       "      <td>0.912065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIVINGAPARTMENTS_AVG</th>\n",
       "      <td>APARTMENTS_MODE</td>\n",
       "      <td>0.906649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             pair  correlation\n",
       "NAME_CONTRACT_TYPE_Revolving loans  NAME_CONTRACT_TYPE_Cash loans     1.000000\n",
       "CODE_GENDER_F                                       CODE_GENDER_M     0.999966\n",
       "YEARS_BUILD_AVG                                  YEARS_BUILD_MEDI     0.998625\n",
       "OBS_60_CNT_SOCIAL_CIRCLE                 OBS_30_CNT_SOCIAL_CIRCLE     0.998473\n",
       "FLOORSMIN_MEDI                                      FLOORSMIN_AVG     0.997280\n",
       "FLOORSMAX_MEDI                                      FLOORSMAX_AVG     0.997094\n",
       "ENTRANCES_MEDI                                      ENTRANCES_AVG     0.996868\n",
       "COMMONAREA_AVG                                    COMMONAREA_MEDI     0.996439\n",
       "ELEVATORS_MEDI                                      ELEVATORS_AVG     0.996105\n",
       "LIVINGAREA_MEDI                                    LIVINGAREA_AVG     0.995623\n",
       "BASEMENTAREA_MEDI                                BASEMENTAREA_AVG     0.995085\n",
       "APARTMENTS_MEDI                                    APARTMENTS_AVG     0.994987\n",
       "LIVINGAPARTMENTS_MEDI                        LIVINGAPARTMENTS_AVG     0.993817\n",
       "YEARS_BEGINEXPLUATATION_AVG          YEARS_BEGINEXPLUATATION_MEDI     0.993356\n",
       "LANDAREA_AVG                                        LANDAREA_MEDI     0.992257\n",
       "NONLIVINGAPARTMENTS_MEDI                  NONLIVINGAPARTMENTS_AVG     0.991873\n",
       "NONLIVINGAREA_MEDI                              NONLIVINGAREA_AVG     0.991335\n",
       "YEARS_BUILD_MODE                                  YEARS_BUILD_AVG     0.989513\n",
       "FLOORSMIN_MODE                                     FLOORSMIN_MEDI     0.988223\n",
       "FLOORSMAX_MODE                                     FLOORSMAX_MEDI     0.988045\n",
       "AMT_CREDIT                                        AMT_GOODS_PRICE     0.986451\n",
       "ELEVATORS_MODE                                     ELEVATORS_MEDI     0.982439\n",
       "NONLIVINGAPARTMENTS_MODE                 NONLIVINGAPARTMENTS_MEDI     0.981891\n",
       "ENTRANCES_MODE                                     ENTRANCES_MEDI     0.980663\n",
       "COMMONAREA_MODE                                    COMMONAREA_AVG     0.977694\n",
       "BASEMENTAREA_MODE                               BASEMENTAREA_MEDI     0.977032\n",
       "APARTMENTS_MODE                                   APARTMENTS_MEDI     0.976759\n",
       "x0^2                                                 EXT_SOURCE_1     0.976631\n",
       "LIVINGAPARTMENTS_MODE                       LIVINGAPARTMENTS_MEDI     0.976294\n",
       "x2^2                                                 EXT_SOURCE_3     0.975462\n",
       "NONLIVINGAREA_MODE                             NONLIVINGAREA_MEDI     0.975432\n",
       "x1^2                                                 EXT_SOURCE_2     0.974734\n",
       "LIVINGAREA_MODE                                   LIVINGAREA_MEDI     0.974149\n",
       "LANDAREA_MODE                                        LANDAREA_AVG     0.974084\n",
       "YEARS_BEGINEXPLUATATION_MODE          YEARS_BEGINEXPLUATATION_AVG     0.971959\n",
       "EMPLOYED_BIRTH_RATIO                                DAYS_EMPLOYED     0.954679\n",
       "REGION_RATING_CLIENT                  REGION_RATING_CLIENT_W_CITY     0.950318\n",
       "TOTALAREA_MODE                                    LIVINGAREA_MEDI     0.920174\n",
       "HOUSETYPE_MODE_block of flats              EMERGENCYSTATE_MODE_No     0.913905\n",
       "APARTMENTS_AVG                                    LIVINGAREA_MEDI     0.912065\n",
       "LIVINGAPARTMENTS_AVG                              APARTMENTS_MODE     0.906649"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Drop irrelevant columns\n",
    "app_df.drop(\n",
    "    columns=highly_decorrelated_from_target.index,\n",
    "    inplace=True,\n",
    "    errors=\"ignore\",\n",
    ")\n",
    "app_df.drop(\n",
    "    columns=highly_correlated.index,\n",
    "    inplace=True,\n",
    "    errors=\"ignore\",\n",
    ")\n",
    "\n",
    "app_df.describe(include=\"all\")\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>OWN_CAR_AGE</th>\n",
       "      <th>...</th>\n",
       "      <th>WALLSMATERIAL_MODE_Monolithic</th>\n",
       "      <th>WALLSMATERIAL_MODE_Panel</th>\n",
       "      <th>WALLSMATERIAL_MODE_Stone, brick</th>\n",
       "      <th>EMERGENCYSTATE_MODE_No</th>\n",
       "      <th>CREDIT_PRICE_RATIO</th>\n",
       "      <th>ANNUITY_CREDIT_RATIO</th>\n",
       "      <th>ANNUITY_INCOME_RATIO</th>\n",
       "      <th>x0 x1</th>\n",
       "      <th>x0 x2</th>\n",
       "      <th>x1 x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>252137.000000</td>\n",
       "      <td>252137</td>\n",
       "      <td>252125.000000</td>\n",
       "      <td>2.518810e+05</td>\n",
       "      <td>252137.000000</td>\n",
       "      <td>252137.000000</td>\n",
       "      <td>252137.000000</td>\n",
       "      <td>252137.000000</td>\n",
       "      <td>252137.000000</td>\n",
       "      <td>94413.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>252137</td>\n",
       "      <td>251881.000000</td>\n",
       "      <td>252125.000000</td>\n",
       "      <td>252125.000000</td>\n",
       "      <td>9.896300e+04</td>\n",
       "      <td>98963.000000</td>\n",
       "      <td>9.896300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>157719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>250616</td>\n",
       "      <td>197798</td>\n",
       "      <td>198699</td>\n",
       "      <td>131341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.086600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27812.325168</td>\n",
       "      <td>5.494057e+05</td>\n",
       "      <td>0.020894</td>\n",
       "      <td>-14769.133174</td>\n",
       "      <td>-2384.169325</td>\n",
       "      <td>-4635.430849</td>\n",
       "      <td>-2800.639724</td>\n",
       "      <td>11.950187</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.123500</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.177266</td>\n",
       "      <td>2.667788e-01</td>\n",
       "      <td>0.246540</td>\n",
       "      <td>2.643175e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.281248</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14647.759104</td>\n",
       "      <td>3.732685e+05</td>\n",
       "      <td>0.013874</td>\n",
       "      <td>3662.573769</td>\n",
       "      <td>2338.360162</td>\n",
       "      <td>3252.169156</td>\n",
       "      <td>1515.360629</td>\n",
       "      <td>11.981952</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.126625</td>\n",
       "      <td>0.022521</td>\n",
       "      <td>0.090983</td>\n",
       "      <td>1.558798e-01</td>\n",
       "      <td>0.152676</td>\n",
       "      <td>1.452604e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980.000000</td>\n",
       "      <td>4.050000e+04</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>-25200.000000</td>\n",
       "      <td>-17912.000000</td>\n",
       "      <td>-22928.000000</td>\n",
       "      <td>-7197.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.022073</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>5.740538e-07</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.255191e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17073.000000</td>\n",
       "      <td>2.475000e+05</td>\n",
       "      <td>0.010006</td>\n",
       "      <td>-17563.000000</td>\n",
       "      <td>-3175.000000</td>\n",
       "      <td>-6952.000000</td>\n",
       "      <td>-4177.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037060</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>1.397380e-01</td>\n",
       "      <td>0.123250</td>\n",
       "      <td>1.466391e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25834.500000</td>\n",
       "      <td>4.500000e+05</td>\n",
       "      <td>0.018850</td>\n",
       "      <td>-14573.000000</td>\n",
       "      <td>-1648.000000</td>\n",
       "      <td>-4265.000000</td>\n",
       "      <td>-2886.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.118800</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.160333</td>\n",
       "      <td>2.514975e-01</td>\n",
       "      <td>0.223662</td>\n",
       "      <td>2.585384e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35617.500000</td>\n",
       "      <td>6.930000e+05</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>-11775.000000</td>\n",
       "      <td>-767.000000</td>\n",
       "      <td>-1845.000000</td>\n",
       "      <td>-1487.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.198000</td>\n",
       "      <td>0.064314</td>\n",
       "      <td>0.224960</td>\n",
       "      <td>3.803050e-01</td>\n",
       "      <td>0.349307</td>\n",
       "      <td>3.744008e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258025.500000</td>\n",
       "      <td>4.050000e+06</td>\n",
       "      <td>0.072508</td>\n",
       "      <td>-7489.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.124430</td>\n",
       "      <td>1.451571</td>\n",
       "      <td>7.458840e-01</td>\n",
       "      <td>0.789032</td>\n",
       "      <td>6.977255e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               TARGET FLAG_OWN_CAR    AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "count   252137.000000       252137  252125.000000     2.518810e+05   \n",
       "unique            NaN            2            NaN              NaN   \n",
       "top               NaN        False            NaN              NaN   \n",
       "freq              NaN       157719            NaN              NaN   \n",
       "mean         0.086600          NaN   27812.325168     5.494057e+05   \n",
       "std          0.281248          NaN   14647.759104     3.732685e+05   \n",
       "min          0.000000          NaN    1980.000000     4.050000e+04   \n",
       "25%          0.000000          NaN   17073.000000     2.475000e+05   \n",
       "50%          0.000000          NaN   25834.500000     4.500000e+05   \n",
       "75%          0.000000          NaN   35617.500000     6.930000e+05   \n",
       "max          1.000000          NaN  258025.500000     4.050000e+06   \n",
       "\n",
       "        REGION_POPULATION_RELATIVE     DAYS_BIRTH  DAYS_EMPLOYED  \\\n",
       "count                252137.000000  252137.000000  252137.000000   \n",
       "unique                         NaN            NaN            NaN   \n",
       "top                            NaN            NaN            NaN   \n",
       "freq                           NaN            NaN            NaN   \n",
       "mean                      0.020894  -14769.133174   -2384.169325   \n",
       "std                       0.013874    3662.573769    2338.360162   \n",
       "min                       0.000290  -25200.000000  -17912.000000   \n",
       "25%                       0.010006  -17563.000000   -3175.000000   \n",
       "50%                       0.018850  -14573.000000   -1648.000000   \n",
       "75%                       0.028663  -11775.000000    -767.000000   \n",
       "max                       0.072508   -7489.000000       0.000000   \n",
       "\n",
       "        DAYS_REGISTRATION  DAYS_ID_PUBLISH   OWN_CAR_AGE  ...  \\\n",
       "count       252137.000000    252137.000000  94413.000000  ...   \n",
       "unique                NaN              NaN           NaN  ...   \n",
       "top                   NaN              NaN           NaN  ...   \n",
       "freq                  NaN              NaN           NaN  ...   \n",
       "mean         -4635.430849     -2800.639724     11.950187  ...   \n",
       "std           3252.169156      1515.360629     11.981952  ...   \n",
       "min         -22928.000000     -7197.000000      0.000000  ...   \n",
       "25%          -6952.000000     -4177.000000      5.000000  ...   \n",
       "50%          -4265.000000     -2886.000000      9.000000  ...   \n",
       "75%          -1845.000000     -1487.000000     15.000000  ...   \n",
       "max              0.000000         0.000000     91.000000  ...   \n",
       "\n",
       "       WALLSMATERIAL_MODE_Monolithic WALLSMATERIAL_MODE_Panel  \\\n",
       "count                         252137                   252137   \n",
       "unique                             2                        2   \n",
       "top                            False                    False   \n",
       "freq                          250616                   197798   \n",
       "mean                             NaN                      NaN   \n",
       "std                              NaN                      NaN   \n",
       "min                              NaN                      NaN   \n",
       "25%                              NaN                      NaN   \n",
       "50%                              NaN                      NaN   \n",
       "75%                              NaN                      NaN   \n",
       "max                              NaN                      NaN   \n",
       "\n",
       "        WALLSMATERIAL_MODE_Stone, brick  EMERGENCYSTATE_MODE_No  \\\n",
       "count                            252137                  252137   \n",
       "unique                                2                       2   \n",
       "top                               False                    True   \n",
       "freq                             198699                  131341   \n",
       "mean                                NaN                     NaN   \n",
       "std                                 NaN                     NaN   \n",
       "min                                 NaN                     NaN   \n",
       "25%                                 NaN                     NaN   \n",
       "50%                                 NaN                     NaN   \n",
       "75%                                 NaN                     NaN   \n",
       "max                                 NaN                     NaN   \n",
       "\n",
       "       CREDIT_PRICE_RATIO ANNUITY_CREDIT_RATIO ANNUITY_INCOME_RATIO  \\\n",
       "count       251881.000000        252125.000000        252125.000000   \n",
       "unique                NaN                  NaN                  NaN   \n",
       "top                   NaN                  NaN                  NaN   \n",
       "freq                  NaN                  NaN                  NaN   \n",
       "mean             1.123500             0.053892             0.177266   \n",
       "std              0.126625             0.022521             0.090983   \n",
       "min              0.150000             0.022073             0.000224   \n",
       "25%              1.000000             0.037060             0.112500   \n",
       "50%              1.118800             0.050000             0.160333   \n",
       "75%              1.198000             0.064314             0.224960   \n",
       "max              6.000000             0.124430             1.451571   \n",
       "\n",
       "               x0 x1         x0 x2         x1 x2  \n",
       "count   9.896300e+04  98963.000000  9.896300e+04  \n",
       "unique           NaN           NaN           NaN  \n",
       "top              NaN           NaN           NaN  \n",
       "freq             NaN           NaN           NaN  \n",
       "mean    2.667788e-01      0.246540  2.643175e-01  \n",
       "std     1.558798e-01      0.152676  1.452604e-01  \n",
       "min     5.740538e-07      0.000014  3.255191e-07  \n",
       "25%     1.397380e-01      0.123250  1.466391e-01  \n",
       "50%     2.514975e-01      0.223662  2.585384e-01  \n",
       "75%     3.803050e-01      0.349307  3.744008e-01  \n",
       "max     7.458840e-01      0.789032  6.977255e-01  \n",
       "\n",
       "[11 rows x 93 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Après simplification, nous voyons que nous avons drastiquement réduit le nombre de variables pour ne conserver que celles réellement pertinentes pour la prédiction de `TARGET`.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Préparation des données\n",
    "\n",
    "Nous allons transformer les données pour que nos modèles puissent les exploiter au mieux. Afin d'éviter la \"fuite d'information\" entre le jeu de données d'entraînement et de test, nous allons maintenant séparer notre jeu de données en deux. Les transformations seront apprises uniquement sur le jeu d'entraînement, mais appliquées aux deux jeux de données (entraînement et test).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Given data\n",
    "X = app_df.drop([\"TARGET\"], axis=1)\n",
    "# Data to predict\n",
    "y = app_df[\"TARGET\"]\n",
    "\n",
    "del app_df\n",
    "\n",
    "# Let's split the whole dataset into a training set (80% of data) and a test set (20% of data)\n",
    "# The dataset will be split in a stratified way, in order to have a good distribution of the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y , random_state=42)\n",
    "\n",
    "# Save the processed data\n",
    "X_train.to_csv(\"../data/processed/X_train.csv\", index=False)\n",
    "X_test.to_csv(\"../data/processed/X_test.csv\", index=False)\n",
    "y_train.to_csv(\"../data/processed/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"../data/processed/y_test.csv\", index=False)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalisation des données\n",
    "\n",
    "Afin d'éviter que certains modèles pondèrent l'importance de certaines variables à cause de leur ordre de grandeur, nous allons normaliser chaque variable afin de les ramener à une moyenne nulle et une variance de 1.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## StandardScaler\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "if os.path.exists(\"../data/processed/X_train_scaled.csv\") and os.path.exists(\"../data/processed/X_test_scaled.csv\"):\n",
    "    X_train = pd.read_csv(\"../data/processed/X_train_scaled.csv\")\n",
    "    X_test = pd.read_csv(\"../data/processed/X_test_scaled.csv\")\n",
    "else:\n",
    "    # define scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # fit scaler on train data only, to avoid data leak\n",
    "    X_train = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train),\n",
    "        columns=X.columns,\n",
    "    )\n",
    "    X_test = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=X.columns,\n",
    "    )\n",
    "\n",
    "    # Save the processed data\n",
    "    X_train.to_csv(\"../data/processed/X_train_scaled.csv\", index=False)\n",
    "    X_test.to_csv(\"../data/processed/X_test_scaled.csv\", index=False)\n",
    "\n",
    "X_train.describe(include=\"all\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous avons ici équilibré les ordres de grandeur de chaque variables, afin que nos futurs modèles ne soient pas influencés par leur différence.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imputation des valeurs manquantes\n",
    "\n",
    "Afin d'éviter que certains modèles ne puissent être utilisés à cause des valeurs manquantes, nous allons remplacer toutes les valeurs nulles par leur meilleure estimation possible.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## IterativeImputer\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html\n",
    "# Impute missing values by modeling each feature with missing values as a function of other features in a round-robin fashion\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "if os.path.exists(\"../data/processed/X_train_imputed.csv\") and os.path.exists(\"../data/processed/X_test_imputed.csv\"):\n",
    "    X_train = pd.read_csv(\"../data/processed/X_train_imputed.csv\")\n",
    "    X_test = pd.read_csv(\"../data/processed/X_test_imputed.csv\")\n",
    "else:\n",
    "    # define imputer\n",
    "    imputer = IterativeImputer(\n",
    "        n_nearest_features=min(5, int(len(X.columns) / 10)),\n",
    "    )\n",
    "\n",
    "    # fit imputer on train data only, to avoid data leak\n",
    "    X_train = pd.DataFrame(\n",
    "        imputer.fit_transform(X_train),\n",
    "        columns=X.columns,\n",
    "    )\n",
    "    X_test = pd.DataFrame(\n",
    "        imputer.transform(X_test),\n",
    "        columns=X.columns,\n",
    "    )\n",
    "\n",
    "    # Save the processed data\n",
    "    X_train.to_csv(\"../data/processed/X_train_imputed.csv\", index=False)\n",
    "    X_test.to_csv(\"../data/processed/X_test_imputed.csv\", index=False)\n",
    "\n",
    "X_train.describe(include=\"all\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ré-équilibrage des classes de TARGET\n",
    "\n",
    "Comme nous l'avons vu, nos classes de TARGET sont largement déséquilibrées, ce qui va introduire un fort biais dans l'apprentissage de nos modèles.\n",
    "Nous allons donc ré-équilibrer ces classes dans nos jeux d'apprentissage afin de palier à ce problème.\n",
    "\n",
    "L'algorithme utilisé va dans un premier temps générer plus de données de la classe sous-représentée (SMOTE: Synthetic Minority Over-sampling Technique), puis éliminer certains individus de la classe sur-représentées (ENN : Edited Nearest Neighbours).\n",
    "\n",
    "![Over- and Under-sampling](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## SMOTEENN re-sampler\n",
    "# https://imbalanced-learn.org/stable/references/generated/imblearn.combine.SMOTEENN.html\n",
    "# Combine over- and under-sampling using SMOTE and Edited Nearest Neighbours.\n",
    "\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "\n",
    "if False and os.path.exists(\"../data/processed/X_train_balanced.csv\") and os.path.exists(\"../data/processed/y_train_balanced.csv\"):\n",
    "    X_train = pd.read_csv(\"../data/processed/X_train_balanced.csv\")\n",
    "    y_train = pd.read_csv(\"../data/processed/y_train_balanced.csv\")\n",
    "else:\n",
    "    # define sampler\n",
    "    smote_enn = SMOTEENN(\n",
    "        sampling_strategy='all',\n",
    "        n_jobs=-1, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Let's both oversample the minority class and undersample the majority class\n",
    "    X_train, y_train = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Save the processed data\n",
    "    X_train.to_csv(\"../data/processed/X_train_balanced.csv\", index=False)\n",
    "    y_train.to_csv(\"../data/processed/y_train_balanced.csv\", index=False)\n",
    "\n",
    "X_train.describe(include=\"all\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's plot the distribution of the TARGET variable\n",
    "if True or DRAW_PLOTS:\n",
    "    fig = px.bar(\n",
    "        y_train[\"TARGET\"].replace({\n",
    "            \"0\": \"TARGET=0 : payments OK\", \n",
    "            \"1\": \"TARGET=1 : payment difficulties\", \n",
    "        }).value_counts(),\n",
    "        title=\"Distribution of TARGET variable\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "    ).update_xaxes(\n",
    "        title=\"TARGET\",\n",
    "    ).update_yaxes(title=\"Count\")\n",
    "    fig.show()\n",
    "\n",
    "target_count = y_train[\"TARGET\"].value_counts()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyse en composantes principales (PCA)\n",
    "\n",
    "Avant de modéliser et prédire la valeur de TARGET, essayons de voir comment sont répartis nos individus dans le plan défini par les deux composantes principales. Ceci nous permettra de visualiser la distribution de nos données et des varaibles dans l'espace des composantes principales et d'évaluer la difficulté de la classification.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's plot the data and features in the 2D PCA plane\n",
    "if True or DRAW_PLOTS:\n",
    "    vis_helpers.plot_pca_2d(X_train, y_train)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modélisation et évaluation des modèles\n",
    "\n",
    "Nous allons ici entraîner différents modèles de classification binaire et évaluer leur efficacité selon trois critères :\n",
    "- le **temps d'exécution** : temps nécessaire au modèle pour les phases d'apprentissage (`fit`) et de prédiction (`predict`).\n",
    "- l'**interprétabilité** du modèle : la capacité du modèle à nous indiquer quelles sont les variables qui expliquent le mieux la prédiction.\n",
    "- la **performance** : nous allons mesurer différentes métriques (`accuracy` = taux de prédictions positives ou négatives correctes, `precision` = taux de prédictions positives correctes, `recall` = taux de positifs correctement prédits, `F1 score` = moyenne harmonique de la `precision` et du `recall`) pour chaque modèle. Nous utiliserons le `F1 score` comme mesure principale de la performance, car nous sommes face à un problème où les deux classes de la variable prédite ne sont pas équilibrées (`TARGET=0 ~ 92%`) dans le jeu de données, et nous voulons maximiser en priorité le `recall` (bien prédire les problèmes de paiement). Nous tracerons les courbes de `Precision-Recall` et de `ROC (Receiver operating characteristic)` qui compare le `recall` au taux de faux positifs.\n",
    "\n",
    "Afin d'améliorer la qualité du modèle et minimiser le biais de sélection du jeu d'entraînement, nous allons utiliser la méthode `StratifiedKFold()` qui permet de \"mixer\" les résultats sur plusieurs sous-ensembles du jeu d'entraînement.\n",
    "\n",
    "Afin de chercher les meilleurs hyper-paramètres de chacun des modèles, nous allons utiliser la recherche `HalvingRandomSearchCV()` qui permet d'éliminer des combinaisons d'hyper-paramètres en plusieurs étapes, et d'optimiser les ressources allouées à chaque étape.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# let's store the results of each model in a DataFrame\n",
    "results = pd.DataFrame(columns=(\n",
    "    'model',\n",
    "    'params',\n",
    "    'score',\n",
    "    'predict_time',\n",
    "    'cv_results_',\n",
    "    'best_index_',\n",
    "    'confusion_matrix',\n",
    "    'f1',\n",
    "    'accuracy',\n",
    "    'precision',\n",
    "    'recall',\n",
    "    'average_precision',\n",
    "    'precision_recall_curve',\n",
    "    'roc_auc_score',\n",
    "    'roc_curve',\n",
    "))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèle de référence\n",
    "\n",
    "Nous allons utiliser un modèle bête et méchant qui se contente de faire des prédictions aléatoires.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Dummy Classifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html\n",
    "\n",
    "# DummyClassifier permet de faire des prédictions aléatoires.\n",
    "\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "best_model_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    estimator=DummyClassifier(random_state=42),\n",
    "    params={\n",
    "        'strategy': ['stratified', 'most_frequent', 'prior', 'uniform']\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": best_model_score['params'], \n",
    "    \"score\": best_model_score['score'],\n",
    "    \"predict_time\": best_model_score['predict_time'],\n",
    "})\n",
    "results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=best_model_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b\n",
    "r\n",
    "e\n",
    "a\n",
    "k"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèle linéaire\n",
    "\n",
    "Nous allons utiliser un modèle linéaire\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Ridge Classifier (linear model)\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html\n",
    "# RidgeClassifier is a linear model with a linear loss function. It is useful for regularized linear models.\n",
    "\n",
    "# - La norme ℓ2 du vecteur de poids peut être utilisée comme terme de régularisation de la régression linéaire.\n",
    "# - Cela s'appelle la régularisation de Tykhonov, ou régression ridge.\n",
    "# - La régression ridge admet toujours une solution analytique unique.\n",
    "# - La régression ridge permet d'éviter le surapprentissage en restraignant l'amplitude des poids.\n",
    "# - La régression ridge a un effet de sélection groupée : les variables corrélées ont le même coefficient.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "best_model_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    estimator=RidgeClassifier(random_state=42),\n",
    "    params={\n",
    "        'alpha': np.logspace(-10, 10, 50),\n",
    "        'fit_intercept': [True, False],\n",
    "        'normalize': [True, False],\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": best_model_score['params'], \n",
    "    \"score\": best_model_score['score'],\n",
    "    \"predict_time\": best_model_score['predict_time'],\n",
    "})\n",
    "results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=best_model_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top_coefficients = pd.Series(\n",
    "    best_model_score[\n",
    "        'model'\n",
    "    ].coef_[0],\n",
    "    X_train.columns,\n",
    ").map(abs).sort_values(ascending=False).head(20)\n",
    "\n",
    "if DRAW_PLOTS:\n",
    "    fig = px.bar(\n",
    "        top_coefficients,\n",
    "        color=top_coefficients.values,\n",
    "        title=\"Top 20 variables importance\",\n",
    "        labels={\n",
    "            \"index\": \"Variable name\",\n",
    "            \"value\": \"Coefficient\",\n",
    "            \"color\": \"Coefficient\",\n",
    "        },\n",
    "        width=1200,\n",
    "        height=800,\n",
    "    )\n",
    "    fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèle Logistique\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Logistic Regression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# Logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix\n",
    "# and a bias vector. It is also able to fit non-linear decision boundaries, such as the\n",
    "# logistic function found in Support Vector Machines.\n",
    "\n",
    "# - La régression logistique modélise la probabilité qu'une observation appartienne à la classe positive comme une transformation logistique d'une combinaison linéaire des variables.\n",
    "# - Les coefficients d'une régression logistique s'apprennent par maximisation de vraisemblance, mais il n'existe pas de solution explicite.\n",
    "# - La vraisemblance est convexe, et de nombreux solveurs peuvent être utilisés pour trouver une solution numérique.\n",
    "# - Les concepts de régularisation ℓ1 et ℓ2 s'appliquent aussi à la régression logistique.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "best_model_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    estimator=LogisticRegression(random_state=42),\n",
    "    params={\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'C': np.logspace(-8, 1, 20),\n",
    "        'l1_ratio': np.linspace(0.00001, 0.99999, 10),\n",
    "        'solver': ['saga'],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": best_model_score['params'], \n",
    "    \"score\": best_model_score['score'],\n",
    "    \"predict_time\": best_model_score['predict_time'],\n",
    "})\n",
    "results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=best_model_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top_coefficients = pd.Series(\n",
    "    best_model_score[\n",
    "        'model'\n",
    "    ].coef_[0],\n",
    "    X_train.columns,\n",
    ").map(abs).sort_values(ascending=False).head(20)\n",
    "\n",
    "if True or DRAW_PLOTS:\n",
    "    fig = px.bar(\n",
    "        top_coefficients,\n",
    "        color=top_coefficients.values,\n",
    "        title=\"Top 20 variables importance\",\n",
    "        labels={\n",
    "            \"index\": \"Variable name\",\n",
    "            \"value\": \"Coefficient\",\n",
    "            \"color\": \"Coefficient\",\n",
    "        },\n",
    "        width=1200,\n",
    "        height=800,\n",
    "    )\n",
    "    fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "c_range = np.logspace(-8, 1, 20)\n",
    "l1_ratio = best_model_score['params']['l1_ratio']\n",
    "\n",
    "coefficients = pd.DataFrame(index=X_train.columns, columns=c_range)\n",
    "errors = []\n",
    "for c in c_range:\n",
    "    logistic = LogisticRegression(C=c, l1_ratio=l1_ratio)\n",
    "    logistic.fit(X_train, y_train)\n",
    "    coefficients.loc[:, c] = logistic.coef_[0]\n",
    "    errors.append(mean_squared_error(y_test, logistic.predict(X_test)))\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "for col in coefficients.index:\n",
    "    fig.add_trace(go.Scatter(x=c_range, y=coefficients.loc[col, :], name=col,))\n",
    "\n",
    "fig.update_xaxes(type=\"log\", autorange=\"reversed\")\n",
    "fig.update_layout(\n",
    "    title=\"Logistic regression coefficients as a function of the regularization\",\n",
    "    xaxis_title=\"log(C)\",\n",
    "    yaxis_title=\"coefficient\",\n",
    "    width=1200,\n",
    "    height=800,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=c_range, y=errors, name=\"MSE\"))\n",
    "fig.update_xaxes(type=\"log\", autorange=\"reversed\")\n",
    "fig.update_layout(\n",
    "    title=\"Logistic regression MSE as a function of the regularization\",\n",
    "    xaxis_title=\"log(C)\",\n",
    "    yaxis_title=\"MSE\",\n",
    "    width=1200,\n",
    "    height=800,\n",
    ")\n",
    "fig.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèle Support Vector Machine \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Support Vector Machine\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "# Support Vector Machines (SVMs) are supervised learning models that learn to classify data points according to a set of hyperplanes.\n",
    "# The model is based on the idea that the data points are separated by a hyperplane, and the hyperplane is the only way to classify the data points.\n",
    "# The model is a linear classifier, meaning that it makes a linear decision boundary.\n",
    "# The model is non-parametric, meaning that it does not require any parameters to be learned.\n",
    "# The model is usually a good choice for handling unbalanced classes.\n",
    "# The model is fast to train, and can be used for large datasets.\n",
    "\n",
    "# Linear SVC\n",
    "# - Les SVM (Support Vector Machines), aussi appelées en français Machines à Vecteurs de Support et parfois Séparatrices à Vaste Marge, cherchent à séparer linéairement les données.\n",
    "# - La version primale résout un problème d'optimisation à p variables et est donc préférable si on a moins de variables que d'échantillons.\n",
    "# - À l'inverse, la version duale résout un problème d'optimisation à n variables et est donc préférable si on a moins d'échantillons que de variables.\n",
    "# - Les vecteurs de support sont les points du jeu de données qui sont les plus proches de l'hyperplan séparateur.\n",
    "# - La fonction de décision peut s'exprimer uniquement en fonction du produit scalaire du point à étiqueter avec les vecteurs de support.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "best_model_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    estimator=SVC(\n",
    "        cache_size=1000, \n",
    "        max_iter=1e4,\n",
    "        random_state=42\n",
    "    ),\n",
    "    params={\n",
    "        \"C\": np.logspace(-2, 2, 5),\n",
    "        \"gamma\": [\"scale\", \"auto\"] | np.logspace(-2, 2, 5),\n",
    "        \"degree\": range(2, 5),\n",
    "        \"kernel\": [\"linear\", \"poly\", \"rbf\"],\n",
    "        \"class_weight\": [None, 'balanced'],\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": best_model_score['params'], \n",
    "    \"score\": best_model_score['score'],\n",
    "    \"predict_time\": best_model_score['predict_time'],\n",
    "})\n",
    "results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=best_model_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# if the kernel is linear, we can use the coefficients to plot the decision boundary\n",
    "if best_model_score['params']['kernel'] == 'linear':\n",
    "    top_coefficients = pd.Series(\n",
    "        best_model_score['model'].coef_[0],\n",
    "        X_train.columns,\n",
    "    ).map(abs).sort_values(ascending=False).head(20)\n",
    "\n",
    "    if True or DRAW_PLOTS:\n",
    "        fig = px.bar(\n",
    "            top_coefficients,\n",
    "            color=top_coefficients.values,\n",
    "            title=\"Top 20 variables importance\",\n",
    "            labels={\n",
    "                \"index\": \"Variable name\",\n",
    "                \"value\": \"Coefficient\",\n",
    "                \"color\": \"Coefficient\",\n",
    "            },\n",
    "            width=1200,\n",
    "            height=800,\n",
    "        )\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"Kernel is not linear, impossible to estimate feature importances.\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K nearest neighbours\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## K-nearest-neighbours\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "# K-nearest neighbors (KNN) is a non-parametric method used for classification and regression.\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "best_model_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    params={\n",
    "        \"n_neighbors\": range(2, 20, 3),\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": best_model_score['params'], \n",
    "    \"score\": best_model_score['score'],\n",
    "    \"predict_time\": best_model_score['predict_time'],\n",
    "})\n",
    "results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=best_model_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèles ensemblistes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Bagging\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "# Bagging is a meta-estimator that fits base learners on random subsets of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n",
    "# The base estimator is a decision tree.\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "best_model_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    estimator=BaggingClassifier(random_state=42),\n",
    "    params={\n",
    "        \"base_estimator\": [\n",
    "            DecisionTreeClassifier(),\n",
    "            DecisionTreeClassifier(max_depth=2),\n",
    "            DecisionTreeClassifier(max_depth=10),\n",
    "            ElasticNet(),\n",
    "            SVC(),\n",
    "        ],\n",
    "        \"n_estimators\": np.logspace(1, 3, 10).astype(int),\n",
    "        \"max_samples\": np.logspace(1, 4, 10).astype(int),\n",
    "        \"max_features\": np.linspace(5, int(len(X_train.columns)/2), 5).astype(int),\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": best_model_score['params'], \n",
    "    \"score\": best_model_score['score'],\n",
    "    \"predict_time\": best_model_score['predict_time'],\n",
    "})\n",
    "results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=best_model_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Random Forest\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "# Random forest is a meta-estimator that fits a number of classifical decision trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n",
    "# The base estimator is a decision tree.\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "best_model_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    params={\n",
    "        \"n_estimators\": np.logspace(1, 3, 10).astype(int),\n",
    "        \"max_depth\": [None, 2, 10],\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "        \"class_weight\": [None, 'balanced_subsample', 'balanced'],\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": best_model_score['params'], \n",
    "    \"score\": best_model_score['score'],\n",
    "    \"predict_time\": best_model_score['predict_time'],\n",
    "})\n",
    "results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=best_model_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## XGBoost\n",
    "# https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "# XGBoost is a fast, scalable, high-performance gradient boosting framework.\n",
    "# It supports both classification and regression.\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "best_model_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    estimator=XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        scale_pos_weight=float(target_count[0]) / target_count[1],\n",
    "        random_state=42,\n",
    "    ),\n",
    "    params={\n",
    "        \"n_estimators\": np.logspace(1, 3, 5).astype(int),\n",
    "        \"learning_rate\": np.linspace(0.01, 0.5, 5),\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": best_model_score['params'], \n",
    "    \"score\": best_model_score['score'],\n",
    "    \"predict_time\": best_model_score['predict_time'],\n",
    "})\n",
    "results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=best_model_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LGBMClassifier\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## LGBMClassifier\n",
    "# https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "# LightGBM is a fast, scalable, high performance gradient boosting library.\n",
    "# It supports both classification and regression.\n",
    "\n",
    "\n",
    "import re\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "X_train = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "X_test = X_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "best_model_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    estimator=LGBMClassifier(random_state=42),\n",
    "    params={\n",
    "        \"n_estimators\": np.logspace(1, 3, 5).astype(int),\n",
    "        \"learning_rate\": np.linspace(0.01, 0.5, 5),\n",
    "        \"is_unbalance\": [True, False],\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": best_model_score['params'], \n",
    "    \"score\": best_model_score['score'],\n",
    "    \"predict_time\": best_model_score['predict_time'],\n",
    "})\n",
    "results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=best_model_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLPClassifier\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Neural Network\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "# Neural network is a supervised learning model that can learn nonlinear functions.\n",
    "# It is a generalization of logistic regression, which is used for binary classification.\n",
    "\n",
    "# - Le perceptron permet d'apprendre des modèles paramétriques basés sur une combinaison linéaire des variables.\n",
    "# - Le perceptron permet d'apprendre des modèles de régression (la fonction d'activation est l'identité), de classification binaire (la fonction d'activation est la fonction logistique) ou de classification multi-classe (la fonction d'activation est la fonction softmax).\n",
    "# - Le perceptron est entraîné par des mises à jour itératives de ses poids grâce à l'algorithme du gradient. La même règle de mise à jour des poids s'applique dans le cas de la régression, de la classification binaire ou de la classification multi-classe.\n",
    "# - Empiler des perceptrons en un réseau de neurones multi-couches (feed-forward) permet de modéliser des fonctions arbitrairement complexes. C'est ce qui donne aux réseaux de neurones profonds la puissance prédictive qui fait actuellement leur succès.\n",
    "# - L'entraînement de ces réseaux se fait par rétro-propagation. Attention, cet algorithme ne converge pas nécessairement, et pas nécessairement vers la solution optimale !\n",
    "# - Plus il y a de paramètres (i.e. de poids de connexion), plus il faut de données pour pouvoir apprendre les valeurs de ces paramètres sans risquer le sur-apprentissage.\n",
    "# - Il existe de nombreuses autres architectures de réseaux de neurones que celle présentée ici, et qui permettent de modéliser des types de données particuliers (images, sons, dépendances temporelles...).\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "best_model_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    estimator=MLPClassifier(random_state=42),\n",
    "    params={\n",
    "        \"hidden_layer_sizes\": [(2,), (2,3,), (2,3,5,), (2,5,3,), (3,), (3,5,), (5,3,), (5,), (10,), (20,)],\n",
    "        \"alpha\": np.logspace(-5, 1, 5),\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": best_model_score['params'], \n",
    "    \"score\": best_model_score['score'],\n",
    "    \"predict_time\": best_model_score['predict_time'],\n",
    "})\n",
    "results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=best_model_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## AutoML\n",
    "# https://automl.github.io/auto-sklearn/\n",
    "# AutoML is a machine learning library that automatically tunes the hyperparameters of a machine learning model.\n",
    "# It is a wrapper around scikit-learn, which is a machine learning library.\n",
    "\n",
    "\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
    "from autosklearn.metrics import roc_auc, f1\n",
    "\n",
    "\n",
    "clf = AutoSklearn2Classifier(\n",
    "    time_left_for_this_task=120, \n",
    "    metric=f1,\n",
    "    n_jobs=4,\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_proba = clf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=clf,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Annexe\n",
    "\n",
    "Les Notebooks Kaggle [Introduction: Home Credit Default Risk Competition](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction) (et suivants) de [Will Koehrsen](https://www.kaggle.com/willkoehrsen) ont été d'une très grande aide dans l'exploration des données.\n",
    "\n",
    "Comment bien choisir la métrique de score d'un algorithme de classification ?\n",
    "\n",
    "![How to Choose a Metric for Imbalanced Classification](https://machinelearningmastery.com/wp-content/uploads/2019/12/How-to-Choose-a-Metric-for-Imbalanced-Classification-latest.png)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87142f5ce831c365860d10ab1d6251a6bfb068b987aa68f91a7996e4fcdedd96"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit (conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}