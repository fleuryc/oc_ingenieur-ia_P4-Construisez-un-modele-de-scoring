{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prêt à dépenser : Construire un modèle de scoring\n",
    "\n",
    "\n",
    "## Contexte\n",
    "\n",
    "\"Prêt à dépenser\" (Home Credit) est une société financière qui propose des crédits à la consommation pour des personnes ayant peu ou pas d'historique de prêt.\n",
    "Pour accorder un crédit à la consommation, l'entreprise calcule la probabilité qu'un client le rembourse, ou non. Elle souhaite donc développer un algorithme de scoring pour aider à décider si un prêt peut être accordé à un client.\n",
    "\n",
    "Les chargés de relation client seront les utilisateurs du modèle de scoring. Puisqu'ils s'adressent aux clients, ils ont besoin que votre modèle soit facilement interprétable. Les chargés de relation souhaitent, en plus, disposer d'une mesure de l'importance des variables qui ont poussé le modèle à donner cette probabilité à un client.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chargement des modules du projet\n",
    "\n",
    "Afin de simplifier le Notebook, le code métier du projet est placé dans le répertoire [src/](../src/).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import project modules from source directory\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "# System modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Append source directory to system path\n",
    "src_path = os.path.abspath(os.path.join(\"../src\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Helper functions\n",
    "import data.helpers as data_helpers\n",
    "import features.helpers as feat_helpers\n",
    "import visualization.helpers as vis_helpers\n",
    "import models.helpers as models_helpers\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons utiliser le langage [Python](https://www.python.org/about/gettingstarted/), et présenter ici le code, les résultats et l'analyse sous forme de [Notebook JupyterLab](https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html).\n",
    "\n",
    "Nous allons aussi utiliser les bibliothèques usuelles d'exploration et analyse de données, afin d'améliorer la simplicité et la performance de notre code :\n",
    "  * [NumPy](https://numpy.org/doc/stable/user/quickstart.html) et [Pandas](https://pandas.pydata.org/docs/user_guide/index.html) : effectuer des calculs scientifiques (statistiques, algèbre, ...) et manipuler des séries et tableaux de données volumineuses et complexes\n",
    "  * [scikit-learn](https://scikit-learn.org/stable/getting_started.html) et [XGBoost](https://xgboost.readthedocs.io/en/latest/get_started.html) : pour effectuer des analyses prédictives \n",
    "  * [Matplotlib](https://matplotlib.org/stable/tutorials/introductory/usage.html), [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html), [Seaborn](https://seaborn.pydata.org/tutorial/function_overview.html) et [Plotly](https://plotly.com/python/getting-started/) : générer des graphiques lisibles, intéractifs et pertinents\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# sklearn tools for model evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "## Necessary to export to HTML\n",
    "# import plotly.io as pio\n",
    "# pio.renderers.default='jupyterlab' # 'notebook' for Jupyter Notebook\n",
    "# import plotly as py\n",
    "# py.offline.init_notebook_mode()\n",
    "\n",
    "\n",
    "# Accelerate the development cycle\n",
    "SAMPLE_FRAC: float = 1\n",
    "\n",
    "# Prevent excessive memory usage used by plotly\n",
    "DRAW_PLOTS: bool = True\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chargement des données\n",
    "\n",
    "Les données mises à disposition sont issues de [Home Credit](https://www.homecredit.net/) et plus précisément de la compétition hébergée sur Kaggle [Home Credit Default Risk - Can you predict how capable each applicant is of repaying a loan?](https://www.kaggle.com/c/home-credit-default-risk)\n",
    "\n",
    "Les données sont fournies sous la forme de plusieurs fichiers CSV pouvant être liés entre eux de la manière suivante :\n",
    "\n",
    "![Home Credit data relations](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)\n",
    "\n",
    "_source : [Introduction: Home Credit Default Risk Competition](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction) by [Will Koehrsen](https://www.kaggle.com/willkoehrsen)_\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Download and extract the raw data\n",
    "data_helpers.download_extract_zip(\n",
    "    zip_file_url=\"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/Projet+-+Impl%C3%A9menter+un+mod%C3%A8le+de+scoring/Projet+Mise+en+prod+-+home-credit-default-risk.zip\",\n",
    "    files_names=(\n",
    "        \"application_test.csv\",\n",
    "        \"application_train.csv\",\n",
    "        \"bureau_balance.csv\",\n",
    "        \"bureau.csv\",\n",
    "        \"credit_card_balance.csv\",\n",
    "        \"installments_payments.csv\",\n",
    "        \"POS_CASH_balance.csv\",\n",
    "        \"previous_application.csv\",\n",
    "    ),\n",
    "    target_path=\"../data/raw/\",\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons charger toutes les données du fichier `application_train.csv` afin de travailler préparer les données du jeu d'entraînement et de test (variable `TARGET` vaut `O` : le client n'a pas fait défaut ou `1` : le client a fait défaut). Nous les séparerons les jeux de données au moment de l'entraînement et évaluation de nos modèles.\n",
    "\n",
    "Les fichiers contiennent un grand nombre de variables booléennes et catégorielles que nous pouvons déjà typer comme telles.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read column names\n",
    "application_train_column_names = pd.read_csv(\n",
    "    \"../data/raw/application_train.csv\", nrows=0\n",
    ").columns.values\n",
    "\n",
    "# TARGET variable must be present in the Train datase\n",
    "if \"TARGET\" not in application_train_column_names:\n",
    "    raise ValueError(\n",
    "        \"TARGET column not found in application_train.csv. Please check that the file is not corrupted.\"\n",
    "    )\n",
    "\n",
    "# SK_ID_CURR variable must be present in the Train datase\n",
    "if \"SK_ID_CURR\" not in application_train_column_names:\n",
    "    raise ValueError(\n",
    "        \"SK_ID_CURR column not found in application_train_column_names.csv. Please check that the file is not corrupted.\"\n",
    "    )\n",
    "\n",
    "# Set column types according to fields description (../data/raw/HomeCredit_columns_description.csv)\n",
    "# Categorical variables\n",
    "column_types = {\n",
    "    col: \"category\"\n",
    "    for col in application_train_column_names\n",
    "    if col.startswith((\"NAME_\",))\n",
    "    or col.endswith((\"_TYPE\"))\n",
    "    or col\n",
    "    in [\n",
    "        \"CODE_GENDER\",\n",
    "        \"WEEKDAY_APPR_PROCESS_START\",\n",
    "        \"FONDKAPREMONT_MODE\",\n",
    "        \"HOUSETYPE_MODE\",\n",
    "        \"WALLSMATERIAL_MODE\",\n",
    "        \"EMERGENCYSTATE_MODE\",\n",
    "    ]\n",
    "}\n",
    "# Boolean variables\n",
    "column_types |= {\n",
    "    col: bool\n",
    "    for col in application_train_column_names\n",
    "    if col.startswith((\"FLAG_\", \"REG_\", \"LIVE_\"))\n",
    "}\n",
    "\n",
    "# Load application data\n",
    "app_df = pd.read_csv(\n",
    "    \"../data/raw/application_train.csv\",\n",
    "    dtype=column_types,\n",
    "    true_values=[\"Y\", \"Yes\", \"1\"],\n",
    "    false_values=[\"N\", \"No\", \"0\"],\n",
    "    na_values=[\"XNA\"], # bad values\n",
    ")\n",
    "\n",
    "# Sample to speed up development\n",
    "if  float == type(SAMPLE_FRAC) and 0 < SAMPLE_FRAC < 1:\n",
    "    app_df = app_df.sample(frac=SAMPLE_FRAC)\n",
    "\n",
    "# Let's display basic statistical info about the data\n",
    "app_df.describe(include=\"all\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "app_df.info()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le jeu de données contient 122 variables, dont la variable cible que nous devons estimer : `TARGET`. Parmis ces variables, nous avons :\n",
    "- 34 variables booléennes\n",
    "- 14 variables catégorielles\n",
    "- 74 variables numériques"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse exploratoire\n",
    "\n",
    "Nous allons analyser la distribution de quelques variables.\n",
    "\n",
    "### Variable cible\n",
    "\n",
    "Voyons spécifiquement la distribution de la variable `TARGET` qui est celle que nous devrons estimer par la suite.\n",
    "Les valeurs nulles représentent notre jeu d'entrainement.\n",
    "Nous pouvons oberver que nous avons à faire à un problème de __classification binaire déséquilibré__ (il y a deux valeurs possibles, mais les deux valeurs ne sont pas également représentées).\n",
    "Ceci va influencer la manière dont nous allons construire et entraîner notre modèle.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's plot the distribution of the TARGET variable\n",
    "if DRAW_PLOTS:\n",
    "    fig = px.bar(\n",
    "        app_df[\"TARGET\"].replace({\n",
    "            \"0\": \"TARGET=0 : payments OK\", \n",
    "            \"1\": \"TARGET=1 : payment difficulties\", \n",
    "        }).value_counts(),\n",
    "        title=\"Distribution of TARGET variable\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "    ).update_xaxes(\n",
    "        title=\"TARGET\",\n",
    "    ).update_yaxes(title=\"Count\")\n",
    "    fig.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Valeurs vides\n",
    "\n",
    "Nous voyons que toutes les variables ont moins de 30% de valeurs vides, et près de la moitié a moins de 1% de valeurs vides. Le jeu de données est donc relativement bien rempli, ce qui ne devrait pas poser de problème pour la suite.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's display variables with missing values ratio\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_empty_values(app_df)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Valeurs impossibles\n",
    "\n",
    "Quelques valeurs présentes dans les données semblent impossibles. Nous allons supprimer ces \"outliers\".\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define data constraints\n",
    "data_constraints = {\n",
    "    \"DAYS_EMPLOYED\": {\"min\": -35000, \"max\": 0,}, # max 100 years, only negative values\n",
    "}\n",
    "\n",
    "if DRAW_PLOTS:\n",
    "    # Let's display box plots for variables with outliers\n",
    "    vis_helpers.plot_boxes(app_df, plot_columns=data_constraints.keys(), categorical_column=\"TARGET\")\n",
    "\n",
    "# Remove values that are outside possible range\n",
    "app_df = feat_helpers.drop_impossible_values(\n",
    "    app_df, constraints=data_constraints,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables quantitatives\n",
    "\n",
    "Nous allons simplement afficher la distribution de quelques variables numériques. Nous voyons déjà que selon la valeur de TARGET, la distribution (moyenne) des variables peut être sensiblement différente.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Draw the BoxPlots of some numeric columns, split per Target\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_boxes(app_df,\n",
    "        plot_columns=[\n",
    "            \"AMT_INCOME_TOTAL\",\n",
    "            \"AMT_CREDIT\",\n",
    "            \"AMT_ANNUITY\",\n",
    "            \"AMT_GOODS_PRICE\",\n",
    "            \"DAYS_BIRTH\",\n",
    "            \"DAYS_EMPLOYED\",\n",
    "            \"OWN_CAR_AGE\",\n",
    "            \"REGION_RATING_CLIENT\",\n",
    "            \"REGION_RATING_CLIENT_W_CITY\",\n",
    "            \"EXT_SOURCE_1\",\n",
    "            \"EXT_SOURCE_2\",\n",
    "            \"EXT_SOURCE_3\",\n",
    "            \"DAYS_LAST_PHONE_CHANGE\",\n",
    "            \"AMT_REQ_CREDIT_BUREAU_YEAR\",\n",
    "        ],\n",
    "        categorical_column=\"TARGET\",\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables qualitatives\n",
    "\n",
    "De la même manière, nous allons simplement afficher la distribution de quelques variables catégorielles. Nous voyons déjà que selon la valeur de TARGET, la distribution (répartition entre classes) des variables peut être sensiblement différente (`TARGET=0` pour 77,6% des `NAME_CONTRACT_TYPE=\"Cash loans\"`, tandis que `TARGET=0` pour 93,1% des `NAME_CONTRACT_TYPE=\"Revolving loans\"`).\n",
    "Certaines variables ont une répartition très inégale entre classes (`FLAG_MOBIL` vaut systématiquement `True` et jamais `False`).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Draw the Bar charts of some categorical columns, split per Target\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_categories_bars(app_df,\n",
    "        plot_columns=[\n",
    "            \"NAME_CONTRACT_TYPE\",\n",
    "            \"CODE_GENDER\",\n",
    "            \"FLAG_OWN_CAR\",\n",
    "            \"FLAG_OWN_REALTY\",\n",
    "            \"NAME_INCOME_TYPE\",\n",
    "            \"NAME_EDUCATION_TYPE\",\n",
    "            \"NAME_FAMILY_STATUS\",\n",
    "            \"NAME_HOUSING_TYPE\",\n",
    "            \"OCCUPATION_TYPE\",\n",
    "            \"FLAG_MOBIL\",\n",
    "        ],\n",
    "        categorical_column=\"TARGET\",\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encodage des catégories\n",
    "\n",
    "Lorsque les données qualitatives ne sont pas ordinales (on ne peu pas les classer selon un certain ordre), l'encodage \"One Hot Encoding\" sera plus performant que le \"Label Encoding\".\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Pandas get_dummies() categorical data encoder\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html\n",
    "# Convert categorical variable into dummy/indicator variables.\n",
    "# a.k.a \"One Hot Encoding\"\n",
    "\n",
    "app_df = pd.get_dummies(app_df, dtype=bool)\n",
    "\n",
    "app_df.describe(include=\"bool\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous avons ici créé 134 nouvelles variables booléennes qui correspondent aux différentes classes de chacune des 14 anciennes variables catégorielles qui ont été encodées et supprimées.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Nous allons tenter d'enrichir nos données en intégrant des variables qui sont des compositions non linéaires des variables existantes.\n",
    "\n",
    "\n",
    "### Données métier\n",
    "\n",
    "Afin d'apporter plus de sens aux données que nous allons fournir à nos modèles, nous pouvons faire appel aux experts métier qui peuvent nous indiquer des informations qui sont réputées importantes afin de prédire si un client risque d'avoir des problèmes de remboursement ou non.\n",
    "\n",
    "Les informations métier pertinentes sont :\n",
    "- Montant emprunté / Prix du bien acheté : `AMT_CREDIT / AMT_GOODS_PRICE`\n",
    "- Montant des annuités / Montant emprunté : `AMT_ANNUITY / AMT_CREDIT`\n",
    "- Montant des annuités / Revenu annuel : `AMT_ANNUITY / AMT_INCOME_TOTAL`\n",
    "- Ancienneté au travail / Age : `DAYS_EMPLOYED / DAYS_BIRTH`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create the new features\n",
    "app_df[\"CREDIT_PRICE_RATIO\"] = app_df[\"AMT_CREDIT\"] / app_df[\"AMT_GOODS_PRICE\"]\n",
    "app_df[\"ANNUITY_CREDIT_RATIO\"] = app_df[\"AMT_ANNUITY\"] / app_df[\"AMT_CREDIT\"]\n",
    "app_df[\"ANNUITY_INCOME_RATIO\"] = app_df[\"AMT_ANNUITY\"] / app_df[\"AMT_INCOME_TOTAL\"]\n",
    "app_df[\"EMPLOYED_BIRTH_RATIO\"] = app_df[\"DAYS_EMPLOYED\"] / app_df[\"DAYS_BIRTH\"]\n",
    "\n",
    "# Draw the BoxPlots for these features\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_boxes(app_df,\n",
    "        plot_columns=[\n",
    "            \"CREDIT_PRICE_RATIO\",\n",
    "            \"ANNUITY_CREDIT_RATIO\",\n",
    "            \"ANNUITY_INCOME_RATIO\",\n",
    "            \"EMPLOYED_BIRTH_RATIO\",\n",
    "        ],\n",
    "        categorical_column=\"TARGET\",\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composition polynomiales de variables existantes\n",
    "\n",
    "Les variables `EXT_SOURCE_{1-3}` n'ont a priori pas de sens concret. On peut imaginer que `TARGET` ne soit pas forcément linéairement dépendant de ces variables. Nous allons donc générer des combinaisons polynomiales de ces variables.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "# Let's keep only non null data\n",
    "ext_source = app_df[[\"SK_ID_CURR\", \"TARGET\", \"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]].dropna()\n",
    "\n",
    "# Let's create the new features\n",
    "poly = PolynomialFeatures()\n",
    "poly_feat = pd.DataFrame(poly.fit_transform(\n",
    "        X=ext_source[[\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]],\n",
    "        y=ext_source[\"TARGET\"],\n",
    "    )\n",
    ")\n",
    "poly_feat.columns=poly.get_feature_names()\n",
    "\n",
    "poly_feat.insert(0, \"SK_ID_CURR\", ext_source[\"SK_ID_CURR\"].values)\n",
    "\n",
    "# Merge the new features with the original dataset\n",
    "app_df = app_df.merge(\n",
    "    poly_feat,\n",
    "    on=\"SK_ID_CURR\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Draw the BoxPlots for these features\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_boxes(app_df,\n",
    "        plot_columns=poly_feat.columns[5:],\n",
    "        categorical_column=\"TARGET\",\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Features selection\n",
    "\n",
    "Le but ici est d'éliminer un certain nombre de variables afin d'accélérer l'entrainement et la prédiction de nos modèles. Nous souhaitons éliminer les variables qui pénaliseront le moins possible les performances de nos modèles.\n",
    "\n",
    "Nous savons déjà que les colonnes `SK_ID_CURR` (simple identifiant sans sens métier), `1` et `x{0-2}` (variables polynomiales d'ordre 0 et 1), et `FLAG_MOBIL` (vaut toujours 1) n'apportent pas d'information. Nous allons donc les éliminer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's drop the features that are not useful for the prediction\n",
    "app_df = app_df.drop(\n",
    "    columns=[\"SK_ID_CURR\", \"1\", \"x0\", \"x1\", \"x2\", \"FLAG_MOBIL\"]\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons ici observer la corrélation :\n",
    "- de chaque variable avec la variables cible `TARGET` : les variables les moins corrélées à `TARGET` seront a priori les moins utiles pour prédire sa valeur.\n",
    "- entre les différentes variables deux à deux : si deux variables sont très corrélées, elles apportent une information redondante et nous pouvons donc en éliminer une des deux.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's compute the correlation matrix\n",
    "app_correlations = app_df.corr().abs().sort_values(\n",
    "    \"TARGET\", ascending=False, axis=0\n",
    ").sort_values(\n",
    "    \"TARGET\", ascending=False, axis=1\n",
    ")\n",
    "\n",
    "if DRAW_PLOTS:\n",
    "    fig = px.imshow(app_correlations,\n",
    "        title=\"Correlations between features\",\n",
    "        width=1200,\n",
    "        height=1200,\n",
    "    )\n",
    "    fig.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons qu'il y a des variables très peu corrélées à `TARGET` (ex. : `abs(corr(\"TARGET\", \"FLAG_EMP_PHONE\")) < 0.0001`), et d'autres très corrélées entre elles (ex. : `abs(corr(\"LIVINGAPARTMENTS_AVG\", \"\"LIVINGAPARTMENTS_MEDI\")) > 0.99`).\n",
    "Nous allons simplifier notre jeu de données en supprimant ces variables.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's find variables that are highly de-correlated from TARGET\n",
    "corr_target_min_threshold = 0.01\n",
    "highly_decorrelated_from_target = pd.DataFrame(columns=[\"correlation with TARGET\"])\n",
    "for col in app_correlations.columns:\n",
    "    if col != \"TARGET\" and (\n",
    "        pd.isnull(app_correlations[col][\"TARGET\"])\n",
    "        or abs(app_correlations[col][\"TARGET\"]) < corr_target_min_threshold\n",
    "    ):\n",
    "        highly_decorrelated_from_target.loc[col] = {\n",
    "            \"correlation with TARGET\": app_correlations[col][\"TARGET\"],\n",
    "        }\n",
    "\n",
    "highly_decorrelated_from_target.sort_values(by=[\"correlation with TARGET\"])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's find variables that have a highly correlated pair\n",
    "corr_pair_max_threshold = 0.9\n",
    "highly_correlated = pd.DataFrame(columns=[\"pair\", \"correlation\"])\n",
    "for i in range(len(app_correlations.columns)):\n",
    "    for j in range(i + 1, len(app_correlations.columns)):\n",
    "        if app_correlations.iloc[i, j] > corr_pair_max_threshold:\n",
    "            # variables are highly correlated\n",
    "            if app_correlations.iloc[0, i] > app_correlations.iloc[0, j]:\n",
    "                # first variable is more correlated with target => we want to keep it\n",
    "                keep_index = i\n",
    "                drop_index = j\n",
    "            else:\n",
    "                keep_index = j\n",
    "                drop_index = i\n",
    "\n",
    "            highly_correlated.loc[app_correlations.columns[drop_index]] = {\n",
    "                \"pair\": app_correlations.columns[keep_index],\n",
    "                \"correlation\": app_correlations.iloc[i, j],\n",
    "            }\n",
    "\n",
    "highly_correlated.sort_values(by=\"correlation\", ascending=False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Drop irrelevant columns\n",
    "app_df.drop(\n",
    "    columns=highly_decorrelated_from_target.index,\n",
    "    inplace=True,\n",
    "    errors=\"ignore\",\n",
    ")\n",
    "app_df.drop(\n",
    "    columns=highly_correlated.index,\n",
    "    inplace=True,\n",
    "    errors=\"ignore\",\n",
    ")\n",
    "\n",
    "app_df.describe(include=\"all\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Après simplification, nous voyons que nous avons drastiquement réduit le nombre de variables pour ne conserver que celles réellement pertinentes pour la prédiction de `TARGET`.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Préparation des données\n",
    "\n",
    "Nous allons transformer les données pour que nos modèles puissent les exploiter au mieux. Afin d'éviter la \"fuite d'information\" entre le jeu de données d'entraînement et de test, nous allons maintenant séparer notre jeu de données en deux. Les transformations seront apprises uniquement sur le jeu d'entraînement, mais appliquées aux deux jeux de données (entraînement et test).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Given data\n",
    "X = app_df.drop([\"TARGET\"], axis=1)\n",
    "# Data to predict\n",
    "y = app_df[\"TARGET\"]\n",
    "\n",
    "# Let's split the whole dataset into a training set (80% of data) and a test set (20% of data)\n",
    "# The dataset will be split in a stratified way, in order to have a good distribution of the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y , random_state=42)\n",
    "\n",
    "# Save the processed data\n",
    "X_train.to_csv(\"../data/processed/X_train.csv\", index=False)\n",
    "X_test.to_csv(\"../data/processed/X_test.csv\", index=False)\n",
    "y_train.to_csv(\"../data/processed/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"../data/processed/y_test.csv\", index=False)\n",
    "\n",
    "# cleanup\n",
    "del app_df, X, y\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalisation des données\n",
    "\n",
    "Afin d'éviter que certains modèles pondèrent l'importance de certaines variables à cause de leur ordre de grandeur, nous allons normaliser chaque variable afin de les ramener à une moyenne nulle et une variance de 1.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## StandardScaler\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "if os.path.exists(\"../data/processed/X_train_scaled.csv\") and os.path.exists(\"../data/processed/X_test_scaled.csv\"):\n",
    "    X_train = pd.read_csv(\"../data/processed/X_train_scaled.csv\")\n",
    "    X_test = pd.read_csv(\"../data/processed/X_test_scaled.csv\")\n",
    "else:\n",
    "    # define scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # fit scaler on train data only, to avoid data leak\n",
    "    X_train = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train),\n",
    "        columns=X.columns,\n",
    "    )\n",
    "    X_test = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=X.columns,\n",
    "    )\n",
    "\n",
    "    # Save the processed data\n",
    "    X_train.to_csv(\"../data/processed/X_train_scaled.csv\", index=False)\n",
    "    X_test.to_csv(\"../data/processed/X_test_scaled.csv\", index=False)\n",
    "\n",
    "X_train.describe(include=\"all\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous avons ici équilibré les ordres de grandeur de chaque variables, afin que nos futurs modèles ne soient pas influencés par leur différence.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imputation des valeurs manquantes\n",
    "\n",
    "Afin d'éviter que certains modèles ne puissent être utilisés à cause des valeurs manquantes, nous allons remplacer toutes les valeurs nulles par leur meilleure estimation possible.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## IterativeImputer\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html\n",
    "# Impute missing values by modeling each feature with missing values as a function of other features in a round-robin fashion\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "if os.path.exists(\"../data/processed/X_train_imputed.csv\") and os.path.exists(\"../data/processed/X_test_imputed.csv\"):\n",
    "    X_train = pd.read_csv(\"../data/processed/X_train_imputed.csv\")\n",
    "    X_test = pd.read_csv(\"../data/processed/X_test_imputed.csv\")\n",
    "else:\n",
    "    # define imputer\n",
    "    imputer = IterativeImputer(\n",
    "        n_nearest_features=min(5, int(len(X.columns) / 10)),\n",
    "    )\n",
    "\n",
    "    # fit imputer on train data only, to avoid data leak\n",
    "    X_train = pd.DataFrame(\n",
    "        imputer.fit_transform(X_train),\n",
    "        columns=X.columns,\n",
    "    )\n",
    "    X_test = pd.DataFrame(\n",
    "        imputer.transform(X_test),\n",
    "        columns=X.columns,\n",
    "    )\n",
    "\n",
    "    # Save the processed data\n",
    "    X_train.to_csv(\"../data/processed/X_train_imputed.csv\", index=False)\n",
    "    X_test.to_csv(\"../data/processed/X_test_imputed.csv\", index=False)\n",
    "\n",
    "X_train.describe(include=\"all\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ré-équilibrage des classes de TARGET\n",
    "\n",
    "Comme nous l'avons vu, nos classes de TARGET sont largement déséquilibrées, ce qui va introduire un fort biais dans l'apprentissage de nos modèles.\n",
    "Nous allons donc ré-équilibrer ces classes dans nos jeux d'apprentissage afin de palier à ce problème.\n",
    "\n",
    "L'algorithme utilisé va dans éliminer aléatoirement des individus de la classe majoritaire.\n",
    "D'autres algorithmes plus poussés permettent de générer plus de données de la classe sous-représentée (SMOTE: Synthetic Minority Over-sampling Technique), puis éliminer certains individus de la classe sur-représentées (ENN : Edited Nearest Neighbours), tout en optimisant les données d'apprentissage.\n",
    "\n",
    "![Over- and Under-sampling](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Random under-sampler\n",
    "# https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n",
    "# Under-sample the majority class(es) by randomly picking samples with or without replacement.\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "if os.path.exists(\"../data/processed/X_train_balanced.csv\") and os.path.exists(\"../data/processed/y_train_balanced.csv\"):\n",
    "    X_train = pd.read_csv(\"../data/processed/X_train_balanced.csv\")\n",
    "    y_train = pd.read_csv(\"../data/processed/y_train_balanced.csv\")\n",
    "else:\n",
    "    # eliminate imbalance by randomly removing data points from the majority class\n",
    "    random_sampler = RandomUnderSampler(\n",
    "        sampling_strategy=1., # perfect balance\n",
    "        random_state=42,\n",
    "    )\n",
    "    X_train, y_train = random_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Save the processed data\n",
    "    X_train.to_csv(\"../data/processed/X_train_balanced.csv\", index=False)\n",
    "    y_train.to_csv(\"../data/processed/y_train_balanced.csv\", index=False)\n",
    "\n",
    "X_train.describe(include=\"all\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's plot the distribution of the TARGET variable\n",
    "if DRAW_PLOTS:\n",
    "    fig = px.bar(\n",
    "        y_train[\"TARGET\"].replace({\n",
    "            0: \"TARGET=0 : payments OK\", \n",
    "            1: \"TARGET=1 : payment difficulties\", \n",
    "        }).value_counts(),\n",
    "        title=\"Distribution of TARGET variable\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "    ).update_xaxes(\n",
    "        title=\"TARGET\",\n",
    "    ).update_yaxes(title=\"Count\")\n",
    "    fig.show()\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "target_count = y_train[\"TARGET\"].value_counts()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons maintenant que les classes sont bien équilibrées pour l'apprentissage."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyse en composantes principales (PCA)\n",
    "\n",
    "Avant de modéliser et prédire la valeur de TARGET, essayons de voir comment sont répartis nos individus dans le plan défini par les deux composantes principales. Ceci nous permettra de visualiser la distribution de nos données et des varaibles dans l'espace des composantes principales et d'évaluer la difficulté de la classification.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's plot the data and features in the 2D PCA plane\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_pca_2d(X_train, y_train)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cette analyse nous permet d'observer nos individus (et leur classe), de manière à maximiser la variance dans les deux axes du plan de projection. Nous pouvons faire deux observations :\n",
    "- les deux classes de `TARGET` se recouvrent assez largement : il est impossible de séparer les deux classes par une ligne (droite ou non)\n",
    "- nous voyons tout de même que la distribution empirique de chaque classe est légèrement décalée\n",
    "\n",
    "Il sera donc a priori très difficile de trouver un modèle avec une excellente performance sans faire de sur-apprentissage : nous aurons a priori toujours beaucoup de faux-positifs et faux-négatifs. Ceci-dit il devrait être possible d'obtenir des résultats meilleurs qu'une simple prédiction aléatoire, car les deux classes n'ont pas la même distribution.\n",
    "\n",
    "Nous voyons aussi le cercle de corrélation des variables, qui nous indique leur rôle dans chacun des axes du plan de projection.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modélisation et évaluation des modèles\n",
    "\n",
    "Nous allons ici entraîner différents modèles de classification binaire et évaluer leur efficacité selon trois critères :\n",
    "- le **temps d'exécution** : temps nécessaire au modèle pour les phases d'apprentissage (`fit`) et de prédiction (`predict`).\n",
    "- l'**interprétabilité** du modèle : la capacité du modèle à nous indiquer quelles sont les variables qui expliquent le mieux la prédiction.\n",
    "- la **performance** : nous allons mesurer différentes métriques (`accuracy` = taux de prédictions positives ou négatives correctes, `precision` = taux de prédictions positives correctes, `recall` = taux de positifs correctement prédits, `F1 score` = moyenne harmonique de la `precision` et du `recall`) pour chaque modèle. Nous utiliserons le `F1 score` comme mesure principale de la performance, car nous sommes face à un problème où les deux classes de la variable prédite ne sont pas équilibrées (`TARGET=0 ~ 92%`) dans le jeu de données, et nous voulons maximiser en priorité le `recall` (bien prédire les problèmes de paiement). Nous tracerons les courbes de `Precision-Recall` et de `ROC (Receiver operating characteristic)` qui compare le `recall` au taux de faux positifs.\n",
    "\n",
    "Afin d'améliorer la qualité du modèle et minimiser le biais de sélection du jeu d'entraînement, nous allons utiliser la méthode `StratifiedKFold()` qui permet de \"mixer\" les résultats sur plusieurs sous-ensembles du jeu d'entraînement.\n",
    "\n",
    "Afin de chercher les meilleurs hyper-paramètres de chacun des modèles, nous allons utiliser la recherche `HalvingRandomSearchCV()` qui permet d'éliminer des combinaisons d'hyper-paramètres en plusieurs étapes, et d'optimiser les ressources allouées à chaque étape.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# let's store the results of each model in a DataFrame\n",
    "results = pd.DataFrame(columns=(\n",
    "    'model',\n",
    "    'params',\n",
    "    'score',\n",
    "    'predict_time',\n",
    "    'cv_results_',\n",
    "    'best_index_', \n",
    "    'confusion_matrix',\n",
    "    'f1',\n",
    "    'accuracy',\n",
    "    'precision',\n",
    "    'recall',\n",
    "    'average_precision',\n",
    "    'precision_recall_curve',    \n",
    "    'roc_auc_score',\n",
    "    'roc_curve',\n",
    "))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèle de référence\n",
    "\n",
    "Nous allons utiliser un modèle bête et méchant qui se contente de faire des prédictions aléatoires.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Dummy Classifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html\n",
    "\n",
    "# DummyClassifier permet de faire des prédictions aléatoires.\n",
    "\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "dummy_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=DummyClassifier(random_state=42),\n",
    "    params={\n",
    "        'strategy': ['stratified', 'most_frequent', 'prior', 'uniform']\n",
    "    },\n",
    ")\n",
    "results = results.append(dummy_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=dummy_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 2.5s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons l'effet du biais dû au déséquilibre des classes de `TARGET` : en prédisant systématiquement `0` (la classe majoritaire : pas de problème de remboursement) nous obtenons une Accuracy assez bonne (~0.9), ce qui n'est pas si mauvais pour un algorithme de décision \"bête\". Mais les autres indicateurs sont très mauvais, notamment la précision moyenne (`AP = 0.09`) et l'aire sous la courbe ROC (`ROC_AUC = 0.5`).\n",
    "\n",
    "En revanche, celà signifie que le modèle prédit systématiquement que les clients n'auront pas de problème de remboursement, ce qui est dangeureux pour la banque. Il est primordial de mieux détecter les cas où il y a un risque de défaut (`TARGET=1`).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèles linéaires\n",
    "\n",
    "\n",
    "#### Modèle Ridge\n",
    "\n",
    "Nous allons modéliser la variable `TARGET` comme une combinaison linéaire de nos autres variables. La régularisation permet de contenir les coefficients et donc d'éviter le sur-apprentissage.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Ridge Classifier (linear model)\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html\n",
    "# RidgeClassifier is a linear model with a linear loss function. It is useful for regularized linear models.\n",
    "\n",
    "# - La norme ℓ2 du vecteur de poids peut être utilisée comme terme de régularisation de la régression linéaire.\n",
    "# - Cela s'appelle la régularisation de Tykhonov, ou régression ridge.\n",
    "# - La régression ridge admet toujours une solution analytique unique.\n",
    "# - La régression ridge permet d'éviter le surapprentissage en restraignant l'amplitude des poids.\n",
    "# - La régression ridge a un effet de sélection groupée : les variables corrélées ont le même coefficient.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "ridge_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=RidgeClassifier(random_state=42),\n",
    "    params={\n",
    "        'alpha': np.logspace(-10, 10, 50),\n",
    "        'fit_intercept': [True, False],\n",
    "        'normalize': [True, False],\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "    },\n",
    ")\n",
    "results = results.append(ridge_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=ridge_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 45.6s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Avec une simple régression linéaire des variables d'entraînement, nous voyons que les résultats sont déjà meilleurs.\n",
    "\n",
    "Voyons comment mesurer l'importance de chaque variable dans notre modèle."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's plot the features importances\n",
    "if DRAW_PLOTS:\n",
    "    top_coefficients = pd.Series(\n",
    "        ridge_results[\n",
    "            'model'\n",
    "        ].coef_[0],\n",
    "        X_train.columns,\n",
    "    ).map(abs).sort_values(ascending=False).head(20)\n",
    "\n",
    "    fig = px.bar(\n",
    "        top_coefficients,\n",
    "        color=top_coefficients.values,\n",
    "        title=\"Top 20 variables importance\",\n",
    "        labels={\n",
    "            \"index\": \"Variable name\",\n",
    "            \"value\": \"Coefficient\",\n",
    "            \"color\": \"Coefficient\",\n",
    "        },\n",
    "        width=1200,\n",
    "        height=800,\n",
    "    )\n",
    "    fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons que les variables XXX jouent un rôle important dans ce modèle.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Modèle de Régression Logistique\n",
    "\n",
    "Ici, nous modélisons la variable `TARGET` comme la probabilité d'appartenance à la classe positive en tant que transformation logistique d'une combinaison linéaire des variables.\n",
    "Ce modèle est sensé mieux classifier les cas \"difficiles\".\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Logistic Regression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# Logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix\n",
    "# and a bias vector. It is also able to fit non-linear decision boundaries, such as the\n",
    "# logistic function found in Support Vector Machines.\n",
    "\n",
    "# - La régression logistique modélise la probabilité qu'une observation appartienne à la classe positive comme une transformation logistique d'une combinaison linéaire des variables.\n",
    "# - Les coefficients d'une régression logistique s'apprennent par maximisation de vraisemblance, mais il n'existe pas de solution explicite.\n",
    "# - La vraisemblance est convexe, et de nombreux solveurs peuvent être utilisés pour trouver une solution numérique.\n",
    "# - Les concepts de régularisation ℓ1 et ℓ2 s'appliquent aussi à la régression logistique.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "logistic_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=LogisticRegression(\n",
    "        penalty='elasticnet',\n",
    "        solver='saga',\n",
    "        max_iter=10000,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    params={\n",
    "        'C': np.logspace(-5, 1, 10),\n",
    "        'l1_ratio': np.linspace(0, 1, 10),\n",
    "        'class_weight': [None, 'balanced'],\n",
    "    },\n",
    ")\n",
    "results = results.append(logistic_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=logistic_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 26.2s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ici, les résultats sont assez comparables à la régression Ridge.\n",
    "\n",
    "Voyons comment nous pouvons mesurer l'importance des variables dans le modèle.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if DRAW_PLOTS:\n",
    "    top_coefficients = pd.Series(\n",
    "        logistic_results[\n",
    "            'model'\n",
    "        ].coef_[0],\n",
    "        X_train.columns,\n",
    "    ).map(abs).sort_values(ascending=False).head(20)\n",
    "\n",
    "    fig = px.bar(\n",
    "        top_coefficients,\n",
    "        color=top_coefficients.values,\n",
    "        title=\"Top 20 variables importance\",\n",
    "        labels={\n",
    "            \"index\": \"Variable name\",\n",
    "            \"value\": \"Coefficient\",\n",
    "            \"color\": \"Coefficient\",\n",
    "        },\n",
    "        width=1200,\n",
    "        height=800,\n",
    "    )\n",
    "    fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous pouvons aussi observer l'importance de chaque feature, selon le niveau de régularisation du modèle. Ceci nous indique quelles sont les variables les plus \"importantes\" : celles qui interviennent en premier dans le modèle et qui ont le coefficient le plus élevé (en valeur absolue).\n",
    "Nous pouvons voir aussi quelles variables sont corrélées : ce sont celles qui ont un coefficient proche.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if DRAW_PLOTS:\n",
    "    c_range = np.logspace(-8, 1, 20)\n",
    "    l1_ratio = logistic_results['params']['l1_ratio']\n",
    "\n",
    "    coefficients = pd.DataFrame(index=X_train.columns, columns=c_range)\n",
    "    errors = []\n",
    "    for c in c_range:\n",
    "        logistic = LogisticRegression(C=c, l1_ratio=l1_ratio)\n",
    "        logistic.fit(X_train, y_train)\n",
    "        coefficients.loc[:, c] = logistic.coef_[0]\n",
    "        errors.append(mean_squared_error(y_test, logistic.predict(X_test)))\n",
    "\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for col in coefficients.index:\n",
    "        fig.add_trace(go.Scatter(x=c_range, y=coefficients.loc[col, :], name=col,))\n",
    "\n",
    "    fig.update_xaxes(type=\"log\", autorange=\"reversed\")\n",
    "    fig.update_layout(\n",
    "        title=\"Logistic regression coefficients as a function of the regularization\",\n",
    "        xaxis_title=\"log(C)\",\n",
    "        yaxis_title=\"coefficient\",\n",
    "        width=1200,\n",
    "        height=600,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=c_range, y=errors, name=\"MSE\"))\n",
    "    fig.update_xaxes(type=\"log\", autorange=\"reversed\")\n",
    "    fig.update_layout(\n",
    "        title=\"Logistic regression MSE as a function of the regularization\",\n",
    "        xaxis_title=\"log(C)\",\n",
    "        yaxis_title=\"MSE\",\n",
    "        width=1200,\n",
    "        height=600,\n",
    "    )\n",
    "    fig.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons que les variables XXX sont les plus importantes dans ce modèle.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèle Machines à Vecteurs de Support (SVM)\n",
    "\n",
    "Ici, nous allons chercher à trouver les lignes séparatrices entre les données de nos deux classes.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Support Vector Machine\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "# Support Vector Machines (SVMs) are supervised learning models that learn to classify data points according to a set of hyperplanes.\n",
    "# The model is based on the idea that the data points are separated by a hyperplane, and the hyperplane is the only way to classify the data points.\n",
    "# The model is a linear classifier, meaning that it makes a linear decision boundary.\n",
    "# The model is non-parametric, meaning that it does not require any parameters to be learned.\n",
    "# The model is usually a good choice for handling unbalanced classes.\n",
    "# The model is fast to train, and can be used for large datasets.\n",
    "\n",
    "# Linear SVC\n",
    "# - Les SVM (Support Vector Machines), aussi appelées en français Machines à Vecteurs de Support et parfois Séparatrices à Vaste Marge, cherchent à séparer linéairement les données.\n",
    "# - La version primale résout un problème d'optimisation à p variables et est donc préférable si on a moins de variables que d'échantillons.\n",
    "# - À l'inverse, la version duale résout un problème d'optimisation à n variables et est donc préférable si on a moins d'échantillons que de variables.\n",
    "# - Les vecteurs de support sont les points du jeu de données qui sont les plus proches de l'hyperplan séparateur.\n",
    "# - La fonction de décision peut s'exprimer uniquement en fonction du produit scalaire du point à étiqueter avec les vecteurs de support.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "svc_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=SVC(random_state=42),\n",
    "    params={\n",
    "        'C': np.logspace(-5, 1, 10),\n",
    "        \"gamma\": [\"scale\", \"auto\"],\n",
    "        \"degree\": range(2, 5),\n",
    "        \"kernel\": [\"linear\", \"poly\", \"rbf\"],\n",
    "        \"class_weight\": [None, 'balanced'],\n",
    "    },\n",
    ")\n",
    "results = results.append(svc_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=svc_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 558.5s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons que les résultats sont sensiblement identiques aux modèles précédents, mais le temps de prédiction est beaucoup plus long.\n",
    "\n",
    "Le meilleur noyau étant linéaire, nous pouvons observer l'importance de chaque variables dans le modèle.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if DRAW_PLOTS:\n",
    "    if svc_results['params']['kernel'] == 'linear':\n",
    "        # if the kernel is linear, we can plot the variables importances\n",
    "        top_coefficients = pd.Series(\n",
    "            svc_results['model'].coef_[0],\n",
    "            X_train.columns,\n",
    "        ).map(abs).sort_values(ascending=False).head(20)\n",
    "\n",
    "        fig = px.bar(\n",
    "            top_coefficients,\n",
    "            color=top_coefficients.values,\n",
    "            title=\"Top 20 variables importance\",\n",
    "            labels={\n",
    "                \"index\": \"Variable name\",\n",
    "                \"value\": \"Coefficient\",\n",
    "                \"color\": \"Coefficient\",\n",
    "            },\n",
    "            width=1200,\n",
    "            height=800,\n",
    "        )\n",
    "        fig.show()\n",
    "    else:\n",
    "        # otherwise we can't plot the variables importances\n",
    "        print(\"Cannot plot variables importances for non-linear kernel\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons que les variables XXX sont les plus importantes.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèles de Discriminant Analysis\n",
    "\n",
    "Comme pour les SVM, ces modèles vont chercher à définir une séparation (linéaire ou quadratique) entre les classes.\n",
    "\n",
    "#### Modèle Linear Discriminant Analysis\n",
    "\n",
    "Ici, la séparation cherchée est linéaire.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Linear Discriminant Analysis\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "# Linear Discriminant Analysis (LDA) is a supervised machine learning method that uses linear discriminant analysis to separate the features of the data into two groups.\n",
    "# The method is a linear classifier, meaning that it makes a linear decision boundary.\n",
    "# The method is non-parametric, meaning that it does not require any parameters to be learned.\n",
    "# The method is usually a good choice for handling unbalanced classes.\n",
    "# The method is fast to train, and can be used for large datasets.\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "lda_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=LinearDiscriminantAnalysis(),\n",
    "    params={},\n",
    ")\n",
    "results = results.append(lda_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=lda_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 3.5s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les résultats sont toujours similaires.\n",
    "\n",
    "Nous pouvons observer l'importance des variables.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if DRAW_PLOTS:\n",
    "    top_coefficients = pd.Series(\n",
    "        lda_results['model'].coef_[0],\n",
    "        X_train.columns,\n",
    "    ).map(abs).sort_values(ascending=False).head(20)\n",
    "\n",
    "    fig = px.bar(\n",
    "        top_coefficients,\n",
    "        color=top_coefficients.values,\n",
    "        title=\"Top 20 variables importance\",\n",
    "        labels={\n",
    "            \"index\": \"Variable name\",\n",
    "            \"value\": \"Coefficient\",\n",
    "            \"color\": \"Coefficient\",\n",
    "        },\n",
    "        width=1200,\n",
    "        height=800,\n",
    "    )\n",
    "    fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons que les variables XXX sont les plus importantes.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Quadratic Discriminant Analysis\n",
    "\n",
    "\n",
    "Ici, la séparation cherchée est quadratique (élipse, parabole ou hyperbole).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Quadratic Discriminant Analysis\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html\n",
    "# A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.\n",
    "# The model fits a Gaussian density to each class.\n",
    "# The model is not necessarily the best model for the data, but it can be a useful baseline to compare the other models against.\n",
    "# The model is non-parametric, meaning that it does not require any parameters to be learned.\n",
    "# The model is fast to train, and can be used for large datasets.\n",
    "# The model is not suitable for handling unbalanced classes.\n",
    "# The model is not suitable for handling categorical features.\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "qda_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=QuadraticDiscriminantAnalysis(),\n",
    "    params={},\n",
    ")\n",
    "results = results.append(qda_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=qda_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 2.7s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons que ce modèle n'est pas adapté à notre problème : les résultats sont nettement moins bons qu'avec les modèles précédents.\n",
    "\n",
    "Pour ce modèle, nous ne pouvons pas directement observer l'importance des variables, mais nous pouvons mesurer la sensibilité de la prédiction à la permutation de variables ( [Permutation feature importance](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance) )"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if DRAW_PLOTS:\n",
    "    # Permutation importance for Train data\n",
    "    vis_helpers.plot_permutation_importance(\n",
    "        qda_results['model'], \n",
    "        X_train, \n",
    "        y_train,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if DRAW_PLOTS:\n",
    "    # Permutation importance for Test data\n",
    "    vis_helpers.plot_permutation_importance(\n",
    "        qda_results['model'], \n",
    "        X_test, \n",
    "        y_test,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "En comparant les deux graphiques, nous pouvons observer que le modèle a \"over-fitté\" sur plusieurs variables (leur importance est très différente entre les jeux d'entraînement et de test) : `NAME_INCOME_TYPE_Working`, `NAME_INCOME_TYPE_Commercialassociate` . Mais les variables les plus importantes restent `EXT_SOURCE_{1-3}` et leurs combinaisons polynomiales.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèle Naive Bayes\n",
    "\n",
    "Ici, nous allons modéliser la variable `TARGET` comme la probabilité d'appartenir à la classe positive, sachant les autres variables, en s'appuyant sur le théorème de Bayes :\n",
    "$$\n",
    "P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots, x_n \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}\n",
    "$$\n",
    "sous la supposition \"naive\" que les variables sont indépendantes deux-à-deux :\n",
    "$$\n",
    "P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y)\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Gaussian Naive Bayes (GaussianNB)\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "# Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable.\n",
    "# Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.\n",
    "# On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "gnb_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=GaussianNB(),\n",
    "    params={\n",
    "        \"var_smoothing\": np.logspace(-12, -3, 10),\n",
    "    },\n",
    ")\n",
    "results = results.append(gnb_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=gnb_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 1.2s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les résultats sont à nouveau relativement mauvais, ce modèle n'est pas adapté.\n",
    "\n",
    "De plus, ce modèle ne nous permet pas de mesurer l'importance des variables pour effectuer les prédictions.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K nearest neighbours\n",
    "\n",
    "Ici, nous allons prédire la classe de nos données de test, simplement en regardant la classe de ses plus proches voisins. La supposition est que deux individus proches (\"ressemblants\") devraient avoir la même classe.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## K-nearest-neighbours\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "# K-nearest neighbors (KNN) is a non-parametric method used for classification and regression.\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "knn_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=KNeighborsClassifier(n_jobs=-1),\n",
    "    params={\n",
    "        \"n_neighbors\": range(2, 12, 2),\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "    },\n",
    ")\n",
    "results = results.append(knn_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=knn_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 243.5s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encore une fois, les résultats ne sont pas très bons et ce type de modèle ne permet pas d'interpréter l'importance des variables pour la prédiction.\n",
    "De plus, le temps de prédiction est assez long (car il est nécessaire de calculer la distance entre plusieurs points).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèles d'arbres de décision\n",
    "\n",
    "Ici, nous allons essayer de construire un arbre de décision qui permette de choisir simplement la classe d'un individu.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Decisioon Tree\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "# Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "decision_tree_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    params={\n",
    "        \"criterion\": ['gini', 'entropy'],\n",
    "        \"splitter\": ['best', 'random'],\n",
    "        \"max_depth\": [None] + list(range(2, 20, 3)),\n",
    "        \"max_features\": [None, 'auto', 'sqrt', 'log2'],\n",
    "        \"class_weight\": [None, 'balanced'],\n",
    "    },\n",
    ")\n",
    "results = results.append(decision_tree_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=decision_tree_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 6.2s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons que les résultats ne sont pas très bons.\n",
    "\n",
    "Mais nous pouvons observer l'importance des variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if DRAW_PLOTS:\n",
    "    top_coefficients = pd.Series(\n",
    "        decision_tree_results[\n",
    "            'model'\n",
    "        ].feature_importances_,\n",
    "        X_train.columns,\n",
    "    ).map(abs).sort_values(ascending=False).head(20)\n",
    "\n",
    "    fig = px.bar(\n",
    "        top_coefficients,\n",
    "        color=top_coefficients.values,\n",
    "        title=\"Top 20 variables importance\",\n",
    "        labels={\n",
    "            \"index\": \"Variable name\",\n",
    "            \"value\": \"Coefficient\",\n",
    "            \"color\": \"Coefficient\",\n",
    "        },\n",
    "        width=1200,\n",
    "        height=800,\n",
    "    )\n",
    "    fig.show()\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous pouvons même observer l'arbre de décision qui a été appris par le modèle. Ceci a l'avantage d'être très concret et compréhensible par une personne du métier.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if DRAW_PLOTS:\n",
    "    plt.figure(figsize=(50,20))\n",
    "    plot_tree(\n",
    "        decision_tree_results['model'],\n",
    "        feature_names=X_train.columns,\n",
    "        fontsize=8,\n",
    "    )\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèles ensemblistes\n",
    "\n",
    "L'idée derrière les modèles ensemblistes est de combiner un (grand) nombre d'\"apprenants faibles\" (modèles très simples) afin de les faire voter et d'obtenir un consensus qui soit meilleur que chacun des modèles de base.\n",
    "\n",
    "\n",
    "#### Modèle Bagging\n",
    "\n",
    "Ici, nous combinons simplement plusieurs petits arbres de décision.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Bagging\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "# Bagging is a meta-estimator that fits base learners on random subsets of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n",
    "# The base estimator is a decision tree.\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "bagging_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=BaggingClassifier(n_jobs=-1, random_state=42),\n",
    "    params={\n",
    "        \"base_estimator\": [DecisionTreeClassifier()] + [DecisionTreeClassifier(max_depth=x) for x in range(2, 20, 3)],\n",
    "        \"n_estimators\": np.logspace(1, 3, 20).astype(int),\n",
    "    },\n",
    ")\n",
    "results = results.append(bagging_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=bagging_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 181.8s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les résultats sont ici un peu moins bons que les meilleurs modèles, mais tout de même corrects. En revanche, ce modèle ne permet pas d'évaluer l'importance des variables dans la prédiction.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest\n",
    "\n",
    "Ici, nous allons construire des forets d'arbres de décision sur des sous-ensembles de données.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Random Forest\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "# Random forest is a meta-estimator that fits a number of classifical decision trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n",
    "# The base estimator is a decision tree.\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "random_forest_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    params={\n",
    "        \"n_estimators\": np.logspace(1, 3, 5).astype(int),\n",
    "        \"max_depth\": [None] + list(range(2, 20, 3)),\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "        \"class_weight\": [None, 'balanced_subsample', 'balanced'],\n",
    "    },\n",
    ")\n",
    "results = results.append(random_forest_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=random_forest_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 344.3s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les résultats sont bons et nous pouvons observer l'importance des variables pour la prédiction.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if DRAW_PLOTS:\n",
    "    top_coefficients = pd.Series(\n",
    "        random_forest_results[\n",
    "            'model'\n",
    "        ].feature_importances_,\n",
    "        X_train.columns,\n",
    "    ).map(abs).sort_values(ascending=False).head(20)\n",
    "\n",
    "    fig = px.bar(\n",
    "        top_coefficients,\n",
    "        color=top_coefficients.values,\n",
    "        title=\"Top 20 variables importance\",\n",
    "        labels={\n",
    "            \"index\": \"Variable name\",\n",
    "            \"value\": \"Coefficient\",\n",
    "            \"color\": \"Coefficient\",\n",
    "        },\n",
    "        width=1200,\n",
    "        height=800,\n",
    "    )\n",
    "    fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons que les variables XXX sont les plus importantes.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèles de Boosting\n",
    "\n",
    "Ici, nous allons utiliser des modèles de base, mais les \"booster\" en les mettant en compétition sur des sous-ensembles du jeu de données d'apprentissage.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### XGBoost\n",
    "\n",
    "XGBoost est un framework classique permettant de faire du bosting.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## XGBoost\n",
    "# https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "# XGBoost is a fast, scalable, high-performance gradient boosting framework.\n",
    "# It supports both classification and regression.\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Hide warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "# Compute scores\n",
    "xgb_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    ),\n",
    ")\n",
    "results = results.append(xgb_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=xgb_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 53.4s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les performances sont bonnes, mais pas les meilleures.\n",
    "\n",
    "Nous pouvons observer l'importance des variables et l'arbre de décision généré.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from xgboost import plot_importance as xgb_plot_importance\n",
    "\n",
    "\n",
    "if DRAW_PLOTS:\n",
    "    # Plot feature importance\n",
    "    xgb_plot_importance(\n",
    "        booster=xgb_results['model'],\n",
    "        max_num_features=20,\n",
    "    )\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20, 10)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from xgboost import plot_tree as xgb_plot_tree\n",
    "\n",
    "\n",
    "if DRAW_PLOTS:\n",
    "    # Plot tree\n",
    "    xgb_plot_tree(booster=xgb_results['model'], rankdir='LR')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(100, 50)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Il est alors très facile de comprendre comment classifier un individu.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LGBMClassifier\n",
    "\n",
    "LightGBM est une autre bibliothèque de boosting classique.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## LGBMClassifier\n",
    "# https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "# LightGBM is a fast, scalable, high performance gradient boosting library.\n",
    "# It supports both classification and regression.\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "X_train = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "X_test = X_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "lgbm_results = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=LGBMClassifier(\n",
    "        objective='binary',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    ),\n",
    ")\n",
    "results = results.append(lgbm_results, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=lgbm_results['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 3.5s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les résultats ici sont les meilleurs observés et le temps de prediction est aussi assez bon.\n",
    "\n",
    "Nous pouvons aussi observer l'importance des variables et un des arbres de décision arbre générés.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from lightgbm import plot_importance as lgbm_plot_importance\n",
    "\n",
    "\n",
    "if DRAW_PLOTS:\n",
    "    # Plot feature importance\n",
    "    lgbm_plot_importance(\n",
    "        booster=lgbm_results['model'],\n",
    "        max_num_features=20,\n",
    "        figsize=(20, 10),\n",
    "    )\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from lightgbm import create_tree_digraph as lgbm_create_tree_digraph\n",
    "\n",
    "\n",
    "if DRAW_PLOTS:\n",
    "    # Plot tree\n",
    "    lgbm_create_tree_digraph(booster=lgbm_results['model'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèle de réseau de neurones\n",
    "\n",
    "Le perceptron permet d'apprendre des modèles paramétriques basés sur une combinaison linéaire des variables. Le perceptron permet d'apprendre des modèles de régression (la fonction d'activation est l'identité), de classification binaire (la fonction d'activation est la fonction logistique) ou de classification multi-classe (la fonction d'activation est la fonction softmax). Le perceptron est entraîné par des mises à jour itératives de ses poids grâce à l'algorithme du gradient. La même règle de mise à jour des poids s'applique dans le cas de la régression, de la classification binaire ou de la classification multi-classe. Empiler des perceptrons en un réseau de neurones multi-couches (feed-forward) permet de modéliser des fonctions arbitrairement complexes. C'est ce qui donne aux réseaux de neurones profonds la puissance prédictive qui fait actuellement leur succès.\n",
    "\n",
    "L'entraînement de ces réseaux se fait par rétro-propagation. Attention, cet algorithme ne converge pas nécessairement, et pas nécessairement vers la solution optimale ! Plus il y a de paramètres (i.e. de poids de connexion), plus il faut de données pour pouvoir apprendre les valeurs de ces paramètres sans risquer le sur-apprentissage.\n",
    "\n",
    "![One hidden layer Multi-layer Perceptron (MLP)](https://scikit-learn.org/stable/_images/multilayerperceptron_network.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Neural Network\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "# Neural network is a supervised learning model that can learn nonlinear functions.\n",
    "# It is a generalization of logistic regression, which is used for binary classification.\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "mlp_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=MLPClassifier(\n",
    "        max_iter=10000,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    params={\n",
    "        \"hidden_layer_sizes\": [(10,), (10,20,), (10,20,50,), (10,50,20,), (20,), (20,50,), (50,20,), (50,), (100,)],\n",
    "        \"alpha\": np.logspace(-8, 0, 20),\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": mlp_score['params'], \n",
    "    \"score\": mlp_score['score'],\n",
    "    \"predict_time\": mlp_score['predict_time'],\n",
    "})\n",
    "results = results.append(mlp_score, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=mlp_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "# 224.7s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les résultats observés ici sont bons, mais pas au niveau des meilleurs modèles. De plus, les réseaux de neurones ne permettent pas de voir l'importance des variables dans la prédiction.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AutoML\n",
    "\n",
    "Enfin, nous allons ici tester un outil qui automatise intégralement le choix du modèle et de ses hyper-paramètres.\n",
    "Les seules contraintes sont des contraintes de \"budget\" : le temps alloué à la recherche du meilleur modèle.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## AutoML\n",
    "# https://automl.github.io/auto-sklearn/\n",
    "# AutoML is a machine learning library that automatically tunes the hyperparameters of a machine learning model.\n",
    "# It is a wrapper around scikit-learn, which is a machine learning library.\n",
    "\n",
    "\n",
    "from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
    "from autosklearn.metrics import f1\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "automl_score = models_helpers.automl_classifier(\n",
    "    X_train=X_train, y_train=y_train.values.ravel(), X_test=X_test, y_test=y_test,\n",
    "    estimator=AutoSklearn2Classifier(\n",
    "        time_left_for_this_task=300,\n",
    "        per_run_time_limit=60,\n",
    "        metric=f1,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    ")\n",
    "results = results.append(automl_score, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=automl_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Nous avons testé plusieurs modèles, en cherchant à trouver pour chacun les meilleurs hyper-paramètres pour prédire si les futurs clients de la banque Home Credit présentent un risque de défaut de paiement ou non.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sélection du meilleur modèle\n",
    "\n",
    "Nous pouvons alors les comparer afin d'en choisir un pour le tester sur de nouvelles prédictions.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's display all the results\n",
    "results\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot prediction times per model\n",
    "if DRAW_PLOTS:\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for i in results.index:\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=results.iloc[i,:]['model'].__class__.__name__ , \n",
    "            x=['Prediction Time'], \n",
    "            y=results.iloc[i,:][['predict_time']],\n",
    "        ))\n",
    "        \n",
    "    fig.update_layout(\n",
    "        title=\"Models prediction times\",\n",
    "        xaxis_title=\"Model\",\n",
    "        yaxis_title=\"Prediction time in s (log scale)\",\n",
    "        yaxis_type='log',\n",
    "        width=1200,\n",
    "        height=600,\n",
    "    )\n",
    "    fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comme nous le voyons, certains modèles sont beaucoup plus rapides que d'autres. Les modèles `SVC`, `KNN`, `AutoSklearn`, `RandomForest` et `Bagging` sont les plus lents, tandis que les modèles `Dummy`, `Ridge`, `Logistic`, `DecisionTree` et `LDA` sont les plus rapides.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot performance scores per model\n",
    "if DRAW_PLOTS:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for i in results.index:\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=results.iloc[i,:]['model'].__class__.__name__ , \n",
    "            x=['F1', 'Accuracy', 'Recall', 'Precision', 'Average Precision', 'ROC_AUC'], \n",
    "            y=results.iloc[i,:][['f1', 'accuracy', 'recall', 'precision', 'average_precision', 'roc_auc_score']],\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Models prediction scores\",\n",
    "        xaxis_title=\"Model\",\n",
    "        yaxis_title=\"Score\",\n",
    "        barmode='group',\n",
    "        width=1200,\n",
    "        height=600,\n",
    "    )\n",
    "    fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons que globalement, le Recall est assez élevé, mais la Precision et le F1 ne sont généralement pas très bons. Ceci signifie que nous aurons plus de faux positifs que de faux négatifs, ce qui revient à une tendance à prédire trop souvent la classe positive, et donc avoir une politique de prêt plutôt précautionneuse : dans le doute, nous préférerons estimer que l'emprunteur présente un risque et donc préférer lui refuser le prêt, même s'il aurait pu ne pas faire défaut.\n",
    "\n",
    "Sur la plupart des mesures, les moins bons modèles sont `Dummy`, `QDA`, `DecisionTree`, `GaussianNB` et `KNN`, tandis que les meilleurs sont `LDA`, `LGBM` et `AutoSklearn`.\n",
    "\n",
    "Nous allons conserver le modèle `LGBM` qui a un temps de prédiction assez rapide et les meilleurs résultats.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explication du modèle\n",
    "\n",
    "Nous avons vu précédemment quelles étaient les variables les plus importantes pour le modèle, et nous avons même pu visualiser un des arbres de décision générés dans ce modèle ensembliste boosté.\n",
    "\n",
    "Nous allons maintenant observer comment les variables les plus importantes influencent les prédictions grâce aux méthodes [Partial Dependence (PD) et Individual Conditional Expectation (ICE)](https://scikit-learn.org/stable/modules/partial_dependence.html).\n",
    "\n",
    "Nous visualiserons ensuite comment notre modèle effectue une prédiction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if DRAW_PLOTS:\n",
    "    # Let's plot the partial dependance graphs of the most important variables\n",
    "    plot_partial_dependence(\n",
    "        estimator=lgbm_results['model'], \n",
    "        X=X_train, \n",
    "        subsample=100,\n",
    "        features=[\n",
    "            'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', \n",
    "            'ANNUITY_CREDIT_RATIO', 'ANNUITY_INCOME_RATIO',\n",
    "            'DAYS_BIRTH', 'DAYS_EMPLOYED',\n",
    "            'DAYS_LAST_PHONE_CHANGE', 'OWN_CAR_AGE',\n",
    "        ],\n",
    "        kind=\"both\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(24, 24)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons bien que les clients qui ont le moins de risque d'avoir des problèmes de remboursement sont :\n",
    "- les personnes ayant un `EXT_SOURCE_{1,3}` élevé\n",
    "- les personnes ayant une forte capacité d'emprunt (`ANNUITY_INCOME_RATIO` faible)\n",
    "- les personnes jeunes (`DAYS_BIRTH` élevé), mais embauchées depuis longtemps (`DAYS_EMPLOYED` faible)\n",
    "- les personnes possédant une voiture et un téléphone plutôt ancien (`OWN_CAR_AGE` et `DAYS_LAST_PHONE_CHANGE` faible )\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observons maintenant quels critères sont utilisés par notre modèle pour effectuer une prédiction.\n",
    "\n",
    "Nous allons utiliser dans un premier temps la méthode [Local Interpretable Model-agnostic Explanations (LIME)](https://christophm.github.io/interpretable-ml-book/lime.html#lime). Cette méthode permet de voir l'influence sur la prédiction de changements sur chaque variable de l'individu testé, afin d'évaluer l'importance de chaque variable dans la prise de décision. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "random_sample = 42\n",
    "\n",
    "# Create the exlainer\n",
    "explainer = LimeTabularExplainer(\n",
    "    X_train.to_numpy(), \n",
    "    feature_names=X_train.columns,\n",
    ")\n",
    "exp = explainer.explain_instance(\n",
    "    X_test.iloc[random_sample], \n",
    "    lgbm_results['model'].predict_proba, \n",
    ")\n",
    "\n",
    "print(f\"True value : { y_test.iloc[random_sample] }\")\n",
    "exp.show_in_notebook()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons que nous avons un niveau de confiance assez élevé que ce client n'aura pas de problème de paiement (prédiction que `TARGET=0` à 76%).\n",
    "\n",
    "Utilisons maintenant la méthode des [Shapley Values](https://christophm.github.io/interpretable-ml-book/shapley.html) qui permet aussi d'évaluer l'importance des variables dans la prédiction de la valeur cible."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(lgbm_results['model'])\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "print(f\"True value : { y_test.iloc[random_sample] }\")\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][random_sample,:], X_test.iloc[random_sample,:])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sur le même exemple que précédemment, nous voyons à nouveau que la classe prédite sera la classe \"négative\" (`TARGET=0`).\n",
    "\n",
    "Comme avec la méthode LIME, les variables expliquant cette décision sont :\n",
    "- `EXT_SOURCE_2` et `EXT_SOURCE_3` sont tous les deux élevés (`x1x2` élevé)\n",
    "- c'est une personne plutôt jeune (`DAYS_BIRTH` élevé)\n",
    "- la personne travaille dans une banque (`ORGANISATION_TYPE_Bank` élevé)\n",
    "- le ratio credit / prix est assez bas : la personne doit avoir un apport (`CREDIT_PRICE_RATIO` faible)\n",
    "- la personne est une femme (`CODE_GENDER_M` négatif)\n",
    "- le montant des annuités est assez faible (`AMT_ANNUITY` faible)\n",
    "\n",
    "En revanche, certains critères jouent tout de même contre cette prédiction :\n",
    "- `EXT_SOURCE_1` est plutôt faible\n",
    "- la personne s'est inscrite il y a plutôt longtemps (`DAYS_REGISTRATION` faible)\n",
    "- la combinaison `EXT_SOURCE_1` x `EXT_SOURCE_3` est faible (`x0x2` faible)\n",
    "\n",
    "La prédiction est effectivement correcte après vérification.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Annexe\n",
    "\n",
    "Les Notebooks Kaggle [Introduction: Home Credit Default Risk Competition](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction) (et suivants) de [Will Koehrsen](https://www.kaggle.com/willkoehrsen) ont été d'une très grande aide dans l'exploration des données.\n",
    "\n",
    "Comment bien choisir la métrique de score d'un algorithme de classification ?\n",
    "\n",
    "![How to Choose a Metric for Imbalanced Classification](https://machinelearningmastery.com/wp-content/uploads/2019/12/How-to-Choose-a-Metric-for-Imbalanced-Classification-latest.png)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87142f5ce831c365860d10ab1d6251a6bfb068b987aa68f91a7996e4fcdedd96"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}