{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prêt à dépenser : Construire un modèle de scoring\n",
    "\n",
    "\n",
    "## Contexte\n",
    "\n",
    "\"Prêt à dépenser\" (Home Credit) est une société financière qui propose des crédits à la consommation pour des personnes ayant peu ou pas d'historique de prêt.\n",
    "Pour accorder un crédit à la consommation, l'entreprise calcule la probabilité qu'un client le rembourse, ou non. Elle souhaite donc développer un algorithme de scoring pour aider à décider si un prêt peut être accordé à un client.\n",
    "\n",
    "Les chargés de relation client seront les utilisateurs du modèle de scoring. Puisqu'ils s'adressent aux clients, ils ont besoin que votre modèle soit facilement interprétable. Les chargés de relation souhaitent, en plus, disposer d'une mesure de l'importance des variables qui ont poussé le modèle à donner cette probabilité à un client.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chargement des modules du projet\n",
    "\n",
    "Afin de simplifier le Notebook, le code métier du projet est placé dans le répertoire [src/](../src/).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import project modules from source directory\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=[UserWarning])\n",
    "\n",
    "# System modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Append source directory to system path\n",
    "src_path = os.path.abspath(os.path.join(\"../src\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Helper functions\n",
    "import data.helpers as data_helpers\n",
    "import features.helpers as feat_helpers\n",
    "import visualization.helpers as vis_helpers\n",
    "import models.helpers as models_helpers\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons utiliser le langage [Python](https://www.python.org/about/gettingstarted/), et présenter ici le code, les résultats et l'analyse sous forme de [Notebook JupyterLab](https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html).\n",
    "\n",
    "Nous allons aussi utiliser les bibliothèques usuelles d'exploration et analyse de données, afin d'améliorer la simplicité et la performance de notre code :\n",
    "  * [NumPy](https://numpy.org/doc/stable/user/quickstart.html) et [Pandas](https://pandas.pydata.org/docs/user_guide/index.html) : effectuer des calculs scientifiques (statistiques, algèbre, ...) et manipuler des séries et tableaux de données volumineuses et complexes\n",
    "  * [scikit-learn](https://scikit-learn.org/stable/getting_started.html) et [XGBoost](https://xgboost.readthedocs.io/en/latest/get_started.html) : pour effectuer des analyses prédictives \n",
    "  * [Matplotlib](https://matplotlib.org/stable/tutorials/introductory/usage.html), [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html), [Seaborn](https://seaborn.pydata.org/tutorial/function_overview.html) et [Plotly](https://plotly.com/python/getting-started/) : générer des graphiques lisibles, intéractifs et pertinents\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Accelerate the development cycle\n",
    "SAMPLE_FRAC: float = 1\n",
    "\n",
    "# Prevent excessive memory usage used by plotly\n",
    "DRAW_PLOTS: bool = False\n",
    "\n",
    "\n",
    "ENCODE: bool = True\n",
    "ENGINEER: bool = False\n",
    "SIMPLIFY:bool = False\n",
    "SCALE:bool = False\n",
    "IMPUTE:bool = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chargement des données\n",
    "\n",
    "Les données mises à disposition sont issues de [Home Credit](https://www.homecredit.net/) et plus précisément de la compétition hébergée sur Kaggle [Home Credit Default Risk - Can you predict how capable each applicant is of repaying a loan?](https://www.kaggle.com/c/home-credit-default-risk)\n",
    "\n",
    "Les données sont fournies sous la forme de plusieurs fichiers CSV pouvant être liés entre eux de la manière suivante :\n",
    "\n",
    "![Home Credit data relations](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)\n",
    "\n",
    "_source : [Introduction: Home Credit Default Risk Competition](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction) by [Will Koehrsen](https://www.kaggle.com/willkoehrsen)_\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Download and extract the raw data\n",
    "data_helpers.download_extract_zip(\n",
    "    zip_file_url=\"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/Projet+-+Impl%C3%A9menter+un+mod%C3%A8le+de+scoring/Projet+Mise+en+prod+-+home-credit-default-risk.zip\",\n",
    "    files_names=(\n",
    "        \"application_test.csv\",\n",
    "        \"application_train.csv\",\n",
    "        \"bureau_balance.csv\",\n",
    "        \"bureau.csv\",\n",
    "        \"credit_card_balance.csv\",\n",
    "        \"installments_payments.csv\",\n",
    "        \"POS_CASH_balance.csv\",\n",
    "        \"previous_application.csv\",\n",
    "    ),\n",
    "    target_path=\"../data/raw/\",\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons charger toutes les données du fichier `application_train.csv` afin de travailler préparer les données du jeu d'entraînement et de test (variable `TARGET` vaut `O` : le client n'a pas fait défaut ou `1` : le client a fait défaut). Nous les séparerons les jeux de données au moment de l'entraînement et évaluation de nos modèles.\n",
    "\n",
    "Les fichiers contiennent un grand nombre de variables booléennes et catégorielles que nous pouvons déjà typer comme telles.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Read column names\n",
    "application_train_column_names = pd.read_csv(\n",
    "    \"../data/raw/application_train.csv\", nrows=0\n",
    ").columns.values\n",
    "\n",
    "# TARGET variable must be present in the Train datase\n",
    "if \"TARGET\" not in application_train_column_names:\n",
    "    raise ValueError(\n",
    "        \"TARGET column not found in application_train.csv. Please check that the file is not corrupted.\"\n",
    "    )\n",
    "\n",
    "# SK_ID_CURR variable must be present in the Train datase\n",
    "if \"SK_ID_CURR\" not in application_train_column_names:\n",
    "    raise ValueError(\n",
    "        \"SK_ID_CURR column not found in application_train_column_names.csv. Please check that the file is not corrupted.\"\n",
    "    )\n",
    "\n",
    "# Set column types according to fields description (../data/raw/HomeCredit_columns_description.csv)\n",
    "# Categorical variables\n",
    "column_types = {\n",
    "    col: \"category\"\n",
    "    for col in application_train_column_names\n",
    "    if col.startswith((\"NAME_\",))\n",
    "    or col.endswith((\"_TYPE\"))\n",
    "    or col\n",
    "    in [\n",
    "        \"CODE_GENDER\",\n",
    "        \"WEEKDAY_APPR_PROCESS_START\",\n",
    "        \"FONDKAPREMONT_MODE\",\n",
    "        \"HOUSETYPE_MODE\",\n",
    "        \"WALLSMATERIAL_MODE\",\n",
    "        \"EMERGENCYSTATE_MODE\",\n",
    "    ]\n",
    "}\n",
    "# Boolean variables\n",
    "column_types |= {\n",
    "    col: bool\n",
    "    for col in application_train_column_names\n",
    "    if col.startswith((\"FLAG_\", \"REG_\", \"LIVE_\"))\n",
    "}\n",
    "\n",
    "# Load application data\n",
    "app_df = pd.read_csv(\n",
    "    \"../data/raw/application_train.csv\",\n",
    "    dtype=column_types,\n",
    "    true_values=[\"Y\", \"Yes\", \"1\"],\n",
    "    false_values=[\"N\", \"No\", \"0\"],\n",
    "    na_values=[\"XNA\"], # bad values\n",
    ")\n",
    "\n",
    "# Sample to speed up development\n",
    "if  float == type(SAMPLE_FRAC) and 0 < SAMPLE_FRAC < 1:\n",
    "    app_df = app_df.sample(frac=SAMPLE_FRAC)\n",
    "\n",
    "# Let's display basic statistical info about the data\n",
    "app_df.describe(include=\"all\")\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>...</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>307511.000000</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>307511</td>\n",
       "      <td>307507</td>\n",
       "      <td>307511</td>\n",
       "      <td>307511</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>3.075110e+05</td>\n",
       "      <td>3.075110e+05</td>\n",
       "      <td>307499.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>307511</td>\n",
       "      <td>307511</td>\n",
       "      <td>307511</td>\n",
       "      <td>307511</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>278232</td>\n",
       "      <td>202448</td>\n",
       "      <td>202924</td>\n",
       "      <td>213312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>305011</td>\n",
       "      <td>307328</td>\n",
       "      <td>307355</td>\n",
       "      <td>307408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>278180.518577</td>\n",
       "      <td>0.080729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.417052</td>\n",
       "      <td>1.687979e+05</td>\n",
       "      <td>5.990260e+05</td>\n",
       "      <td>27108.573909</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.034362</td>\n",
       "      <td>0.267395</td>\n",
       "      <td>0.265474</td>\n",
       "      <td>1.899974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>102790.175348</td>\n",
       "      <td>0.272419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.722121</td>\n",
       "      <td>2.371231e+05</td>\n",
       "      <td>4.024908e+05</td>\n",
       "      <td>14493.737315</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083849</td>\n",
       "      <td>0.110757</td>\n",
       "      <td>0.204685</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.794056</td>\n",
       "      <td>1.869295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>100002.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.565000e+04</td>\n",
       "      <td>4.500000e+04</td>\n",
       "      <td>1615.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>189145.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.125000e+05</td>\n",
       "      <td>2.700000e+05</td>\n",
       "      <td>16524.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>278202.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.471500e+05</td>\n",
       "      <td>5.135310e+05</td>\n",
       "      <td>24903.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>367142.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.025000e+05</td>\n",
       "      <td>8.086500e+05</td>\n",
       "      <td>34596.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>456255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.170000e+08</td>\n",
       "      <td>4.050000e+06</td>\n",
       "      <td>258025.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           SK_ID_CURR         TARGET NAME_CONTRACT_TYPE CODE_GENDER  \\\n",
       "count   307511.000000  307511.000000             307511      307507   \n",
       "unique            NaN            NaN                  2           2   \n",
       "top               NaN            NaN         Cash loans           F   \n",
       "freq              NaN            NaN             278232      202448   \n",
       "mean    278180.518577       0.080729                NaN         NaN   \n",
       "std     102790.175348       0.272419                NaN         NaN   \n",
       "min     100002.000000       0.000000                NaN         NaN   \n",
       "25%     189145.500000       0.000000                NaN         NaN   \n",
       "50%     278202.000000       0.000000                NaN         NaN   \n",
       "75%     367142.500000       0.000000                NaN         NaN   \n",
       "max     456255.000000       1.000000                NaN         NaN   \n",
       "\n",
       "       FLAG_OWN_CAR FLAG_OWN_REALTY   CNT_CHILDREN  AMT_INCOME_TOTAL  \\\n",
       "count        307511          307511  307511.000000      3.075110e+05   \n",
       "unique            2               2            NaN               NaN   \n",
       "top           False            True            NaN               NaN   \n",
       "freq         202924          213312            NaN               NaN   \n",
       "mean            NaN             NaN       0.417052      1.687979e+05   \n",
       "std             NaN             NaN       0.722121      2.371231e+05   \n",
       "min             NaN             NaN       0.000000      2.565000e+04   \n",
       "25%             NaN             NaN       0.000000      1.125000e+05   \n",
       "50%             NaN             NaN       0.000000      1.471500e+05   \n",
       "75%             NaN             NaN       1.000000      2.025000e+05   \n",
       "max             NaN             NaN      19.000000      1.170000e+08   \n",
       "\n",
       "          AMT_CREDIT    AMT_ANNUITY  ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19  \\\n",
       "count   3.075110e+05  307499.000000  ...            307511           307511   \n",
       "unique           NaN            NaN  ...                 2                2   \n",
       "top              NaN            NaN  ...             False            False   \n",
       "freq             NaN            NaN  ...            305011           307328   \n",
       "mean    5.990260e+05   27108.573909  ...               NaN              NaN   \n",
       "std     4.024908e+05   14493.737315  ...               NaN              NaN   \n",
       "min     4.500000e+04    1615.500000  ...               NaN              NaN   \n",
       "25%     2.700000e+05   16524.000000  ...               NaN              NaN   \n",
       "50%     5.135310e+05   24903.000000  ...               NaN              NaN   \n",
       "75%     8.086500e+05   34596.000000  ...               NaN              NaN   \n",
       "max     4.050000e+06  258025.500000  ...               NaN              NaN   \n",
       "\n",
       "       FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
       "count            307511           307511              265992.000000   \n",
       "unique                2                2                        NaN   \n",
       "top               False            False                        NaN   \n",
       "freq             307355           307408                        NaN   \n",
       "mean                NaN              NaN                   0.006402   \n",
       "std                 NaN              NaN                   0.083849   \n",
       "min                 NaN              NaN                   0.000000   \n",
       "25%                 NaN              NaN                   0.000000   \n",
       "50%                 NaN              NaN                   0.000000   \n",
       "75%                 NaN              NaN                   0.000000   \n",
       "max                 NaN              NaN                   4.000000   \n",
       "\n",
       "       AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
       "count              265992.000000               265992.000000   \n",
       "unique                       NaN                         NaN   \n",
       "top                          NaN                         NaN   \n",
       "freq                         NaN                         NaN   \n",
       "mean                    0.007000                    0.034362   \n",
       "std                     0.110757                    0.204685   \n",
       "min                     0.000000                    0.000000   \n",
       "25%                     0.000000                    0.000000   \n",
       "50%                     0.000000                    0.000000   \n",
       "75%                     0.000000                    0.000000   \n",
       "max                     9.000000                    8.000000   \n",
       "\n",
       "        AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
       "count               265992.000000              265992.000000   \n",
       "unique                        NaN                        NaN   \n",
       "top                           NaN                        NaN   \n",
       "freq                          NaN                        NaN   \n",
       "mean                     0.267395                   0.265474   \n",
       "std                      0.916002                   0.794056   \n",
       "min                      0.000000                   0.000000   \n",
       "25%                      0.000000                   0.000000   \n",
       "50%                      0.000000                   0.000000   \n",
       "75%                      0.000000                   0.000000   \n",
       "max                     27.000000                 261.000000   \n",
       "\n",
       "        AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "count                265992.000000  \n",
       "unique                         NaN  \n",
       "top                            NaN  \n",
       "freq                           NaN  \n",
       "mean                      1.899974  \n",
       "std                       1.869295  \n",
       "min                       0.000000  \n",
       "25%                       0.000000  \n",
       "50%                       1.000000  \n",
       "75%                       3.000000  \n",
       "max                      25.000000  \n",
       "\n",
       "[11 rows x 122 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "app_df.info()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 307511 entries, 0 to 307510\n",
      "Columns: 122 entries, SK_ID_CURR to AMT_REQ_CREDIT_BUREAU_YEAR\n",
      "dtypes: bool(34), category(14), float64(65), int64(9)\n",
      "memory usage: 187.7 MB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le jeu de données contient 122 variables, dont la variable cible que nous devons estimer : `TARGET`. Parmis ces variables, nous avons :\n",
    "- 34 variables booléennes\n",
    "- 14 variables catégorielles\n",
    "- 74 variables numériques"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse exploratoire\n",
    "\n",
    "Nous allons analyser la distribution de quelques variables.\n",
    "\n",
    "### Variable cible\n",
    "\n",
    "Voyons spécifiquement la distribution de la variable `TARGET` qui est celle que nous devrons estimer par la suite.\n",
    "Les valeurs nulles représentent notre jeu d'entrainement.\n",
    "Nous pouvons oberver que nous avons à faire à un problème de __classification binaire déséquilibré__ (il y a deux valeurs possibles, mais les deux valeurs ne sont pas également représentées).\n",
    "Ceci va influencer la manière dont nous allons construire et entraîner notre modèle.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Let's plot the distribution of the TARGET variable\n",
    "if DRAW_PLOTS:\n",
    "    fig = px.bar(\n",
    "        app_df[\"TARGET\"].replace({\n",
    "            \"0\": \"TARGET=0 : payments OK\", \n",
    "            \"1\": \"TARGET=1 : payment difficulties\", \n",
    "        }).value_counts(),\n",
    "        title=\"Distribution of TARGET variable\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "    ).update_xaxes(\n",
    "        title=\"TARGET\",\n",
    "    ).update_yaxes(title=\"Count\")\n",
    "    fig.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Valeurs vides\n",
    "\n",
    "Nous voyons que toutes les variables ont moins de 30% de valeurs vides, et près de la moitié a moins de 1% de valeurs vides. Le jeu de données est donc relativement bien rempli, ce qui ne devrait pas poser de problème pour la suite.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Let's display variables with missing values ratio\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_empty_values(app_df)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Valeurs impossibles\n",
    "\n",
    "Quelques valeurs présentes dans les données semblent impossibles. Nous allons supprimer ces \"outliers\".\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Define data constraints\n",
    "data_constraints = {\n",
    "    \"DAYS_EMPLOYED\": {\"min\": -35000, \"max\": 0,}, # max 100 years, only negative values\n",
    "}\n",
    "\n",
    "if DRAW_PLOTS:\n",
    "    # Let's display box plots for variables with outliers\n",
    "    vis_helpers.plot_boxes(app_df, plot_columns=data_constraints.keys(), categorical_column=\"TARGET\")\n",
    "\n",
    "# Remove values that are outside possible range\n",
    "app_df = feat_helpers.drop_impossible_values(\n",
    "    app_df, constraints=data_constraints,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables quantitatives\n",
    "\n",
    "Nous allons simplement afficher la distribution de quelques variables numériques. Nous voyons déjà que selon la valeur de TARGET, la distribution (moyenne) des variables peut être sensiblement différente.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Draw the BoxPlots of some numeric columns, split per Target\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_boxes(app_df,\n",
    "        plot_columns=[\n",
    "            \"AMT_INCOME_TOTAL\",\n",
    "            \"AMT_CREDIT\",\n",
    "            \"AMT_ANNUITY\",\n",
    "            \"AMT_GOODS_PRICE\",\n",
    "            \"DAYS_BIRTH\",\n",
    "            \"DAYS_EMPLOYED\",\n",
    "            \"OWN_CAR_AGE\",\n",
    "            \"REGION_RATING_CLIENT\",\n",
    "            \"REGION_RATING_CLIENT_W_CITY\",\n",
    "            \"EXT_SOURCE_1\",\n",
    "            \"EXT_SOURCE_2\",\n",
    "            \"EXT_SOURCE_3\",\n",
    "            \"DAYS_LAST_PHONE_CHANGE\",\n",
    "            \"AMT_REQ_CREDIT_BUREAU_YEAR\",\n",
    "        ],\n",
    "        categorical_column=\"TARGET\",\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables qualitatives\n",
    "\n",
    "De la même manière, nous allons simplement afficher la distribution de quelques variables catégorielles. Nous voyons déjà que selon la valeur de TARGET, la distribution (répartition entre classes) des variables peut être sensiblement différente (`TARGET=0` pour 77,6% des `NAME_CONTRACT_TYPE=\"Cash loans\"`, tandis que `TARGET=0` pour 93,1% des `NAME_CONTRACT_TYPE=\"Revolving loans\"`).\n",
    "Certaines variables ont une répartition très inégale entre classes (`FLAG_MOBIL` vaut systématiquement `True` et jamais `False`).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Draw the Bar charts of some categorical columns, split per Target\n",
    "if DRAW_PLOTS:\n",
    "    vis_helpers.plot_categories_bars(app_df,\n",
    "        plot_columns=[\n",
    "            \"NAME_CONTRACT_TYPE\",\n",
    "            \"CODE_GENDER\",\n",
    "            \"FLAG_OWN_CAR\",\n",
    "            \"FLAG_OWN_REALTY\",\n",
    "            \"NAME_INCOME_TYPE\",\n",
    "            \"NAME_EDUCATION_TYPE\",\n",
    "            \"NAME_FAMILY_STATUS\",\n",
    "            \"NAME_HOUSING_TYPE\",\n",
    "            \"OCCUPATION_TYPE\",\n",
    "            \"FLAG_MOBIL\",\n",
    "        ],\n",
    "        categorical_column=\"TARGET\",\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encodage des catégories\n",
    "\n",
    "Lorsque les données qualitatives ne sont pas ordinales (on ne peu pas les classer selon un certain ordre), l'encodage \"One Hot Encoding\" sera plus performant que le \"Label Encoding\".\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Encode categorical variables with One Hot Encoding\n",
    "if ENCODE:\n",
    "    app_df = pd.get_dummies(app_df, dtype=bool)\n",
    "\n",
    "    app_df.describe(include=\"bool\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous avons ici créé 134 nouvelles variables booléennes qui correspondent aux différentes classes de chacune des 14 anciennes variables catégorielles qui ont été encodées et supprimées.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Nous allons tenter d'enrichir nos données en intégrant des variables qui sont des compositions non linéaires des variables existantes.\n",
    "\n",
    "\n",
    "### Données métier\n",
    "\n",
    "Afin d'apporter plus de sens aux données que nous allons fournir à nos modèles, nous pouvons faire appel aux experts métier qui peuvent nous indiquer des informations qui sont réputées importantes afin de prédire si un client risque d'avoir des problèmes de remboursement ou non.\n",
    "\n",
    "Les informations métier pertinentes sont :\n",
    "- Montant emprunté / Prix du bien acheté : `AMT_CREDIT / AMT_GOODS_PRICE`\n",
    "- Montant des annuités / Montant emprunté : `AMT_ANNUITY / AMT_CREDIT`\n",
    "- Montant des annuités / Revenu annuel : `AMT_ANNUITY / AMT_INCOME_TOTAL`\n",
    "- Ancienneté au travail / Age : `DAYS_EMPLOYED / DAYS_BIRTH`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Create the new features\n",
    "if ENGINEER:\n",
    "    app_df[\"CREDIT_PRICE_RATIO\"] = app_df[\"AMT_CREDIT\"] / app_df[\"AMT_GOODS_PRICE\"]\n",
    "    app_df[\"ANNUITY_CREDIT_RATIO\"] = app_df[\"AMT_ANNUITY\"] / app_df[\"AMT_CREDIT\"]\n",
    "    app_df[\"ANNUITY_INCOME_RATIO\"] = app_df[\"AMT_ANNUITY\"] / app_df[\"AMT_INCOME_TOTAL\"]\n",
    "    app_df[\"EMPLOYED_BIRTH_RATIO\"] = app_df[\"DAYS_EMPLOYED\"] / app_df[\"DAYS_BIRTH\"]\n",
    "\n",
    "    # Draw the BoxPlots for these features\n",
    "    if DRAW_PLOTS:\n",
    "        vis_helpers.plot_boxes(app_df,\n",
    "            plot_columns=[\n",
    "                \"CREDIT_PRICE_RATIO\",\n",
    "                \"ANNUITY_CREDIT_RATIO\",\n",
    "                \"ANNUITY_INCOME_RATIO\",\n",
    "                \"EMPLOYED_BIRTH_RATIO\",\n",
    "            ],\n",
    "            categorical_column=\"TARGET\",\n",
    "        )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composition polynomiales de variables existantes\n",
    "\n",
    "Les variables `EXT_SOURCE_{1-3}` n'ont a priori pas de sens concret. On peut imaginer que `TARGET` ne soit pas forcément linéairement dépendant de ces variables. Nous allons donc générer des combinaisons polynomiales de ces variables.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "if ENGINEER:\n",
    "    # Let's keep only non null data\n",
    "    ext_source = app_df[[\"SK_ID_CURR\", \"TARGET\", \"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]].dropna()\n",
    "\n",
    "    # Let's create the new features\n",
    "    poly = PolynomialFeatures()\n",
    "    poly_feat = pd.DataFrame(poly.fit_transform(\n",
    "            X=ext_source[[\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]],\n",
    "            y=ext_source[\"TARGET\"],\n",
    "        )\n",
    "    )\n",
    "    poly_feat.columns=poly.get_feature_names()\n",
    "\n",
    "    poly_feat.insert(0, \"SK_ID_CURR\", ext_source[\"SK_ID_CURR\"].values)\n",
    "\n",
    "    # Merge the new features with the original dataset\n",
    "    app_df = app_df.merge(\n",
    "        poly_feat,\n",
    "        on=\"SK_ID_CURR\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Draw the BoxPlots for these features\n",
    "    if DRAW_PLOTS:\n",
    "        vis_helpers.plot_boxes(app_df,\n",
    "            plot_columns=poly_feat.columns[5:],\n",
    "            categorical_column=\"TARGET\",\n",
    "        )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Features selection\n",
    "\n",
    "Le but ici est d'éliminer un certain nombre de variables afin d'accélérer l'entrainement et la prédiction de nos modèles. Nous souhaitons éliminer les variables qui pénaliseront le moins possible les performances de nos modèles.\n",
    "\n",
    "Nous savons déjà que les colonnes `SK_ID_CURR` (simple identifiant sans sens métier), `1` et `x{0-2}` (variables polynomiales d'ordre 0 et 1), et `FLAG_MOBIL` (vaut toujours 1) n'apportent pas d'information. Nous allons donc les éliminer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's drop the features that are not useful for the prediction\n",
    "if SIMPLIFY:\n",
    "    app_df = app_df.drop(\n",
    "        columns=[\"SK_ID_CURR\", \"1\", \"x0\", \"x1\", \"x2\", \"FLAG_MOBIL\"]\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons ici observer la corrélation :\n",
    "- de chaque variable avec la variables cible `TARGET` : les variables les moins corrélées à `TARGET` seront a priori les moins utiles pour prédire sa valeur.\n",
    "- entre les différentes variables deux à deux : si deux variables sont très corrélées, elles apportent une information redondante et nous pouvons donc en éliminer une des deux.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's compute the correlation matrix\n",
    "if SIMPLIFY:\n",
    "    app_correlations = app_df.corr().abs().sort_values(\n",
    "        \"TARGET\", ascending=False, axis=0\n",
    "    ).sort_values(\n",
    "        \"TARGET\", ascending=False, axis=1\n",
    "    )\n",
    "\n",
    "    if DRAW_PLOTS:\n",
    "        fig = px.imshow(app_correlations,\n",
    "            title=\"Correlations between features\",\n",
    "            width=1200,\n",
    "            height=1200,\n",
    "        )\n",
    "        fig.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous voyons qu'il y a des variables très peu corrélées à `TARGET` (ex. : `abs(corr(\"TARGET\", \"FLAG_EMP_PHONE\")) < 0.0001`), et d'autres très corrélées entre elles (ex. : `abs(corr(\"LIVINGAPARTMENTS_AVG\", \"\"LIVINGAPARTMENTS_MEDI\")) > 0.99`).\n",
    "Nous allons simplifier notre jeu de données en supprimant ces variables.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's find variables that are highly de-correlated from TARGET\n",
    "if SIMPLIFY:\n",
    "    corr_target_min_threshold = 0.01\n",
    "    highly_decorrelated_from_target = pd.DataFrame(columns=[\"correlation with TARGET\"])\n",
    "    for col in app_correlations.columns:\n",
    "        if col != \"TARGET\" and (\n",
    "            pd.isnull(app_correlations[col][\"TARGET\"])\n",
    "            or abs(app_correlations[col][\"TARGET\"]) < corr_target_min_threshold\n",
    "        ):\n",
    "            highly_decorrelated_from_target.loc[col] = {\n",
    "                \"correlation with TARGET\": app_correlations[col][\"TARGET\"],\n",
    "            }\n",
    "\n",
    "    highly_decorrelated_from_target.sort_values(by=[\"correlation with TARGET\"])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's find variables that have a highly correlated pair\n",
    "if SIMPLIFY:\n",
    "    corr_pair_max_threshold = 0.9\n",
    "    highly_correlated = pd.DataFrame(columns=[\"pair\", \"correlation\"])\n",
    "    for i in range(len(app_correlations.columns)):\n",
    "        for j in range(i + 1, len(app_correlations.columns)):\n",
    "            if app_correlations.iloc[i, j] > corr_pair_max_threshold:\n",
    "                # variables are highly correlated\n",
    "                if app_correlations.iloc[0, i] > app_correlations.iloc[0, j]:\n",
    "                    # first variable is more correlated with target => we want to keep it\n",
    "                    keep_index = i\n",
    "                    drop_index = j\n",
    "                else:\n",
    "                    keep_index = j\n",
    "                    drop_index = i\n",
    "\n",
    "                highly_correlated.loc[app_correlations.columns[drop_index]] = {\n",
    "                    \"pair\": app_correlations.columns[keep_index],\n",
    "                    \"correlation\": app_correlations.iloc[i, j],\n",
    "                }\n",
    "\n",
    "    highly_correlated.sort_values(by=\"correlation\", ascending=False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Drop irrelevant columns\n",
    "if SIMPLIFY:\n",
    "    app_df.drop(\n",
    "        columns=highly_decorrelated_from_target.index,\n",
    "        inplace=True,\n",
    "        errors=\"ignore\",\n",
    "    )\n",
    "    app_df.drop(\n",
    "        columns=highly_correlated.index,\n",
    "        inplace=True,\n",
    "        errors=\"ignore\",\n",
    "    )\n",
    "\n",
    "    app_df.describe(include=\"all\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Après simplification, nous voyons que nous avons drastiquement réduit le nombre de variables pour ne conserver que celles réellement pertinentes pour la prédiction de `TARGET`.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Préparation des données\n",
    "\n",
    "Nous allons transformer les données pour que nos modèles puissent les exploiter au mieux. Afin d'éviter la \"fuite d'information\" entre le jeu de données d'entraînement et de test, nous allons maintenant séparer notre jeu de données en deux. Les transformations seront apprises uniquement sur le jeu d'entraînement, mais appliquées aux deux jeux de données (entraînement et test).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Let's rename columns to remove special characters\n",
    "app_df = app_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "# Given data\n",
    "X = app_df.drop([\"TARGET\"], axis=1)\n",
    "# Data to predict\n",
    "y = app_df[\"TARGET\"]\n",
    "\n",
    "# Let's split the whole dataset into a training set (80% of data) and a test set (20% of data)\n",
    "# The dataset will be split in a stratified way, in order to have a good distribution of the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Save the processed data\n",
    "X_train.to_csv(\"../data/processed/X_train.csv\", index=False)\n",
    "X_test.to_csv(\"../data/processed/X_test.csv\", index=False)\n",
    "y_train.to_csv(\"../data/processed/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"../data/processed/y_test.csv\", index=False)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalisation des données\n",
    "\n",
    "Afin d'éviter que certains modèles pondèrent l'importance de certaines variables à cause de leur ordre de grandeur, nous allons normaliser chaque variable afin de les ramener à une moyenne nulle et une variance de 1.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Scale each variable of the DataFrame\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "if SCALE:\n",
    "    # define scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # fit scaler on train data only, to avoid data leak\n",
    "    X_train = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train),\n",
    "        columns=X.columns,\n",
    "    )\n",
    "    X_test = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=X.columns,\n",
    "    )\n",
    "\n",
    "    # Save the processed data\n",
    "    X_train.to_csv(\"../data/processed/X_train_scaled.csv\", index=False)\n",
    "    X_test.to_csv(\"../data/processed/X_test_scaled.csv\", index=False)\n",
    "\n",
    "    X_train.describe(include=\"all\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous avons ici équilibré les ordres de grandeur de chaque variables, afin que nos futurs modèles ne soient pas influencés par leur différence.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imputation des valeurs manquantes\n",
    "\n",
    "Afin d'éviter que certains modèles ne puissent être utilisés à cause des valeurs manquantes, nous allons remplacer toutes les valeurs nulles par leur meilleure estimation possible.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Impute missing values by modeling each feature with missing values as a function of other features in a round-robin fashion\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "if IMPUTE:\n",
    "    if os.path.exists(\"../data/processed/X_train_imputed.csv\") and os.path.exists(\"../data/processed/X_test_imputed.csv\"):\n",
    "        X_train = pd.read_csv(\"../data/processed/X_train_imputed.csv\")\n",
    "        X_test = pd.read_csv(\"../data/processed/X_test_imputed.csv\")\n",
    "    else:\n",
    "        # define imputer\n",
    "        imputer = IterativeImputer(\n",
    "            n_nearest_features=min(5, int(len(X.columns) / 10)),\n",
    "        )\n",
    "\n",
    "        # fit imputer on train data only, to avoid data leak\n",
    "        X_train = pd.DataFrame(\n",
    "            imputer.fit_transform(X_train),\n",
    "            columns=X.columns,\n",
    "        )\n",
    "        X_test = pd.DataFrame(\n",
    "            imputer.transform(X_test),\n",
    "            columns=X.columns,\n",
    "        )\n",
    "\n",
    "        # Save the processed data\n",
    "        X_train.to_csv(\"../data/processed/X_train_imputed.csv\", index=False)\n",
    "        X_test.to_csv(\"../data/processed/X_test_imputed.csv\", index=False)\n",
    "\n",
    "    X_train.describe(include=\"all\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modélisation et évaluation des modèles\n",
    "\n",
    "Nous allons ici entraîner différents modèles de classification binaire et évaluer leur efficacité selon trois critères :\n",
    "- le **temps d'exécution** : temps nécessaire au modèle pour les phases d'apprentissage (`fit`) et de prédiction (`predict`).\n",
    "- l'**interprétabilité** du modèle : la capacité du modèle à nous indiquer quelles sont les variables qui expliquent le mieux la prédiction.\n",
    "- la **performance** : nous allons mesurer différentes métriques (`accuracy` = taux de prédictions positives ou négatives correctes, `precision` = taux de prédictions positives correctes, `recall` = taux de positifs correctement prédits, `F1 score` = moyenne harmonique de la `precision` et du `recall`) pour chaque modèle. Nous utiliserons le `F1 score` comme mesure principale de la performance, car nous sommes face à un problème où les deux classes de la variable prédite ne sont pas équilibrées (`TARGET=0 ~ 92%`) dans le jeu de données, et nous voulons maximiser en priorité le `recall` (bien prédire les problèmes de paiement). Nous tracerons les courbes de `Precision-Recall` et de `ROC (Receiver operating characteristic)` qui compare le `recall` au taux de faux positifs.\n",
    "\n",
    "Afin d'améliorer la qualité du modèle et minimiser le biais de sélection du jeu d'entraînement, nous allons utiliser la méthode `StratifiedKFold` qui permet de \"mixer\" les résultats sur plusieurs sous-ensembles du jeu d'entraînement.\n",
    "\n",
    "Afin de chercher les meilleurs hyper-paramètres de chacun des modèles, nous allons "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# let's store the results of each model in a DataFrame\n",
    "results = pd.DataFrame(columns=(\n",
    "    'model',\n",
    "    'params',\n",
    "    'score',\n",
    "    'time',\n",
    "    'cv_results_',\n",
    "    'best_index_',\n",
    "    'confusion_matrix',\n",
    "    'f1',\n",
    "    'accuracy',\n",
    "    'precision',\n",
    "    'recall',\n",
    "    'average_precision',\n",
    "    'precision_recall_curve',\n",
    "    'roc_auc_score',\n",
    "    'roc_curve',\n",
    "))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèle de référence\n",
    "\n",
    "Nous allons utiliser un modèle bête et méchant qui se contente de faire des prédictions aléatoires.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Dummy Classifier\n",
    "\n",
    "# DummyClassifier permet de faire des prédictions aléatoires.\n",
    "\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "best_model_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    estimator=DummyClassifier(random_state=42),\n",
    "    params={\n",
    "        'strategy': ['stratified', 'most_frequent', 'prior', 'uniform']\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": best_model_score['params'], \n",
    "    \"score\": best_model_score['score'],\n",
    "    \"predict_time\": best_model_score['predict_time'],\n",
    "})\n",
    "results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=best_model_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèle linéaire\n",
    "\n",
    "Nous allons utiliser un modèle linéaire\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Ridge Regression\n",
    "# Prevents overfitting by penalizing the magnitude of coefficients with L2 regularization of the loss function\n",
    "\n",
    "# - La norme ℓ2 du vecteur de poids peut être utilisée comme terme de régularisation de la régression linéaire.\n",
    "# - Cela s'appelle la régularisation de Tykhonov, ou régression ridge.\n",
    "# - La régression ridge admet toujours une solution analytique unique.\n",
    "# - La régression ridge permet d'éviter le surapprentissage en restraignant l'amplitude des poids.\n",
    "# - La régression ridge a un effet de sélection groupée : les variables corrélées ont le même coefficient.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "if ENCODE and IMPUTE:\n",
    "    # Compute scores\n",
    "    best_model_score = models_helpers.find_best_params_classifier(\n",
    "        X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "        estimator=RidgeClassifier(random_state=42),\n",
    "        params={\n",
    "            'alpha': np.logspace(-10, 10, 50),\n",
    "            'fit_intercept': [True, False],\n",
    "            'normalize': [True, False],\n",
    "            'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "        },\n",
    "    )\n",
    "    print({\n",
    "        \"params\": best_model_score['params'], \n",
    "        \"score\": best_model_score['score'],\n",
    "        \"predict_time\": best_model_score['predict_time'],\n",
    "    })\n",
    "    results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "    # Plot best model curves\n",
    "    vis_helpers.plot_classifier_results(\n",
    "        classifier=best_model_score['model'],\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if ENCODE and IMPUTE:\n",
    "    top_coefficients = pd.Series(\n",
    "        best_model_score[\n",
    "            'model'\n",
    "        ].coef_[0],\n",
    "        X_train.columns,\n",
    "    ).map(abs).sort_values(ascending=False).head(20)\n",
    "\n",
    "    if DRAW_PLOTS:\n",
    "        fig = px.bar(\n",
    "            top_coefficients,\n",
    "            color=top_coefficients.values,\n",
    "            title=\"Top 20 variables importance\",\n",
    "            labels={\n",
    "                \"index\": \"Variable name\",\n",
    "                \"value\": \"Coefficient\",\n",
    "                \"color\": \"Coefficient\",\n",
    "            },\n",
    "            width=1200,\n",
    "            height=800,\n",
    "        )\n",
    "        fig.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèle Logistique\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logistic Regression\n",
    "#\n",
    "# - La régression logistique modélise la probabilité qu'une observation appartienne à la classe positive comme une transformation logistique d'une combinaison linéaire des variables.\n",
    "# - Les coefficients d'une régression logistique s'apprennent par maximisation de vraisemblance, mais il n'existe pas de solution explicite.\n",
    "# - La vraisemblance est convexe, et de nombreux solveurs peuvent être utilisés pour trouver une solution numérique.\n",
    "# - Les concepts de régularisation ℓ1 et ℓ2 s'appliquent aussi à la régression logistique.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "if ENCODE and IMPUTE:\n",
    "    # Compute scores\n",
    "    best_model_score = models_helpers.find_best_params_classifier(\n",
    "        X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "        estimator=LogisticRegression(random_state=42),\n",
    "        params={\n",
    "            'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "            'C': np.logspace(-8, 1, 20),\n",
    "            'l1_ratio': np.linspace(0.00001, 0.99999, 10),\n",
    "            'solver': ['saga']\n",
    "        },\n",
    "    )\n",
    "    print({\n",
    "        \"params\": best_model_score['params'], \n",
    "        \"score\": best_model_score['score'],\n",
    "        \"predict_time\": best_model_score['predict_time'],\n",
    "    })\n",
    "    results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "    # Plot best model curves\n",
    "    vis_helpers.plot_classifier_results(\n",
    "        classifier=best_model_score['model'],\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if ENCODE and IMPUTE:\n",
    "    top_coefficients = pd.Series(\n",
    "        best_model_score[\n",
    "            'model'\n",
    "        ].coef_[0],\n",
    "        X_train.columns,\n",
    "    ).map(abs).sort_values(ascending=False).head(20)\n",
    "\n",
    "    if True or DRAW_PLOTS:\n",
    "        fig = px.bar(\n",
    "            top_coefficients,\n",
    "            color=top_coefficients.values,\n",
    "            title=\"Top 20 variables importance\",\n",
    "            labels={\n",
    "                \"index\": \"Variable name\",\n",
    "                \"value\": \"Coefficient\",\n",
    "                \"color\": \"Coefficient\",\n",
    "            },\n",
    "            width=1200,\n",
    "            height=800,\n",
    "        )\n",
    "        fig.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if ENCODE and IMPUTE:\n",
    "    c_range = np.logspace(-8, 1, 20)\n",
    "    l1_ratio = best_model_score['params']['l1_ratio']\n",
    "\n",
    "    coefficients = pd.DataFrame(index=X_train.columns, columns=c_range)\n",
    "    errors = []\n",
    "    for c in c_range:\n",
    "        logistic = LogisticRegression(C=c, l1_ratio=l1_ratio)\n",
    "        logistic.fit(X_train, y_train)\n",
    "        coefficients.loc[:, c] = logistic.coef_[0]\n",
    "        errors.append(mean_squared_error(y_test, logistic.predict(X_test)))\n",
    "\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for col in coefficients.index:\n",
    "        fig.add_trace(go.Scatter(x=c_range, y=coefficients.loc[col, :], name=col,))\n",
    "\n",
    "    fig.update_xaxes(type=\"log\", autorange=\"reversed\")\n",
    "    fig.update_layout(\n",
    "        title=\"Logistic regression coefficients as a function of the regularization\",\n",
    "        xaxis_title=\"log(C)\",\n",
    "        yaxis_title=\"coefficient\",\n",
    "        width=1200,\n",
    "        height=800,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=c_range, y=errors, name=\"MSE\"))\n",
    "    fig.update_xaxes(type=\"log\", autorange=\"reversed\")\n",
    "    fig.update_layout(\n",
    "        title=\"Logistic regression MSE as a function of the regularization\",\n",
    "        xaxis_title=\"log(C)\",\n",
    "        yaxis_title=\"MSE\",\n",
    "        width=1200,\n",
    "        height=800,\n",
    "    )\n",
    "    fig.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèle Support Vector Machine \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "# Linear SVC\n",
    "# - Les SVM (Support Vector Machines), aussi appelées en français Machines à Vecteurs de Support et parfois Séparatrices à Vaste Marge, cherchent à séparer linéairement les données.\n",
    "# - La version primale résout un problème d'optimisation à p variables et est donc préférable si on a moins de variables que d'échantillons.\n",
    "# - À l'inverse, la version duale résout un problème d'optimisation à n variables et est donc préférable si on a moins d'échantillons que de variables.\n",
    "# - Les vecteurs de support sont les points du jeu de données qui sont les plus proches de l'hyperplan séparateur.\n",
    "# - La fonction de décision peut s'exprimer uniquement en fonction du produit scalaire du point à étiqueter avec les vecteurs de support.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "if ENCODE and IMPUTE:\n",
    "    # Compute scores\n",
    "    best_model_score = models_helpers.find_best_params_classifier(\n",
    "        X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "        estimator=SVC(\n",
    "            cache_size=1000, \n",
    "            max_iter=1e4,\n",
    "            random_state=42\n",
    "        ),\n",
    "        params={\n",
    "            \"C\": np.logspace(-2, 2, 5),\n",
    "            \"gamma\": [\"scale\", \"auto\"] + list(np.logspace(-2, 2, 5)),\n",
    "            \"degree\": range(2, 5),\n",
    "            \"kernel\": [\"linear\", \"poly\", \"rbf\"],\n",
    "        },\n",
    "    )\n",
    "    print({\n",
    "        \"params\": best_model_score['params'], \n",
    "        \"score\": best_model_score['score'],\n",
    "        \"predict_time\": best_model_score['predict_time'],\n",
    "    })\n",
    "    results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "    # Plot best model curves\n",
    "    vis_helpers.plot_classifier_results(\n",
    "        classifier=best_model_score['model'],\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if ENCODE and IMPUTE:\n",
    "    # if the kernel is linear, we can use the coefficients to plot the decision boundary\n",
    "    if best_model_score['params']['kernel'] == 'linear':\n",
    "        top_coefficients = pd.Series(\n",
    "            best_model_score['model'].coef_[0],\n",
    "            X_train.columns,\n",
    "        ).map(abs).sort_values(ascending=False).head(20)\n",
    "\n",
    "        if True or DRAW_PLOTS:\n",
    "            fig = px.bar(\n",
    "                top_coefficients,\n",
    "                color=top_coefficients.values,\n",
    "                title=\"Top 20 variables importance\",\n",
    "                labels={\n",
    "                    \"index\": \"Variable name\",\n",
    "                    \"value\": \"Coefficient\",\n",
    "                    \"color\": \"Coefficient\",\n",
    "                },\n",
    "                width=1200,\n",
    "                height=800,\n",
    "            )\n",
    "            fig.show()\n",
    "    else:\n",
    "        print(\"Kernel is not linear, impossible to estimate feature importances.\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K nearest neighbours\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## K-nearest-neighbours\n",
    "\n",
    "# Linear SVC\n",
    "# - Les SVM (Support Vector Machines), aussi appelées en français Machines à Vecteurs de Support et parfois Séparatrices à Vaste Marge, cherchent à séparer linéairement les données.\n",
    "# - La version primale résout un problème d'optimisation à p variables et est donc préférable si on a moins de variables que d'échantillons.\n",
    "# - À l'inverse, la version duale résout un problème d'optimisation à n variables et est donc préférable si on a moins d'échantillons que de variables.\n",
    "# - Les vecteurs de support sont les points du jeu de données qui sont les plus proches de l'hyperplan séparateur.\n",
    "# - La fonction de décision peut s'exprimer uniquement en fonction du produit scalaire du point à étiqueter avec les vecteurs de support.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "if ENCODE and IMPUTE:\n",
    "    # Compute scores\n",
    "    best_model_score = models_helpers.find_best_params_classifier(\n",
    "        X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "        estimator=KNeighborsClassifier(),\n",
    "        params={\n",
    "            \"n_neighbors\": range(2, 10),\n",
    "            \"weights\": [\"uniform\", \"distance\"],\n",
    "        },\n",
    "    )\n",
    "    print({\n",
    "        \"params\": best_model_score['params'], \n",
    "        \"score\": best_model_score['score'],\n",
    "        \"predict_time\": best_model_score['predict_time'],\n",
    "    })\n",
    "    results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "    # Plot best model curves\n",
    "    vis_helpers.plot_classifier_results(\n",
    "        classifier=best_model_score['model'],\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modèles ensemblistes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Bagging\n",
    "\n",
    "# Linear SVC\n",
    "# - Les SVM (Support Vector Machines), aussi appelées en français Machines à Vecteurs de Support et parfois Séparatrices à Vaste Marge, cherchent à séparer linéairement les données.\n",
    "# - La version primale résout un problème d'optimisation à p variables et est donc préférable si on a moins de variables que d'échantillons.\n",
    "# - À l'inverse, la version duale résout un problème d'optimisation à n variables et est donc préférable si on a moins d'échantillons que de variables.\n",
    "# - Les vecteurs de support sont les points du jeu de données qui sont les plus proches de l'hyperplan séparateur.\n",
    "# - La fonction de décision peut s'exprimer uniquement en fonction du produit scalaire du point à étiqueter avec les vecteurs de support.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "if ENCODE and IMPUTE:\n",
    "    # Compute scores\n",
    "    best_model_score = models_helpers.find_best_params_classifier(\n",
    "        X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "        estimator=BaggingClassifier(random_state=42),\n",
    "        params={\n",
    "            \"base_estimator\": [\n",
    "                DecisionTreeClassifier(),\n",
    "                DecisionTreeClassifier(max_depth=2),\n",
    "                DecisionTreeClassifier(max_depth=10),\n",
    "                ElasticNet(),\n",
    "                SVC(),\n",
    "            ],\n",
    "            \"n_estimators\": np.logspace(1, 3, 10).astype(int),\n",
    "            \"max_samples\": np.logspace(1, 4, 10).astype(int),\n",
    "            \"max_features\": np.linspace(5, int(len(X_train.columns)/2), 5).astype(int),\n",
    "        },\n",
    "    )\n",
    "    print({\n",
    "        \"params\": best_model_score['params'], \n",
    "        \"score\": best_model_score['score'],\n",
    "        \"predict_time\": best_model_score['predict_time'],\n",
    "    })\n",
    "    results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "    # Plot best model curves\n",
    "    vis_helpers.plot_classifier_results(\n",
    "        classifier=best_model_score['model'],\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Random Forest\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "if ENCODE and IMPUTE:\n",
    "    # Compute scores\n",
    "    best_model_score = models_helpers.find_best_params_classifier(\n",
    "        X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "        estimator=RandomForestClassifier(random_state=42),\n",
    "        params={\n",
    "            \"n_estimators\": np.logspace(1, 3, 10).astype(int),\n",
    "            \"max_depth\": [None, 2, 10],\n",
    "            \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "        },\n",
    "    )\n",
    "    print({\n",
    "        \"params\": best_model_score['params'], \n",
    "        \"score\": best_model_score['score'],\n",
    "        \"predict_time\": best_model_score['predict_time'],\n",
    "    })\n",
    "    results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "    # Plot best model curves\n",
    "    vis_helpers.plot_classifier_results(\n",
    "        classifier=best_model_score['model'],\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## XGBoost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "if ENCODE:\n",
    "    # Compute scores\n",
    "    best_model_score = models_helpers.find_best_params_classifier(\n",
    "        X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "        estimator=XGBClassifier(random_state=42),\n",
    "        params={\n",
    "            \"n_estimators\": np.logspace(1, 3, 5).astype(int),\n",
    "            \"learning_rate\": np.linspace(0.01, 0.5, 5),\n",
    "        },\n",
    "    )\n",
    "    print({\n",
    "        \"params\": best_model_score['params'], \n",
    "        \"score\": best_model_score['score'],\n",
    "        \"predict_time\": best_model_score['predict_time'],\n",
    "    })\n",
    "    results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "    # Plot best model curves\n",
    "    vis_helpers.plot_classifier_results(\n",
    "        classifier=best_model_score['model'],\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LGBMClassifier\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## LGBMClassifier\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "best_model_score = models_helpers.find_best_params_classifier(\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "    estimator=LGBMClassifier(random_state=42),\n",
    "    params={\n",
    "        \"n_estimators\": np.logspace(1, 3, 5).astype(int),\n",
    "        \"learning_rate\": np.linspace(0.01, 0.5, 5),\n",
    "    },\n",
    ")\n",
    "print({\n",
    "    \"params\": best_model_score['params'], \n",
    "    \"score\": best_model_score['score'],\n",
    "    \"predict_time\": best_model_score['predict_time'],\n",
    "})\n",
    "results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=best_model_score['model'],\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLPClassifier\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "if ENCODE:\n",
    "    # Compute scores\n",
    "    best_model_score = models_helpers.find_best_params_classifier(\n",
    "        X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "        estimator=MLPClassifier(random_state=42),\n",
    "        params={\n",
    "            \"hidden_layer_sizes\": [(2,), (2,3,), (2,3,5,), (2,5,3,), (3,), (3,5,), (5,3,), (5,), (10,), (20,)],\n",
    "            \"alpha\": np.logspace(-5, 1, 5),\n",
    "        },\n",
    "    )\n",
    "    print({\n",
    "        \"params\": best_model_score['params'], \n",
    "        \"score\": best_model_score['score'],\n",
    "        \"predict_time\": best_model_score['predict_time'],\n",
    "    })\n",
    "    results = results.append(best_model_score, ignore_index=True)\n",
    "\n",
    "    # Plot best model curves\n",
    "    vis_helpers.plot_classifier_results(\n",
    "        classifier=best_model_score['model'],\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## AutoML\n",
    "\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
    "from autosklearn.metrics import roc_auc, f1\n",
    "\n",
    "\n",
    "\n",
    "clf = AutoSklearn2Classifier(\n",
    "    time_left_for_this_task=120, \n",
    "    metric=f1,\n",
    "    n_jobs=4,\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_proba = clf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# Plot best model curves\n",
    "vis_helpers.plot_classifier_results(\n",
    "    classifier=clf,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv_results = pd.DataFrame(clf.cv_results_).sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
    "\n",
    "cv_results\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf.leaderboard()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf.sprint_statistics()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Annexe\n",
    "\n",
    "Les Notebooks Kaggle [Introduction: Home Credit Default Risk Competition](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction) (et suivants) de [Will Koehrsen](https://www.kaggle.com/willkoehrsen) ont été d'une très grande aide dans l'exploration des données.\n",
    "\n",
    "Comment bien choisir la métrique de score d'un algorithme de classification ?\n",
    "\n",
    "![How to Choose a Metric for Imbalanced Classification](https://machinelearningmastery.com/wp-content/uploads/2019/12/How-to-Choose-a-Metric-for-Imbalanced-Classification-latest.png)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87142f5ce831c365860d10ab1d6251a6bfb068b987aa68f91a7996e4fcdedd96"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit (conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}