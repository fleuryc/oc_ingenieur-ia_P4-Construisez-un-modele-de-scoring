{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_See [Readme](https://github.com/fleuryc/oc_ingenieur-ia_P3-Preparez-des-donnees-pour-un-organisme-de-sante-publique#readme) for installation instructions_\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santé Publique France : rendre les données de santé publique plus accessibles\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Santé Publique France (SPF) souhaite mettre à disposition de ses agents des informations plus claires, lisibles et accessibles que les données brutes disponibles. Nous allons ici étudier les données [Open Food Facts](https://world.openfoodfacts.org/) afin de les aider à mieux observer et comprendre quels sont les enjeux de santé publique liés aux produits alimentaires de la grande distribution.\n",
    "\n",
    "L'objectif est donc ici de produire des analyses graphiques parlantes au plus grand nombre et pertinentes du point de vue des problématiques de santé publique.\n",
    "\n",
    "\n",
    "## Outils utilisés\n",
    "\n",
    "Nous allons utiliser le langage Python, et présenter ici le code, les résultats et l'analyse sous forme de [Notebook JupyterLab](https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html).\n",
    "\n",
    "Nous allons aussi utiliser les bibliothèques usuelles d'exploration et analyse de données, afin d'améliorer la simplicité et la performance de notre code :\n",
    "  * [NumPy](https://numpy.org/doc/stable/user/quickstart.html) et [Pandas](https://pandas.pydata.org/docs/user_guide/index.html) : effectuer des calculs scientifiques (statistiques, algèbre, ...) et manipuler des séries et tableaux de données volumineuses et complexes\n",
    "  * [scikit-learn](https://scikit-learn.org/) : pour effectuer des analyses prédictives\n",
    "  * [Matplotlib](https://matplotlib.org/stable/tutorials/introductory/usage.html), [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html), [Seaborn](https://seaborn.pydata.org/tutorial/function_overview.html) et [Plotly](https://plotly.com/python/getting-started/) : générer des graphiques lisibles, intéractifs et pertinents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6fgKy6soGU0"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# System libraries to import the data\n",
    "import os.path \n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Math libraries to process the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Library for predictive data analysis\n",
    "from sklearn import decomposition, preprocessing\n",
    "\n",
    "# Graph libraries to produce graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "## If you use Notebook (and not JupyterLab), uncomment following lines\n",
    "# import plotly.io as pio\n",
    "# pio.renderers.default='notebook'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données et premier aperçu\n",
    "\n",
    "Les données mises à disposition sont issues de [Open Food Facts](https://world.openfoodfacts.org/) et présentent les données sur les produits alimentaires.\n",
    "\n",
    "Nous allons télécharger et extraire le fichier ZIP, puis effectuer une première passe afin de traiter les irrégularités du fichier, avant de charger les données et observer quelques valeurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Téléchargement et extraction des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ZIP and extract CSV\n",
    "data_local_path = 'data/'\n",
    "csv_filename = 'fr.openfoodfacts.org.products.csv'\n",
    "csv_local_path = data_local_path+csv_filename\n",
    "\n",
    "if not os.path.isfile(csv_local_path):\n",
    "    # only if the file is not already present\n",
    "    zip_filename = csv_filename+'.zip'\n",
    "    zip_url = 'https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/parcours-data-scientist/P2/'+zip_filename\n",
    "    zip_local_path = data_local_path+zip_filename\n",
    "\n",
    "    with urlopen(zip_url) as zip_response:\n",
    "        with ZipFile(BytesIO(zip_response.read())) as zip_file:\n",
    "            # extract all files do local data/ directory\n",
    "            zip_file.extractall(data_local_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion des irrégularités du fichier CSV téléchargé\n",
    "\n",
    "Le fichier contenant les données est mal formé à plusieurs endroits : des sauts de ligne sont présents dans 23 lignes à la fin de la colonne `first_packaging_code_geo`. Ces irrégularités sont facilement repérables car ce sont les seules lignes qui ne commencent pas par le code de l'article (`code`), mais par un séparateur `\\t`. Nous allons donc corriger ces irrégularités en supprimant les sauts de ligne superflus, puis écrire les données propres dans un nouveau fichier CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_filename = 'fr.openfoodfacts.org.products-clean.csv'\n",
    "clean_local_path = data_local_path+clean_filename\n",
    "\n",
    "if not os.path.isfile(clean_local_path):\n",
    "    # only if the clean file is not already presnt\n",
    "    with open(csv_local_path, 'r') as csv_file, open(clean_local_path, 'w') as clean_file:\n",
    "        \"\"\" Deal with irregularities\n",
    "\n",
    "            23 data points are wrongly split into two lines : \n",
    "            - lines : 189070, 189105, 189111, 189121, 189154, 189162, 189164, 189170, 189244, 189246, \n",
    "                    189250, 189252, 189262, 189264, 189271, 189274, 189347, 189364, 189366, 189381, \n",
    "                    189406, 189408, 189419\n",
    "            \n",
    "            The pattern is always the same : \n",
    "            - a NewLine character (`\\n`) is placed at the end of column \"first_packaging_code_geo\" \n",
    "            - and the next line starts with a TAB separator (`\\t`) : column \"cities\" is empty.\n",
    "            \n",
    "            Since the first column (\"code\") is never empty, we just remove any `\\n` character that is \n",
    "            directly followed by a TAB separator (`\\t`).\n",
    "        \"\"\"\n",
    "\n",
    "        data = csv_file.read()\n",
    "        clean_file.write(data.replace('\\n\\t', '\\t'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données\n",
    "\n",
    "Nous allons charger les données en mémoire et convertir les valeurs dans le bon type, selon les [spécifications fournies](https://static.openfoodfacts.org/data/data-fields.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read column names\n",
    "column_names = pd.read_csv(clean_local_path, sep='\\t', encoding='utf-8', nrows=0).columns.values\n",
    "\n",
    "# Set column types according to fields description (https://static.openfoodfacts.org/data/data-fields.txt)\n",
    "column_types = {col: 'Int64' for (col) in column_names if col.endswith(('_t', '_n'))}\n",
    "column_types |= {col: float for (col) in column_names if col.endswith(('_100g', '_serving'))}\n",
    "column_types |= {col: str for (col) in column_names if not col.endswith(('_t', '_n', '_100g', '_serving', '_tags'))}\n",
    "\n",
    "tags_converter = lambda list_as_string_value : list_as_string_value.split(',') if list_as_string_value else pd.NA\n",
    "\n",
    "# Load raw data\n",
    "raw_data = pd.read_csv(clean_local_path, sep='\\t', encoding='utf-8',\n",
    "    dtype=column_types,\n",
    "    parse_dates=[col for (col) in column_names if col.endswith('_datetime')],\n",
    "    infer_datetime_format=True,\n",
    "    converters={\n",
    "        # Convert '_tags' columns into list of values (separator : ',')\n",
    "        col: tags_converter\n",
    "        for (col) in column_names if col.endswith('_tags')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display DataFrame size\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier de données fourni contient 162 variables pour 320749 individus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection des données pertinentes\n",
    "\n",
    "Nous allons chercher à n'utiliser que les variables pertinentes pour SPF : celles pour lesquelles nous avons suffisament de valeurs non vides pour pouvoir faire une analyse statistique fiable, et qui peuvent avoir un réel sens du point de vue des problématiques de santé publique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function to reuse this graph later\n",
    "def plot_empty_values(dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\" Plot a histogram of empty values percentage per columns of the input DataFrame\n",
    "    \"\"\"\n",
    "    num_rows = len(dataframe.index)\n",
    "    columns_emptiness = pd.DataFrame({\n",
    "        col : { \n",
    "            'count': dataframe[col].isna().sum(),\n",
    "            'percent': 100 * dataframe[col].isna().sum() / num_rows,\n",
    "        } for col in dataframe.columns\n",
    "    }).transpose().sort_values(by=['count'])\n",
    "\n",
    "    fig = px.bar(columns_emptiness,\n",
    "        color='percent',\n",
    "        y='percent',\n",
    "        labels={\n",
    "            'index':'column name',\n",
    "            'percent':'% of empty values',\n",
    "            'count':'# of empty values',\n",
    "        },\n",
    "        hover_data=['count'],\n",
    "        title='Empty values per column',\n",
    "        width=1200,\n",
    "        height=600,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_empty_values(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons que un grand nombre de variables ont un taux de complétude très faible et ne seront donc pas utilisables.\n",
    "Nous allons donc restreindre notre analyse aux variables utilisées pour le calcul du [Nutri Score](https://www.santepubliquefrance.fr/determinants-de-sante/nutrition-et-activite-physique/articles/nutri-score), qui est un indicateur très parlant du point de vue de la santé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premier aperçu\n",
    "\n",
    "Affichons quelques informations et les premières valeurs observées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep only meaningful columns\n",
    "meaningful_columns = [\n",
    "    # General information\n",
    "    'code', 'product_name', 'main_category', 'additives_n', \n",
    "\n",
    "    # Nutri-Score\n",
    "    'nutrition_grade_fr', 'nutrition-score-fr_100g',\n",
    "\n",
    "    # Positive nutrition facts\n",
    "    'energy_100g', 'saturated-fat_100g', 'sugars_100g', 'salt_100g',\n",
    "\n",
    "    # Negative nutrition facts \n",
    "    'fruits-vegetables-nuts_100g', 'fiber_100g', 'proteins_100g',\n",
    "]\n",
    "meaningful_data = raw_data.loc[:, meaningful_columns].copy()\n",
    "\n",
    "# Display DataFrame size\n",
    "meaningful_data.info()\n",
    "\n",
    "# Display first values of each column\n",
    "meaningful_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Première analyse statistique\n",
    "\n",
    "Voyons quelle est la répartition des différentes variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistical summary of each column\n",
    "meaningful_data.describe(include=\"all\", datetime_is_numeric=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution des différentes valeurs\n",
    "\n",
    "Voyons comment sont distribuées certaines variables.\n",
    "\n",
    "\n",
    "### Variable catégorique nominale : `main_category`\n",
    "\n",
    "Voyons comment sont réparties les catégories de produits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the density of product categories\n",
    "fig = px.line(raw_data['main_category'].value_counts())\n",
    "fig.update_layout(\n",
    "    title_text=\"Product categories\",\n",
    "    width=1200,\n",
    "    height=800,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons déjà qu'il y a une répartition très inégale des catégories de produits et qu'il faudrait certainement améliorer la catégorisation afin d'éviter d'avoir un très grand nombre de catégories à 1 seul élément.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep only the top values and merge the rest into \"Other\"\n",
    "meaningful_data.loc[:,'top_category'] = raw_data['main_category'].where(\n",
    "    raw_data['main_category'].isna() | raw_data['main_category'].isin(raw_data['main_category'].value_counts().index[:20]), \n",
    "    other='other', \n",
    ")\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2, \n",
    "    subplot_titles=(\"Top 20 with 'other'\", \"Top 20\"), \n",
    "    specs=[[{'type':'domain'}, {'type':'domain'}]],\n",
    ")\n",
    "fig.add_trace(go.Pie(\n",
    "    labels=meaningful_data['top_category'].value_counts().index, \n",
    "    values=meaningful_data['top_category'].value_counts().values, \n",
    "    name=\"Including 'other'\",\n",
    "    pull=[0.05 if cat == 'other' else 0 for cat in meaningful_data['top_category'].value_counts().index],\n",
    "), row=1, col=1)\n",
    "fig.add_trace(go.Pie(\n",
    "    labels=meaningful_data['top_category'].value_counts().index[1:], \n",
    "    values=meaningful_data['top_category'].value_counts().values[1:], \n",
    "    name=\"Top 20\",\n",
    "), row=1, col=2)\n",
    "fig.update_traces(\n",
    "    textposition='inside',\n",
    "    textinfo='percent+label'\n",
    ")\n",
    "fig.update_layout(\n",
    "    title_text=\"Product categories\",\n",
    "    width=1200,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons que les 20 catégories les plus représentées représentent près de 50% de toutes les valeurs. De même, les 5 premières catégories représentent plus de 80% des 3543 valeurs possibles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Variable catégorique ordinale : `nutrition_grade_fr`\n",
    "\n",
    "Voyons comment sont réparties les notes de Nutri-Score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTRITION_GRADES = ('a', 'b', 'c', 'd', 'e')\n",
    "TOP_CATEGORIES = tuple(meaningful_data['top_category'].value_counts().index)\n",
    "\n",
    "# Display the nutrition grade distribution per product category\n",
    "fig = px.histogram(meaningful_data.loc[meaningful_data['top_category'].notnull() & meaningful_data['nutrition_grade_fr'].notnull()],\n",
    "    x='nutrition_grade_fr',\n",
    "    category_orders={'nutrition_grade_fr': NUTRITION_GRADES, 'top_category': TOP_CATEGORIES},\n",
    "    color='top_category',\n",
    "    title='Global nutrition grade repartition by product category',\n",
    "    width=1200,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons que parmis les produits répertoriés, la répartition des produits par Nutri-Score est globalement comparable, avec une sur-représentation du label \"D\", et une légère sous-représentation du label \"B\". Nous voyons aussi de quels catégories de produits sont composés chaque scores. Par exemple, il y a beaucoup de conserves parmis les produits de score \"A\", et de chocolats parmis les produits de score \"E\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the product category distribution by nutrition grade\n",
    "fig = px.histogram(meaningful_data.loc[meaningful_data['top_category'] != 'other'].loc[meaningful_data['top_category'].notnull() & meaningful_data['nutrition_grade_fr'].notnull()],\n",
    "    x='top_category',\n",
    "    category_orders={'nutrition_grade_fr': NUTRITION_GRADES, 'top_category': TOP_CATEGORIES},\n",
    "    color='nutrition_grade_fr',\n",
    "    title='Top 20 product categories repartition by nutrition grade',\n",
    "    width=1200,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la même manière, nous voyons que la plupart des chocolats ont un score de \"E\" et la plupart des conserves ont un score de \"A\". Nous mesurerons précisément la corrélation entre les catégories de produits et les notes de Nutri-Score plus loin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables numérique\n",
    "\n",
    "Nous allons dans un premier temps nettoyer les données numérique, avant d'en étudier la répartition. Ceci permettra d'avoir des données plus fiables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nettoyage : suppression des données aberrantes\n",
    "\n",
    "Parmis les données fournies, nous voyons des valeurs négatives pour des variables comme le nombre d'additifs, ou la quantité de sucre pour 100g de produit, ce qui est impossible.\n",
    "Nous allons aussi utiliser la méthode IQR (Inter Quartile Range) pour identifier les données aberrantes (outliers) et les supprimer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function that can be reused\n",
    "def remove_negative_values(dataframe: pd.DataFrame, columns: list[str]) -> pd.DataFrame:\n",
    "    \"\"\" Remove negative values from specified columns of DataFrame\n",
    "    \"\"\"\n",
    "    df = dataframe.copy()\n",
    "    for col in columns:\n",
    "        df.loc[:,col] = dataframe[col].where(dataframe[col] >= 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "positive_columns = [\n",
    "    'additives_n', \n",
    "    'energy_100g', 'saturated-fat_100g', 'sugars_100g', 'salt_100g',\n",
    "    'fruits-vegetables-nuts_100g', 'fiber_100g', 'proteins_100g',\n",
    "]\n",
    "\n",
    "clean_data = remove_negative_values(meaningful_data, positive_columns)\n",
    "\n",
    "clean_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons bien supprimé les valeurs négatives impossibles, mais nous voyons encore des valeurs maximum aberrantes (ex. : 550g d'acides gras saturés pour 100g de produit).\n",
    "Nous allons donc supprimer les valeurs aberrantes restantes grâce à la méthode IQR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function that can be reused\n",
    "def remove_outliers(dataframe: pd.DataFrame, columns: list[str]) -> pd.DataFrame:\n",
    "    \"\"\" Remove outlier values from specified columns of DataFrame\n",
    "\n",
    "        Compute the Inter-Quartile Ranges and set outliers to NaN\n",
    "    \"\"\"\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # compute quartiles and define range\n",
    "    quartiles = df[columns].quantile([0.25, 0.75])\n",
    "    iqr = quartiles.loc[0.75]-quartiles.loc[0.25]\n",
    "    limits = pd.DataFrame({\n",
    "        col: [\n",
    "            quartiles.loc[0.25, col] - 1.5 * iqr[col], # min\n",
    "            quartiles.loc[0.75, col] + 1.5 * iqr[col], # max\n",
    "        ] for col in columns\n",
    "    }, index=['min', 'max'])\n",
    "\n",
    "    # set to NaN data that are outside the range\n",
    "    for col in columns:\n",
    "        df.loc[:,col] = dataframe[col].where(\n",
    "            limits.loc['min', col] <= dataframe[col]\n",
    "        ).where(\n",
    "            dataframe[col] <= limits.loc['max', col]\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "numeric_columns = positive_columns.copy()\n",
    "numeric_columns.append('nutrition-score-fr_100g')\n",
    "\n",
    "clean_data = remove_outliers(clean_data, numeric_columns)\n",
    "\n",
    "clean_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons maintenant des valeurs qui semblent correctes. Observons leur répartition avant de compléter les valeurs vides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function to plot multiple BoxPlots\n",
    "def draw_boxplots(dataframe: pd.DataFrame, categorical_column: str, numerical_columns: list[str], order_values: tuple[str] = None, num_cols: int = 3) -> None:\n",
    "    \"\"\" Draw one boxplot per numerical variable, split per categories.\n",
    "\n",
    "        Arguments :\n",
    "        - dataframe : Pandas DataFrame containing the data, including the categorical_column and numerical_columns\n",
    "        - categorical_column : string representing the name of the variable containing the categories\n",
    "        - numerical_columns : list of strings representing the name of the numerical variables to plot\n",
    "        - order_values : list of strings representing the values of the numerical variables to plot\n",
    "\n",
    "        Returns : None\n",
    "    \"\"\"\n",
    "    num_lines = int(np.ceil(len(numerical_columns) / num_cols))\n",
    "    fig, axes = plt.subplots(num_lines, num_cols, figsize=(8 * num_cols , 8 * num_lines))\n",
    "    fig.suptitle(f'Numeric variables distribution, per { categorical_column }', fontsize=24)\n",
    "\n",
    "    for i, col in enumerate(numerical_columns):\n",
    "        sns.boxplot(data=dataframe,\n",
    "            x=categorical_column, \n",
    "            y=col,\n",
    "            order=order_values,\n",
    "            showmeans=True,\n",
    "            ax=axes[int(np.floor(i / num_cols)), i % num_cols],\n",
    "        )\n",
    "\n",
    "# Draw the BoxPlots of each numeric column, split per Nutrition Grade\n",
    "draw_boxplots(\n",
    "    dataframe=clean_data, \n",
    "    categorical_column='nutrition_grade_fr', \n",
    "    numerical_columns=numeric_columns, \n",
    "    order_values=NUTRITION_GRADES\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons qu'il y a encore des données aberrantes, notamment en analysant les données selon la note de Nutri-Score.\n",
    "Nous allons donc à nouveau supprimer ces données aberrantes grâce à la fonction `remove_outliers()` définie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's work on a copy of our clean data\n",
    "super_clean_data = clean_data.copy()\n",
    "\n",
    "for grade in NUTRITION_GRADES:\n",
    "    # for each nutrition grade, \n",
    "    # we remove the outliers of each numeric column detected \n",
    "    # after filtering the data by nutrition grade\n",
    "    super_clean_data.loc[super_clean_data['nutrition_grade_fr'] == grade] = remove_outliers(clean_data.loc[clean_data['nutrition_grade_fr'] == grade], numeric_columns)\n",
    "\n",
    "super_clean_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's draw the BoxPlots again after more data cleaning\n",
    "draw_boxplots(\n",
    "    dataframe=super_clean_data, \n",
    "    categorical_column='nutrition_grade_fr', \n",
    "    numerical_columns=numeric_columns, \n",
    "    order_values=NUTRITION_GRADES\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons maintenant que les données sont quasiment toutes contenues dans les \"moustaches\" des boxplots, autrement dit il n'y a presque plus de données aberrantes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nettoyage : suppression des doublons et lignes vides\n",
    "\n",
    "Il est inutile de conserver des données en doublons, nous allons donc supprimer ces lignes. En l'occurrence, comme le `code` est unique à chaque produit, il ne peut pas y avoir de lignes en doublon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's count duplucated lines\n",
    "duplicates = super_clean_data.duplicated()\n",
    "duplicates.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la suite, toute notre analyse va se baser sur le paramètre `nutrition_grade_fr`. Nous allons supprimer les lignes où la valeur de cette variable est vide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop raws where nutrition_grade_fr is empty\n",
    "nutrition_data = super_clean_data.dropna(subset=['nutrition_grade_fr']).copy()\n",
    "plot_empty_values(nutrition_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Nettoyage : remplacement des valeurs manquantes\n",
    "\n",
    "Nous pouvons alors considérer les données restantes comme fiables, et nous allons nous baser sur ces valeurs pour extrapoler les valeurs manquantes dans notre jeu de données.\n",
    "Nous allons remplacer, pour chaque valeur numérique vide, la valeur moyenne des individus ayant la même note de Nutri-Score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the mean of each numeric variable\n",
    "means = nutrition_data.groupby('nutrition_grade_fr').mean()\n",
    "\n",
    "# Special processing for the number of additives, which has to be a round number\n",
    "means.loc[:,'additives_n'] = means['additives_n'].map(np.round)\n",
    "\n",
    "# Fill empty values with means\n",
    "for grade in means.index:\n",
    "    nutrition_data.loc[nutrition_data['nutrition_grade_fr'] == grade] = nutrition_data[nutrition_data['nutrition_grade_fr'] == grade].fillna(\n",
    "        value=means.loc[grade]\n",
    "    )\n",
    "\n",
    "# Let's see if we still have empty values\n",
    "plot_empty_values(nutrition_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons que nous n'avons plus de valeurs vides pour toutes les variables numériques sélectionnées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche et analyse des corrélations\n",
    "\n",
    "Maintenant nos données nettoyées, nous allons l'exploiter afin d'observer et mesurer les tendances significatives au sein du jeu de données.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrélation entre catégories de produits et Nutri-Score\n",
    "\n",
    "Nous allons ici analyser l'influence réciproque des deux variables catégoriques : `top_category` et `nutrition_grade_fr`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's compute the contingency matrix, excluding category 'other'\n",
    "contingency_table = nutrition_data.loc[nutrition_data['top_category'] != 'other'].pivot_table(\n",
    "    values='code',\n",
    "    index='top_category',\n",
    "    columns='nutrition_grade_fr',\n",
    "    aggfunc='count',\n",
    "    fill_value=0,\n",
    "    observed=True,\n",
    ").sort_values(by=list(NUTRITION_GRADES))\n",
    "\n",
    "# Let's compute the normalized contributions to dependency of each variable\n",
    "tx = contingency_table.sum(axis='columns').to_frame()\n",
    "ty = contingency_table.sum(axis='index').to_frame().T\n",
    "indep = tx.dot(ty) / len(nutrition_data)\n",
    "measure = (contingency_table - indep)**2 / indep\n",
    "xi_n = measure.sum().sum()\n",
    "table = measure / xi_n \n",
    "\n",
    "table.sort_values(\n",
    "    by=list(NUTRITION_GRADES), \n",
    "    ascending=False,\n",
    "    inplace=True, \n",
    ")\n",
    "\n",
    "# Display the heatmap of top product categories per nutrition grade\n",
    "fig = px.imshow(table,\n",
    "    title=\"Normalised correlations between Nutrition Grades and Product Categories\",\n",
    "    width=1200,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce tableau nous indique les corrélations fortes entre les catégories de produits et le Nutri-Scores.\n",
    "\n",
    "Nous voyons déjà que parmis les 20 types de produits les plus représentés, ceux ayant un meilleur Nutri-Score sont les produits en boîte, les produits à base de plantes et les pâtes. Les légumes frais sont le plus souvent de Nutri-Score A, tandis que les chocolats, bonbons, biscuits et en-cas sucrés ont le plus souvent un mauvais Nutri-Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrélation entre valeurs nutritives et note de Nutri-Score\n",
    "\n",
    "Nous allons ici analyser l'influence réciproque des variables numériques prises en compte dans le calcul du Nutri-Score et la note de Nutri-Score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's draw the BoxPlots again after more data cleaning\n",
    "draw_boxplots(\n",
    "    dataframe=nutrition_data, \n",
    "    categorical_column='nutrition_grade_fr', \n",
    "    numerical_columns=numeric_columns, \n",
    "    order_values=NUTRITION_GRADES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons que, suite à l'imputation des valeurs vides, les médianes et les quartiles se sont rapprochés de la moyenne, ce qui fait apparaitre de nouveaux outliners, que nous allons conserver.\n",
    "\n",
    "Il faut noter le cas particulier de la variable `fruits-vegetables-nuts_100g` pour laquelle nous avons remplacé les 98,7% de valeurs vides par la moyenne des 1,3% de valeurs non vides. La variance est donc très faible pour cette variable et cette imputation pourrait entraîner des résultats biaisés par la suite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons chercher à quantifier la corrélation entre chacune de ces valeurs nutritives et la note de Nutri-Score. Pour celà, nous allons effectuer une analyse de la variance (ANOVA) entre les variables numériques (valeurs nutritives) et les catégories (notes Nutri-Score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the correlation function\n",
    "def eta_squared(x,y):\n",
    "    \"\"\" Compute the correlation ratio between categorical variable (x) and numeric variable (y)\n",
    "    \"\"\"\n",
    "    moyenne_y = y.mean()\n",
    "    classes = []\n",
    "    for classe in x.unique():\n",
    "        yi_classe = y[x==classe]\n",
    "        classes.append({'ni': len(yi_classe),\n",
    "                        'moyenne_classe': yi_classe.mean()})\n",
    "    SCT = sum([(yj-moyenne_y)**2 for yj in y])\n",
    "    SCE = sum([c['ni']*(c['moyenne_classe']-moyenne_y)**2 for c in classes])\n",
    "    return SCE/SCT\n",
    "\n",
    "# Compute the E² value for each numeric variable\n",
    "anova=pd.DataFrame(columns=['eta_squared'])\n",
    "for col in numeric_columns:\n",
    "    anova.loc[col] = eta_squared(nutrition_data['nutrition_grade_fr'],nutrition_data[col])\n",
    "\n",
    "# Sort the variables : most influential first\n",
    "sorted_anova = anova.sort_values(by='eta_squared', ascending=False)\n",
    "\n",
    "# plot the graph\n",
    "fig = px.scatter(sorted_anova,\n",
    "    x=sorted_anova.index,\n",
    "    y=sorted_anova['eta_squared'],\n",
    "    color='eta_squared',\n",
    "    size='eta_squared',\n",
    "    title='Correlation between Nutri-Score grade and nutrition variables',\n",
    "    width=1200,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons ici qu'il y a une corrélation presque parfaite ($ \\eta^2 = 0.94 $) entre `nutrition-score-fr_100g` et `nutrition_grade_fr` : ceci est attendu, puisque la note de Nutri-Score est défini linéairement à partir du Nutri-Score. Le fait que la corrélation ne soit pas de 1 montre qu'il y a des erreurs ou exceptions qu'il faudrait corriger ou expliquer.\n",
    "\n",
    "Nous observons une très forte corrélation ($ \\eta^2 = 0.88 $) entre `fruits-vegetables-nuts_100g` et `nutrition_grade_fr` : ceci s'explique principalemet par l'imputation faite par la moyenne sur la plupart des valeurs de cette variable. Ce résultat n'est donc pas fiable et ne permet pas de tirer de conclusion.\n",
    "\n",
    "Nous voyons ensuite que les variables ayant la plus grande corrélation avec le Nutri-Score sont les valeurs nutritives influencant négativement le Nutri-Score : la densité d'énergie, les gresses saturées et les sucres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrélation entre valeurs nutritives et catégories de produits\n",
    "\n",
    "Nous allons ici analyser l'influence réciproque des variables numériques prises en compte dans le calcul du Nutri-Score et les trois catégories de produits les plus représentées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep only the top 3 values and merge the rest into \"Other\"\n",
    "nutrition_data.loc[:,'top_top_category'] = nutrition_data['main_category'].where(\n",
    "    nutrition_data['main_category'].isna() | nutrition_data['main_category'].isin(nutrition_data['main_category'].value_counts().index[:3]), \n",
    "    other='other', \n",
    ")\n",
    "\n",
    "# Let's draw the BoxPlots again after more data cleaning\n",
    "draw_boxplots(\n",
    "    dataframe=nutrition_data, \n",
    "    categorical_column='top_top_category', \n",
    "    numerical_columns=numeric_columns, \n",
    "    order_values=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette analyse nous permet de confirmer les corrélations observées précédemment : les chocolats ont généralement des valeurs élevées d'énergie, de gras et de sucre, et un Nutri-Score plus élevé que le reste des produits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrélation entre les différentes valeurs nutritives\n",
    "\n",
    "Nous allons ici analyser l'influence réciproque des variables numériques prises en compte dans le calcul du Nutri-Score. Nous allons observer séparément les variables qui doivent avoir une influence positive sur le Nutri-Score, puis les variables devant avoir une influence négative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(nutrition_data.sample(frac=.01),\n",
    "    dimensions=[\n",
    "        'energy_100g', \n",
    "        'saturated-fat_100g', \n",
    "        'sugars_100g', \n",
    "        'salt_100g', \n",
    "        'nutrition-score-fr_100g',\n",
    "    ],\n",
    "    color=\"nutrition_grade_fr\",\n",
    "    symbol=\"nutrition_grade_fr\",\n",
    "    category_orders={'nutrition_grade_fr': NUTRITION_GRADES},\n",
    "    hover_data=['product_name', 'main_category'],\n",
    "    opacity=.2,\n",
    "    width=1200,\n",
    "    height=1200,\n",
    "    title=\"Distribution of products over variables increasing the Nutri-Score\",\n",
    ")\n",
    "fig.update_traces(\n",
    "    showupperhalf=False,\n",
    "    diagonal_visible=False,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons ici qu'il semble y avoir une corrélation positive entre les variables `energy`, `sugars` et `fat`.\n",
    "La corrélation positive entre le Nutri-Score et toutes les variables listées ici est aussi assez visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(nutrition_data.sample(frac=.01),\n",
    "    dimensions=[\n",
    "        'fruits-vegetables-nuts_100g', \n",
    "        'fiber_100g',\n",
    "        'proteins_100g',\n",
    "        'nutrition-score-fr_100g',\n",
    "    ],\n",
    "    color=\"nutrition_grade_fr\", \n",
    "    symbol=\"nutrition_grade_fr\",\n",
    "    category_orders={'nutrition_grade_fr': NUTRITION_GRADES},\n",
    "    hover_data=['product_name', 'main_category'],\n",
    "    opacity=.2,\n",
    "    width=1200,\n",
    "    height=1200,\n",
    "    title=\"Distribution of products over variables decreasing the Nutri-Score\",\n",
    ")\n",
    "fig.update_traces(\n",
    "    showupperhalf=False,\n",
    "    diagonal_visible=False,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est ici assez difficile de distingueer une corrélation claire entre les différentes variables.\n",
    "\n",
    "Noous allons devoir recourrir à une mesure mathématique des corrélations afin de les quantifier et les apprécier réellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = nutrition_data[[\n",
    "        'additives_n', \n",
    "        'energy_100g', \n",
    "        'saturated-fat_100g', \n",
    "        'sugars_100g', \n",
    "        'salt_100g', \n",
    "        # 'nutrition-score-fr_100g',\n",
    "        'fruits-vegetables-nuts_100g', \n",
    "        'fiber_100g',\n",
    "        'proteins_100g',\n",
    "    ]].corr()\n",
    "\n",
    "fig = px.imshow(corr.where(np.tril(np.ones(corr.shape), -1).astype(bool)),\n",
    "    title=\"Pairwise Pearson correlations of nutrition variables\",\n",
    "    width=1200,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons nettement ici qu'il y a une forte corrélation positive entre les produits à forte densité énergétique et gras , ainsi qu'une forte corrélation négative entre les produits gras et à forte teneur en fruits, légules et noix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modélisation de l'influence des valeurs nutritives sur le Nutri-Score\n",
    "\n",
    "Nous allons ici tenter de modéliser nos différents produits selont leurs valeurs nutritives, et comparer ce modèle aux notes de Nutri-Score, ainsi qu'aux catégories de produits afin d'essayer de généraliser des tendances observables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans en premier temps, nous allons normaliser chacune des variables numériques afin de mieux pouvoir les comparer.\n",
    "Nous en profitons pour modéliser (via une régression linéaire) comment sont corrélées chacune des valeurs nutritives avec le Nutri-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalize each numeric value\n",
    "normalized_nutrition_data = nutrition_data.copy()\n",
    "normalized_nutrition_data[numeric_columns]=( nutrition_data[numeric_columns] - nutrition_data[numeric_columns].mean() ) / nutrition_data[numeric_columns].std()\n",
    "\n",
    "# Plot \n",
    "fig = px.scatter(normalized_nutrition_data.sample(frac=.01),\n",
    "    x=[\n",
    "        'additives_n', \n",
    "        'energy_100g', \n",
    "        'saturated-fat_100g', \n",
    "        'sugars_100g', \n",
    "        'salt_100g', \n",
    "        'fruits-vegetables-nuts_100g', \n",
    "        'fiber_100g',\n",
    "        'proteins_100g',\n",
    "    ],\n",
    "    y='nutrition-score-fr_100g',\n",
    "    hover_data=['product_name', 'main_category'],\n",
    "    trendline='ols', # Ordinary Least Squares\n",
    "    opacity=.2,\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    title=\"Linear regression models of nutritive values\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons ici que la variable ayant la plus grande influence positive est le taux de graisses saturées ($ R² = 0,54 $ , $ a = 0,75 $). A contrario, le taux de fruits-legumes-noix a une très forte influence négative sur le Nutri-Score ($ R² = 0,49 $ , $ a = -0,67 $).\n",
    "Les taux de protéines a en reanche très peu de corrélation avec le Nutri-Score ( $ R² < 0.002 $ )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant chercher à définir de nouvelles variables qui sont des composantes des variables existantes et qui permettent de résumer de manière optimisée les informations contenues dans les variables existantes. Cette méthode s'appelle l'Analyse en Composantes Principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's choose 3 new components\n",
    "num_components=3\n",
    "\n",
    "# Let's instantiate our PCA proccessor\n",
    "pca = decomposition.PCA(n_components=num_components)\n",
    "\n",
    "# Let's compute the components and project our data onto the new frame defined by the components\n",
    "projected_nutrition_data = pd.DataFrame(\n",
    "    data=pca.fit_transform(normalized_nutrition_data[numeric_columns]), \n",
    "    index=normalized_nutrition_data.index, \n",
    "    columns=[ f'PC{i}' for i in range(1, num_components+1) ]\n",
    ")\n",
    "projected_nutrition_data[['nutrition_grade_fr', 'top_top_category']] = nutrition_data[['nutrition_grade_fr', 'top_top_category']]\n",
    "\n",
    "# Let's plot a 3D Scatter graph of our data in the new frame of reference\n",
    "fig = px.scatter_3d(projected_nutrition_data.sample(frac=.01),\n",
    "    x=projected_nutrition_data.columns[0], \n",
    "    y=projected_nutrition_data.columns[1], \n",
    "    z=projected_nutrition_data.columns[2],\n",
    "    color=\"nutrition_grade_fr\",\n",
    "    symbol=\"nutrition_grade_fr\",\n",
    "    category_orders={'nutrition_grade_fr': NUTRITION_GRADES},\n",
    "    opacity=.2,\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    title=\"Products projection in the Principal Components frame\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette nouvelle projection nous permet de visualiser les produits en maximisant la variance dans chaque axe, ce qui permet de bien distinguer chaque individu.\n",
    "Nous observons notamment que selon l'axe \"PC1\", les produits de Nutri-Score \"A\" sont bien isolés du reste des points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons s'il est nécessaire d'ajouter de nouvelles composantes à notre analyse en mesurant la variance des composantes déjà sélectionnées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the variance ratio of each component\n",
    "pca_components_variance_ratio = pd.DataFrame(\n",
    "    data=pca.explained_variance_ratio_,\n",
    "    index=projected_nutrition_data.columns[:-2],\n",
    "    columns=['variance'],\n",
    ")\n",
    "\n",
    "fig = px.scatter(pca_components_variance_ratio,\n",
    "    x=pca_components_variance_ratio.index,\n",
    "    y='variance',\n",
    "    color=pca_components_variance_ratio['variance'],\n",
    "    size=pca_components_variance_ratio['variance'],\n",
    "    title='Variance ratio of each principal componants',\n",
    "    width=1200,\n",
    "    height=400,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons que la troisième composante apporte relativement peu de nouvelle information par rapport aux deux premières. Il n'est donc pas nécessaire d'ajouter de nouvelles composantes à notre analyse.\n",
    "\n",
    "Voyons maintenant comment se répartissent les notes de Nutri-Score , ainsi que les catégories de produits dans nos nouvelles composantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "    x=projected_nutrition_data['nutrition_grade_fr'],\n",
    "    y=projected_nutrition_data['PC1'],\n",
    "    name='PC1',\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "    x=projected_nutrition_data['nutrition_grade_fr'],\n",
    "    y=projected_nutrition_data['PC2'],\n",
    "    name='PC2',\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "    x=projected_nutrition_data['nutrition_grade_fr'],\n",
    "    y=projected_nutrition_data['PC3'],\n",
    "    name='PC3',\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis={\n",
    "        'categoryorder': 'array',\n",
    "        'categoryarray': NUTRITION_GRADES,\n",
    "    },\n",
    "    xaxis_title='Nutri-Score grade',\n",
    "    yaxis_title='Products repartition along Principal Components, per Nutri-Score grade',\n",
    "    boxmode='group',\n",
    "    width=1200,\n",
    "    height=800,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "    x=projected_nutrition_data['top_top_category'],\n",
    "    y=projected_nutrition_data['PC1'],\n",
    "    name='PC1',\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "    x=projected_nutrition_data['top_top_category'],\n",
    "    y=projected_nutrition_data['PC2'],\n",
    "    name='PC2',\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "    x=projected_nutrition_data['top_top_category'],\n",
    "    y=projected_nutrition_data['PC3'],\n",
    "    name='PC3',\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Product Category',\n",
    "    yaxis_title='Products repartition along Principal Components, per Product Category',\n",
    "    boxmode='group',\n",
    "    width=1200,\n",
    "    height=800,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cherchons maintenant à interpréter ces composantes principales en mesurant combien chaque variable initiale contribue à chaque nouvelle composate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's decompose each component in the initial variables frame\n",
    "pca_components=pd.DataFrame(\n",
    "    data=pca.components_,\n",
    "    index=projected_nutrition_data.columns[:-2],\n",
    "    columns=numeric_columns,\n",
    ")\n",
    "\n",
    "pca_components.sort_values(\n",
    "    by=list(pca_components.index), \n",
    "    axis='columns',\n",
    "    ascending=False,\n",
    "    inplace=True, \n",
    ")\n",
    "\n",
    "fig = px.imshow(pca_components,\n",
    "    title=\"Contribution of numeric variable to each principal Component\",\n",
    "    width=1200,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons que la composante \"PC1\" correspond aux produits contenant beaucoup de fruits-legumes-noix et peu de sucres, energie, graisses, et ayant donc un Nutri-Score très faible.\n",
    "La seconde composante correspond aux produits à haute teneur en protéines et fibres, et faible en additifs et sucres. La dernière composante correspond aux produits contenant très peu de sel et plutôt sucrées.\n",
    "\n",
    "Ces derniers graphiques nous confirment les observations précédentes et nous permettent de généraliser le sens de chaque composantes :\n",
    "  * PC1 pourrait correspondre aux légumes (haute teneur en produits végétaux, très peu de sucres et graisses)\n",
    "  * PC2 pourrait correspondre aux viandes (haute teneur en protéines et fibres, peu de sucres)\n",
    "  * PC3 pourrait correspondre aux desserts (haute teneur en sucres, très peu de sel)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "main.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a9cd86936e2ed79c88e354605985c1d2e1f30b04e4ab0c2ca683da4978857858"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "c90de28d937fea21fa0e8c0b21949a1a37ba0cff78a53465482821b458660fc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
